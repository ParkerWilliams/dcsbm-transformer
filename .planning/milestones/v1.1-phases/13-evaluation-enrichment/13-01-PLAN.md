---
phase: 13-evaluation-enrichment
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/analysis/pr_curves.py
  - src/visualization/pr_curves.py
  - src/visualization/render.py
  - src/reporting/single.py
  - src/reporting/templates/single_report.html
  - src/results/schema.py
  - tests/test_pr_curves.py
autonomous: true
requirements: [PRCL-01, PRCL-03]

must_haves:
  truths:
    - "compute_pr_at_lookback returns precision, recall, auprc, and prevalence for given events and metric at a specific lookback distance"
    - "run_pr_analysis produces nested dict with by_r_value containing auprc_by_lookback per metric, matching auroc_horizon.py structure"
    - "PR curve plot shows one subplot per metric with no-skill baseline at prevalence rate"
    - "AUPRC summary table is rendered in the HTML report in a collapsible section after AUROC"
    - "result.json pr_curves block validates through schema.py"
  artifacts:
    - src/analysis/pr_curves.py
    - src/visualization/pr_curves.py
    - tests/test_pr_curves.py
  key_links:
    - "pr_curves.py reuses extract_events, filter_contaminated_events, stratify_by_r from event_extraction.py"
    - "render.py calls plot_pr_curves and saves pr_curve_* figures"
    - "single.py collects pr_curve_* figures and passes to template"
    - "single_report.html renders PR curve section as collapsible details/summary block"
---

<objective>
Implement precision-recall curves and AUPRC computation per metric per lookback distance, with visualization and HTML report integration.

Purpose: PRCL-01 requires PR curves using the same event extraction as AUROC. PRCL-03 requires rendering in HTML reports alongside AUROC plots.
Output: PR curve analysis module, plot function, report section, schema validation, and tests.
</objective>

<execution_context>
@/root/.claude/get-shit-done/workflows/execute-plan.md
@/root/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-evaluation-enrichment/13-RESEARCH.md

@src/analysis/auroc_horizon.py
@src/analysis/event_extraction.py
@src/visualization/auroc.py
@src/visualization/render.py
@src/visualization/style.py
@src/reporting/single.py
@src/reporting/templates/single_report.html
@src/results/schema.py

<interfaces>
<!-- Key types and contracts the executor needs. -->

From src/analysis/event_extraction.py:
```python
@dataclass(frozen=True, slots=True)
class AnalysisEvent:
    walk_idx: int
    encounter_step: int
    resolution_step: int
    r_value: int
    outcome: int
    is_first_violation: bool

def extract_events(generated, rule_outcome, failure_index, jumper_map) -> list[AnalysisEvent]
def filter_contaminated_events(events) -> tuple[list[AnalysisEvent], dict]
def stratify_by_r(events) -> dict[int, list[AnalysisEvent]]
```

From src/analysis/auroc_horizon.py:
```python
PRIMARY_METRICS: frozenset[str]  # 5 pre-registered primary metrics
def auroc_from_groups(violations, controls) -> float
def compute_auroc_curve(violation_events, control_events, metric_array, r_value, min_per_class=2) -> np.ndarray
def run_auroc_analysis(eval_result_data, jumper_map, metric_keys, ...) -> dict
```

From src/visualization/style.py:
```python
PALETTE = sns.color_palette("colorblind", n_colors=8)
VIOLATION_COLOR, CONTROL_COLOR, BASELINE_COLOR, THRESHOLD_COLOR
def apply_style() -> None
def save_figure(fig, output_dir, name) -> tuple[Path, Path]
```

From src/reporting/single.py:
```python
def _collect_figures(figures_dir: Path) -> dict[str, Any]  # categorizes by filename prefix
def generate_single_report(result_dir, output_path=None) -> Path
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create PR curve analysis module and tests</name>
  <files>
    src/analysis/pr_curves.py
    tests/test_pr_curves.py
  </files>
  <action>
Create `src/analysis/pr_curves.py` mirroring the structure of `auroc_horizon.py`:

1. **Helper function `_gather_values_at_lookback`:** Given violation_events, control_events, metric_array, and lookback j, gather metric values at position (resolution_step - j) for each event. Filter NaN values. Return (viol_vals: np.ndarray, ctrl_vals: np.ndarray). This is the same gathering logic used in `compute_auroc_curve` but factored out.

2. **`compute_pr_at_lookback`:** Given violation_events, control_events, metric_array, r_value, lookback j, and min_per_class=2:
   - Call _gather_values_at_lookback to get viol_vals and ctrl_vals
   - If either has fewer than min_per_class, return dict with all NaN values
   - Determine score direction: compute AUROC via auroc_from_groups. If AUROC < 0.5, negate all scores (so higher always means "more like violation")
   - Build binary labels: 1 for violations, 0 for controls
   - Concatenate scores = viol_vals + ctrl_vals (with negation if needed)
   - Use `sklearn.metrics.precision_recall_curve(labels, scores)` to get precision, recall, thresholds
   - Use `sklearn.metrics.average_precision_score(labels, scores)` for AUPRC
   - Compute prevalence = n_violations / (n_violations + n_controls)
   - Return dict: {"auprc": float, "prevalence": float, "n_violations": int, "n_controls": int}
   - Do NOT store full precision/recall arrays in the return dict (they go in result.json and would bloat it). Only AUPRC scalar.

3. **`run_pr_analysis`:** Orchestrator function matching `run_auroc_analysis` signature:
   - Takes eval_result_data, jumper_map, metric_keys, min_events_per_class=5
   - Reuse extract_events, filter_contaminated_events, stratify_by_r from event_extraction
   - For each r_value, for each metric_key, compute PR at each lookback j=1..r
   - Return nested dict:
     ```python
     {
         "config": {"min_events_per_class": N},
         "by_r_value": {
             r_val: {
                 "n_violations": N,
                 "n_controls": N,
                 "by_metric": {
                     metric_key: {
                         "auprc_by_lookback": [float, ...],  # length r
                         "prevalence": float,
                     }
                 }
             }
         }
     }
     ```

Create `tests/test_pr_curves.py`:

1. **test_compute_pr_perfect_separation:** Create synthetic events where violations have metric values clearly above controls. AUPRC should be close to 1.0.

2. **test_compute_pr_random_data:** Random metric values. AUPRC should be close to prevalence (no-skill baseline).

3. **test_compute_pr_insufficient_events:** Fewer than min_per_class events. Should return NaN AUPRC.

4. **test_run_pr_analysis_structure:** Build synthetic eval_result_data dict with generated, rule_outcome, failure_index arrays and a metric array. Run run_pr_analysis. Verify output has correct nested structure with by_r_value containing by_metric with auprc_by_lookback lists of correct length.

5. **test_score_direction_handling:** Create data where violations have LOWER metric values. Verify AUPRC is still computed correctly (score negation kicks in based on AUROC direction check).

Use the same synthetic event construction pattern as tests/test_auroc_horizon.py.
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -m pytest tests/test_pr_curves.py -x -v</automated>
  </verify>
  <done>
    - compute_pr_at_lookback returns correct AUPRC for perfect separation (~1.0) and random data (~prevalence)
    - run_pr_analysis returns correctly structured nested dict
    - Score direction is handled (works for both higher=violation and lower=violation metrics)
    - Insufficient events produce NaN gracefully
    - All tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Create PR curve visualization and integrate into render pipeline and HTML report</name>
  <files>
    src/visualization/pr_curves.py
    src/visualization/render.py
    src/reporting/single.py
    src/reporting/templates/single_report.html
    src/results/schema.py
  </files>
  <action>
**Create `src/visualization/pr_curves.py`:**

1. **`plot_pr_curves`:** Function matching the pattern of `plot_auroc_curves` in auroc.py:
   - Takes pr_results (dict mapping metric_name -> dict with auprc_by_lookback and prevalence), r_value, optional ax
   - Creates figure with AUPRC vs lookback distance (mirroring AUROC curve layout)
   - Plot each metric as a line with markers (same PALETTE as AUROC plots)
   - Add no-skill baseline as dashed horizontal line at mean prevalence rate
   - Add AUPRC annotation on each line at the peak value
   - Labels: x="Lookback distance j", y="AUPRC", title="AUPRC vs lookback distance (r={r_value})"
   - Returns plt.Figure

**Modify `src/visualization/render.py`:**

Add a new section after the AUROC curves section (around line 370), following the existing try/except pattern:

```python
# -- PR Curves (PRCL-01, PRCL-03) --
pred_horizon = result.get("metrics", {}).get("predictive_horizon", {})
pr_curves_data = result.get("metrics", {}).get("pr_curves", {})
if pr_curves_data:
    try:
        from src.visualization.pr_curves import plot_pr_curves

        for r_val_str, r_data in pr_curves_data.get("by_r_value", {}).items():
            by_metric = r_data.get("by_metric", {})
            if not by_metric:
                continue
            r_value = int(r_val_str)
            fig = plot_pr_curves(by_metric, r_value=r_value)
            paths = save_figure(fig, figures_dir, f"pr_curve_r{r_value}")
            generated_files.extend(paths)
            log.info("Generated: pr_curve_r%d", r_value)
    except Exception as e:
        log.warning("Failed to generate PR curves: %s", e)
```

**Modify `src/reporting/single.py`:**

1. In `_collect_figures`, add to the result dict initialization:
   ```python
   "pr_curve_figures": [],
   ```
   Add handling in the for loop:
   ```python
   elif name.startswith("pr_curve"):
       result["pr_curve_figures"].append({"title": title, "data_uri": data_uri})
   ```

2. In `generate_single_report`, extract pr_curves data from metrics and pass to template:
   ```python
   pr_curves = metrics.get("pr_curves")
   ```
   Add to template.render() call:
   ```python
   pr_curves=pr_curves,
   pr_curve_figures=figures.get("pr_curve_figures", []),
   ```

**Modify `src/reporting/templates/single_report.html`:**

1. Add CSS for collapsible sections (in the style block):
   ```css
   details.enrichment-section {
     margin-top: 2em;
     border: 1px solid #e0e0e0;
     border-radius: 4px;
     padding: 0;
   }
   details.enrichment-section summary {
     padding: 0.8em 1em;
     background: #f5f5f5;
     cursor: pointer;
     list-style: none;
   }
   details.enrichment-section summary::-webkit-details-marker { display: none; }
   details.enrichment-section summary::before {
     content: "+" ;
     font-weight: bold;
     margin-right: 0.5em;
   }
   details.enrichment-section[open] summary::before {
     content: "-" ;
   }
   details.enrichment-section .section-content {
     padding: 1em;
   }
   ```

2. Add PR curves section AFTER the AUROC Curves section, BEFORE Event-Aligned Plots:
   ```html
   <!-- PR Curves (PRCL-01, PRCL-03) -->
   {% if pr_curves or pr_curve_figures %}
   <details class="enrichment-section">
     <summary><strong>Precision-Recall Curves</strong></summary>
     <div class="section-content">
       {% if pr_curves %}
       <h3>AUPRC Summary</h3>
       <table>
         <thead>
           <tr>
             <th>r-value</th>
             <th>Metric</th>
             <th>AUPRC (by lookback j=1..r)</th>
             <th>Prevalence</th>
           </tr>
         </thead>
         <tbody>
           {% for r_str, r_data in pr_curves.by_r_value|dictsort %}
           {% for metric_name, m_data in r_data.by_metric|dictsort %}
           <tr>
             <td>{{ r_str }}</td>
             <td>{{ metric_name }}</td>
             <td>{{ m_data.auprc_by_lookback | map('round', 4) | join(', ') if m_data.auprc_by_lookback else 'N/A' }}</td>
             <td>{{ "%.4f"|format(m_data.prevalence) if m_data.prevalence is number else 'N/A' }}</td>
           </tr>
           {% endfor %}
           {% endfor %}
         </tbody>
       </table>
       {% endif %}

       {% if pr_curve_figures %}
       <h3>PR Curve Plots</h3>
       {% for fig in pr_curve_figures %}
       <div class="figure-container">
         <div class="figure-title">{{ fig.title }}</div>
         <img src="{{ fig.data_uri }}" alt="{{ fig.title }}">
       </div>
       {% endfor %}
       {% endif %}
     </div>
   </details>
   {% endif %}
   ```

**Modify `src/results/schema.py`:**

Add optional pr_curves validation block (backward-compatible, only validated when present):

After the null_model validation block, add:
```python
# Optional pr_curves validation (Phase 13, backward compatible)
if "metrics" in result and isinstance(result["metrics"], dict):
    pr_curves = result["metrics"].get("pr_curves")
    if pr_curves is not None:
        if not isinstance(pr_curves, dict):
            errors.append("metrics.pr_curves must be a dict")
        else:
            if "by_r_value" not in pr_curves:
                errors.append("metrics.pr_curves missing required block: by_r_value")
```
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -m pytest tests/test_pr_curves.py tests/test_reporting.py -x -v</automated>
  </verify>
  <done>
    - PR curve plot function creates valid matplotlib figure with metric lines and no-skill baseline
    - render.py generates pr_curve_rN.png files when pr_curves data exists in result.json
    - single.py collects pr_curve_* figures and passes to template
    - HTML template renders PR curves section as collapsible block after AUROC
    - schema.py validates pr_curves block when present, passes when absent (backward compatible)
    - All existing tests still pass
  </done>
</task>

</tasks>

<verification>
- `pytest tests/test_pr_curves.py -x -v` passes all PR curve tests
- `pytest tests/test_reporting.py -x -v` passes (existing + any new PR curve rendering tests)
- `pytest tests/ -x` full suite passes (no regressions)
- PR curves analysis mirrors AUROC pipeline structure exactly
- HTML report renders collapsible PR curves section
</verification>

<success_criteria>
- PRCL-01: PR curves and AUPRC computed per metric per lookback distance using same event extraction as AUROC
- PRCL-03 (partial): PR curves rendered in HTML report alongside AUROC plots in collapsible section
- Schema validates pr_curves block in result.json
- All tests pass including existing suite
</success_criteria>

<output>
After completion, create `.planning/phases/13-evaluation-enrichment/13-01-SUMMARY.md`
</output>
