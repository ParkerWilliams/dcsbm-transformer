---
phase: 09-reporting-and-math-verification
plan: 02
type: execute
wave: 2
depends_on: ["09-01"]
files_modified:
  - src/reporting/comparison.py
  - src/reporting/templates/comparison_report.html
  - src/reporting/__init__.py
  - tests/test_reporting.py
autonomous: true
requirements:
  - REPT-02
  - REPT-03

must_haves:
  truths:
    - "A comparison HTML report is generated across multiple experiments"
    - "The comparison report contains a scalar metrics table with sparklines showing relative values"
    - "Curve overlays are displayed as a grid of individual subplots with aligned axes"
    - "Config diff table shows all parameters with differing rows highlighted"
    - "An auto-generated summary verdict indicates which experiment outperforms on how many metrics"
    - "Each compared experiment includes its own reproduction block"
  artifacts:
    - path: "src/reporting/comparison.py"
      provides: "Multi-experiment comparison HTML report generator"
      exports: ["generate_comparison_report"]
    - path: "src/reporting/templates/comparison_report.html"
      provides: "Jinja2 template for comparison reports"
      contains: "sparkline"
  key_links:
    - from: "src/reporting/comparison.py"
      to: "src/visualization/render.py"
      via: "load_result_data for each experiment"
      pattern: "load_result_data"
    - from: "src/reporting/comparison.py"
      to: "src/reporting/embed.py"
      via: "embed_figure for sparklines and overlay figures"
      pattern: "embed_figure"
    - from: "src/reporting/comparison.py"
      to: "src/reporting/reproduction.py"
      via: "build_reproduction_block per experiment"
      pattern: "build_reproduction_block"
---

<objective>
Build the multi-experiment comparison HTML report with scalar metrics sparklines, curve overlay grids, config diff with highlighting, and auto-generated summary verdict.

Purpose: Delivers REPT-02 (comparison report across multiple experiments) and continues REPT-03 (reproduction block in every report). Uses the embed.py and reproduction.py utilities from Plan 01.
Output: `src/reporting/comparison.py`, comparison template, and additional tests.
</objective>

<execution_context>
@/root/.claude/get-shit-done/workflows/execute-plan.md
@/root/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-reporting-and-math-verification/09-CONTEXT.md
@.planning/phases/09-reporting-and-math-verification/09-RESEARCH.md
@.planning/phases/09-reporting-and-math-verification/09-01-SUMMARY.md
@src/reporting/embed.py
@src/reporting/reproduction.py
@src/reporting/single.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement comparison report generator with sparklines and config diff</name>
  <files>
    src/reporting/comparison.py
    src/reporting/templates/comparison_report.html
  </files>
  <action>
Create `src/reporting/comparison.py` with these functions:

1. **`_flatten_config(config: dict) -> dict[str, Any]`**: Recursively flatten nested config dict with dot-separated keys (e.g., `{"model": {"d_model": 128}}` -> `{"model.d_model": 128}`).

2. **`compute_config_diff(configs: list[dict]) -> list[dict]`**: For each flattened config parameter, create a row with `param`, `values` (list of values per experiment), and `differs` (bool: True if any value differs). Sort by param name.

3. **`generate_sparkline(values: list[float], width: int = 120, height: int = 30) -> str`**: Use matplotlib to create a tiny horizontal bar chart (one bar per experiment). Set figure size to width/height in pixels via dpi. Use the project's colorblind-safe palette for bars. Save to BytesIO as PNG, base64-encode, return as data URI. Close figure immediately to avoid memory leaks. Per user decision: "sparklines (mini bar charts) showing relative values across experiments."

4. **`compute_verdict(experiment_data: list[dict]) -> str`**: Compare scalar metrics across experiments. For each common metric (edge_compliance, rule_compliance, predictive_horizon), determine which experiment has the best value. Return a summary string like "Experiment B outperforms A on 3/5 metrics" per user decision. Handle ties and missing metrics gracefully.

5. **`generate_comparison_report(result_dirs: list[str | Path], output_path: str | Path | None = None) -> Path`**:
   - Load data for each experiment via `load_result_data()`
   - Build scalar metrics comparison: rows per metric, each cell has value, sparkline across experiments
   - Build config diff via `compute_config_diff()`
   - Build reproduction block per experiment via `build_reproduction_block()`
   - Compute auto-generated verdict via `compute_verdict()`
   - Collect figures from each experiment's figures/ directory; for curve overlays, embed the same-named figure from each experiment as a grid of subplots (per user decision: "grid of individual subplots with aligned axes -- not overlaid on single plot"). Use matplotlib to create a figure with N subplots (one per experiment), each showing the same figure type. Embed the composite as base64.
   - Create Jinja2 Environment with FileSystemLoader, render comparison_report.html
   - Write to output_path (default: first result_dir parent / "comparison_report.html")
   - Return output path

Create `src/reporting/templates/comparison_report.html` with academic style matching single report:
- Header: "Experiment Comparison" with experiment IDs listed
- Summary Verdict section (auto-generated text)
- Scalar Metrics Comparison table: rows = metric names, columns = experiment IDs + sparkline. Per user decision: "per-metric rows with sparklines (mini bar charts)"
- Config Diff table: all parameters, rows where values differ highlighted with `.diff-highlight` CSS class (per user decision: "full table with all parameters, rows where values differ highlighted")
- Curve Overlays: grid of figures organized by plot type, with experiment labels
- Reproduction blocks: one per experiment, each with copy button
- Same CSS as single report (serif fonts, academic style)
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -c "from src.reporting.comparison import generate_comparison_report, compute_config_diff, generate_sparkline; print('imports ok')"</automated>
  </verify>
  <done>comparison.py provides generate_comparison_report with sparklines, config diff, and verdict; template renders with academic styling and highlighted diff rows</done>
</task>

<task type="auto">
  <name>Task 2: Add comparison report tests and update exports</name>
  <files>
    tests/test_reporting.py
    src/reporting/__init__.py
  </files>
  <action>
Add tests to `tests/test_reporting.py` (append to existing file from Plan 01):

- `test_flatten_config`: Verify nested dict flattening produces dot-separated keys
- `test_config_diff_identical`: Two identical configs produce no `differs=True` rows
- `test_config_diff_different`: Two configs differing in one param produce exactly one `differs=True` row with correct values
- `test_sparkline_generation`: Call generate_sparkline([0.8, 0.9, 0.7]), verify returns string starting with `data:image/png;base64,`
- `test_verdict_generation`: Create mock experiment data with known metric values, verify verdict string contains correct winner identification
- `test_generate_comparison_report`: Create two minimal result_dirs with result.json files containing different configs, call generate_comparison_report(), verify HTML file created, contains "Experiment Comparison", "diff-highlight" class in HTML (for differing rows), sparkline data URIs, and reproduction blocks for both experiments
- `test_comparison_report_single_experiment`: Call with only one result_dir, verify it still generates without crashing (degenerate case)

Update `src/reporting/__init__.py` to also export `generate_comparison_report`.
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -m pytest tests/test_reporting.py -x -v 2>&1 | tail -30</automated>
  </verify>
  <done>All reporting tests pass (both Plan 01 and Plan 02 tests); generate_comparison_report produces HTML with sparklines, config diff highlighting, verdict, and per-experiment reproduction blocks</done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_reporting.py -x -v` -- all tests pass (both single and comparison)
2. `python -c "from src.reporting import generate_single_report, generate_comparison_report"` -- both import
3. Comparison report HTML contains sparklines, diff-highlighted rows, and reproduction blocks
</verification>

<success_criteria>
- generate_comparison_report() produces HTML with scalar metrics comparison table and sparklines (REPT-02)
- Config diff table shows all params with differing rows highlighted
- Auto-generated verdict section summarizes which experiment outperforms
- Curve overlays as grid of subplots, not overlaid on single plot
- Reproduction block included per experiment (REPT-03)
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/09-reporting-and-math-verification/09-02-SUMMARY.md`
</output>
