---
phase: 06-behavioral-evaluation-and-svd-collection
plan: 02
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/evaluation/behavioral.py
  - tests/test_behavioral.py
autonomous: true
requirements:
  - EVAL-01
  - EVAL-02
  - EVAL-03
  - EVAL-04

must_haves:
  truths:
    - "Each generation step is classified into one of 4 classes: (edge_valid=True, rule=NOT_APPLICABLE), (edge_valid=True, rule=FOLLOWED), (edge_valid=True, rule=VIOLATED), (edge_valid=False, rule=NOT_APPLICABLE/FOLLOWED/VIOLATED)"
    - "Edge validity is checked against the DCSBM adjacency CSR matrix at every step"
    - "Rule compliance is checked by tracking active jumper constraints: when model lands on a jumper vertex, record (encounter_step, r, target_block); at encounter_step+r, check block of generated token"
    - "failure_index records the first step where rule_outcome==VIOLATED, or -1 for fully-correct sequences"
    - "Generation continues after rule violations (no early stopping)"
  artifacts:
    - path: "src/evaluation/behavioral.py"
      provides: "RuleOutcome enum, classify_steps function, active jumper tracking"
      exports: ["RuleOutcome", "classify_steps"]
    - path: "tests/test_behavioral.py"
      provides: "Unit tests for edge validity, rule compliance, failure_index"
      min_lines: 120
  key_links:
    - from: "src/evaluation/behavioral.py"
      to: "src/graph/types.py"
      via: "GraphData.adjacency CSR for edge checks"
      pattern: "adjacency\\.indptr.*adjacency\\.indices"
    - from: "src/evaluation/behavioral.py"
      to: "src/graph/jumpers.py"
      via: "JumperInfo for rule compliance"
      pattern: "jumper_map|JumperInfo"
---

<objective>
Implement 4-class behavioral classification (edge valid/invalid x rule followed/violated/not-applicable) with failure_index annotation for generated sequences.

Purpose: Behavioral labels are one of the two core outputs of the fused evaluation pass. They classify every generation step so Phase 7 can compute AUROC at each lookback distance, comparing SVD metric values before violations vs. non-violations. The classifier must work on freely-generated sequences where the model may produce invalid edges or violate jumper rules.

Output: `src/evaluation/behavioral.py` with RuleOutcome enum and classify_steps function, `tests/test_behavioral.py` with comprehensive unit tests.
</objective>

<execution_context>
@/root/.claude/get-shit-done/workflows/execute-plan.md
@/root/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-behavioral-evaluation-and-svd-collection/06-RESEARCH.md
@src/graph/types.py
@src/graph/jumpers.py
@src/training/evaluate.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: RED - Create failing tests for behavioral classification</name>
  <files>
    tests/test_behavioral.py
    src/evaluation/behavioral.py
  </files>
  <action>
Create `src/evaluation/behavioral.py` with `RuleOutcome` IntEnum (NOT_APPLICABLE=0, FOLLOWED=1, VIOLATED=2) and a stub `classify_steps` function that raises `NotImplementedError`.

Create `tests/test_behavioral.py` with the following test classes using small synthetic graph fixtures:

**Test fixtures (shared across classes):**
- Small directed graph (5 vertices, known edges) as scipy.sparse.csr_matrix
- block_assignments array [0, 0, 1, 1, 2]
- jumper_map with vertex 0 as jumper: r=2, target_block=1

**TestEdgeValidity:**
- Sequence following only valid edges: all edge_valid=True
- Sequence with one invalid edge: that step edge_valid=False, others True
- Sequence where every step is invalid: all edge_valid=False
- Single-step sequence (2 tokens): edge validity checked for the one transition

**TestRuleCompliance:**
- Sequence that encounters jumper vertex 0, then at step+r lands in correct block: rule_outcome=FOLLOWED
- Sequence that encounters jumper, at step+r lands in wrong block: rule_outcome=VIOLATED
- Sequence with no jumper encounter: all rule_outcome=NOT_APPLICABLE
- Multiple jumper encounters in same sequence: each checked independently
- Jumper encountered too late for resolution (step+r >= seq_len): rule_outcome stays NOT_APPLICABLE for that constraint

**TestFailureIndex:**
- Fully correct sequence: failure_index = -1
- Rule violation at step 5: failure_index = 5 (the deadline step where violation is detected)
- Multiple violations: failure_index = first one
- Edge invalidity does NOT affect failure_index (failure_index is rule violations only)
- No jumper encounters: failure_index = -1

**TestBatchedClassification:**
- classify_steps with batch of 3 sequences: returns arrays of shape [3, L-1] for edge_valid and rule_outcome, [3] for failure_index
- Mixed batch: one correct, one with violation, one with no jumpers

**TestContinuationAfterViolation:**
- Sequence that violates at step 5, then encounters another jumper at step 8: both labels recorded
- Verify generation continues classifying after first violation (no early stopping)

All tests use `torch.tensor` for generated sequences and construct GraphData/jumper_map fixtures manually. Follow the existing pattern from `tests/test_training_evaluate.py` for graph fixture construction.

Run tests -- they must ALL FAIL (RED phase). Commit: `test(06-02): add failing tests for behavioral classification`
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -m pytest tests/test_behavioral.py -x 2>&1 | tail -5</automated>
    <manual>All tests fail with NotImplementedError (RED phase)</manual>
    <sampling_rate>run after this task commits, before next task begins</sampling_rate>
  </verify>
  <done>tests/test_behavioral.py exists with 15+ tests covering edge validity, rule compliance, failure_index, batching, and continuation after violation, all failing against stubs</done>
</task>

<task type="auto">
  <name>Task 2: GREEN - Implement behavioral classification with 4-class labeling</name>
  <files>
    src/evaluation/behavioral.py
    src/evaluation/__init__.py
  </files>
  <action>
Implement in `src/evaluation/behavioral.py`:

**RuleOutcome IntEnum:**
```python
class RuleOutcome(IntEnum):
    NOT_APPLICABLE = 0
    FOLLOWED = 1
    VIOLATED = 2
```

**classify_steps function:**
```python
def classify_steps(
    generated: torch.Tensor,       # [B, L]
    graph_data: GraphData,
    jumper_map: dict[int, JumperInfo],
) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
```

Implementation details:
1. Convert `generated` to numpy via `.cpu().numpy()` for efficient iteration
2. Get CSR arrays: `indptr = graph_data.adjacency.indptr`, `indices = graph_data.adjacency.indices`
3. For each sequence in the batch:
   a. Track active constraints: list of `(deadline_step, target_block)` tuples
   b. For each step t from 0 to L-2:
      - **Edge validity**: Look up neighbors via `indices[indptr[u]:indptr[u+1]]`, check if v is in neighbors
      - **Jumper encounter tracking**: If `u in jumper_map`, append `(t + jumper.r, jumper.target_block)` to active_constraints
      - **Rule compliance**: For each active constraint, if `t + 1 == deadline`, check `block_assignments[v] == target_block`. Set FOLLOWED or VIOLATED. Record first violation in failure_index.
      - Default rule_outcome = NOT_APPLICABLE if no constraint resolves at this step
4. Return `(edge_valid, rule_outcome, failure_index)` as numpy arrays

Key behavioral rules (from CONTEXT.md locked decisions):
- Jumper encounters are detected from the generated path itself (model lands on jumper vertex)
- "Not-applicable" for steps where no jumper rule is active at this deadline
- Generation continues after violations (failure_index tracks first but classification continues)
- failure_index = -1 for sequences with no rule violations

Update `src/evaluation/__init__.py` to export `RuleOutcome` and `classify_steps`.

Run all tests -- they must ALL PASS (GREEN phase). Commit: `feat(06-02): implement 4-class behavioral classification`
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -m pytest tests/test_behavioral.py -x -v</automated>
    <manual>All tests pass, verify rule_outcome values match RuleOutcome enum</manual>
    <sampling_rate>run after this task commits, before next task begins</sampling_rate>
  </verify>
  <done>All behavioral tests pass green. classify_steps correctly produces 4-class labels for all sequences in a batch. failure_index correctly identifies first rule violation. Full test suite has zero regressions.</done>
</task>

</tasks>

<verification>
1. `pytest tests/test_behavioral.py -v` -- all tests pass
2. `pytest tests/ -x` -- full suite passes with zero regressions
3. Edge validity matches CSR adjacency lookup for all test cases
4. Rule compliance correctly tracks jumper encounters from generated path
5. failure_index is -1 for correct sequences, first violation step otherwise
6. Batch processing works correctly across multiple sequences
</verification>

<success_criteria>
- RuleOutcome enum with NOT_APPLICABLE=0, FOLLOWED=1, VIOLATED=2 (EVAL-01)
- Edge validity checked via CSR adjacency at every step (EVAL-02)
- Rule compliance checks active jumper constraints at deadline steps (EVAL-03)
- failure_index annotation per sequence (EVAL-04)
- Classification continues after violations (no early stopping)
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/06-behavioral-evaluation-and-svd-collection/06-02-SUMMARY.md`
</output>
