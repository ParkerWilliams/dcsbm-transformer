---
phase: 13-evaluation-enrichment
plan: 02
type: execute
wave: 2
depends_on: [13-01]
files_modified:
  - src/analysis/calibration.py
  - src/visualization/calibration.py
  - src/visualization/render.py
  - src/reporting/single.py
  - src/reporting/templates/single_report.html
  - src/results/schema.py
  - tests/test_calibration.py
autonomous: true
requirements: [PRCL-02, PRCL-03]

must_haves:
  truths:
    - "compute_calibration returns fraction_of_positives, mean_predicted_value, ece, and bin_counts for a given metric at a lookback distance"
    - "run_calibration_analysis produces nested dict with by_metric containing ece_by_lookback per metric"
    - "Reliability diagram shows per-metric plot with lookback distances as colored lines plus perfect-calibration diagonal"
    - "Histogram of predicted probabilities appears below each reliability diagram"
    - "ECE summary table is rendered in the HTML report in a collapsible section"
    - "result.json calibration block validates through schema.py"
  artifacts:
    - src/analysis/calibration.py
    - src/visualization/calibration.py
    - tests/test_calibration.py
  key_links:
    - "calibration.py reuses extract_events, filter_contaminated_events, stratify_by_r from event_extraction.py"
    - "Pseudo-probability conversion uses empirical CDF (rank-based)"
    - "render.py calls plot_reliability_diagram and saves calibration_* figures"
    - "single.py collects calibration_* figures and passes to template"
    - "single_report.html renders Calibration section as collapsible details/summary block"
---

<objective>
Implement reliability diagrams (calibration curves) with Expected Calibration Error (ECE) for violation prediction, with visualization and HTML report integration.

Purpose: PRCL-02 requires reliability diagrams showing whether predicted probabilities match observed frequencies. PRCL-03 requires rendering in HTML reports.
Output: Calibration analysis module, reliability diagram plot function, report section, schema validation, and tests.
</objective>

<execution_context>
@/root/.claude/get-shit-done/workflows/execute-plan.md
@/root/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-evaluation-enrichment/13-RESEARCH.md
@.planning/phases/13-evaluation-enrichment/13-01-SUMMARY.md

@src/analysis/event_extraction.py
@src/analysis/auroc_horizon.py
@src/visualization/style.py
@src/visualization/render.py
@src/reporting/single.py
@src/reporting/templates/single_report.html
@src/results/schema.py

<interfaces>
<!-- Key types and contracts the executor needs. -->

From src/analysis/event_extraction.py:
```python
@dataclass(frozen=True, slots=True)
class AnalysisEvent:
    walk_idx: int
    encounter_step: int
    resolution_step: int
    r_value: int
    outcome: int
    is_first_violation: bool

def extract_events(generated, rule_outcome, failure_index, jumper_map) -> list[AnalysisEvent]
def filter_contaminated_events(events) -> tuple[list[AnalysisEvent], dict]
def stratify_by_r(events) -> dict[int, list[AnalysisEvent]]
```

From src/analysis/auroc_horizon.py:
```python
def auroc_from_groups(violations, controls) -> float  # for score direction check
```

From src/visualization/style.py:
```python
PALETTE = sns.color_palette("colorblind", n_colors=8)
def apply_style() -> None
def save_figure(fig, output_dir, name) -> tuple[Path, Path]
```

Pattern established by 13-01 for collapsible sections in template:
```html
<details class="enrichment-section">
  <summary><strong>Section Title</strong></summary>
  <div class="section-content">...</div>
</details>
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create calibration analysis module and tests</name>
  <files>
    src/analysis/calibration.py
    tests/test_calibration.py
  </files>
  <action>
Create `src/analysis/calibration.py`:

1. **`_metric_to_pseudo_probability`:** Convert raw SVD metric values to pseudo-probabilities using empirical CDF (rank-based). Given an array of scores, return rankdata(scores) / len(scores). This is non-parametric and doesn't require fitting (preserving pre-registration integrity per Out of Scope in REQUIREMENTS.md).

2. **`compute_calibration_at_lookback`:** Given violation_events, control_events, metric_array, r_value, lookback j, n_bins=10, min_per_class=2:
   - Gather metric values at lookback distance (same pattern as PR curves / AUROC)
   - If either class has fewer than min_per_class, return dict with NaN ECE
   - Determine score direction using auroc_from_groups (same as PR curves)
   - Convert metric values to pseudo-probabilities via `_metric_to_pseudo_probability`
   - Build binary labels (1=violation, 0=control)
   - Use `sklearn.calibration.calibration_curve(labels, prob_pred, n_bins=10, strategy='uniform')` to get fraction_of_positives, mean_predicted_value
   - Compute bin_counts using `np.histogram(prob_pred, bins=n_bins, range=(0,1))[0]`
   - Compute ECE: `sum(bin_counts[i]/total * |fraction_of_positives[i] - mean_predicted_value[i]|)` for non-empty bins
   - Return dict: {"ece": float, "fraction_of_positives": list, "mean_predicted_value": list, "bin_counts": list, "n_bins": 10}

3. **`run_calibration_analysis`:** Orchestrator function:
   - Takes eval_result_data, jumper_map, metric_keys, n_bins=10, min_events_per_class=5
   - Reuse extract_events, filter_contaminated_events, stratify_by_r
   - For each r_value, for each metric_key, compute calibration at each lookback j=1..r
   - Return nested dict:
     ```python
     {
         "config": {"n_bins": 10, "min_events_per_class": N, "probability_method": "empirical_cdf"},
         "by_r_value": {
             r_val: {
                 "n_violations": N,
                 "n_controls": N,
                 "by_metric": {
                     metric_key: {
                         "ece_by_lookback": [float, ...],  # length r
                     }
                 }
             }
         }
     }
     ```

Create `tests/test_calibration.py`:

1. **test_metric_to_pseudo_probability:** Verify rank-based conversion: [1, 2, 3, 4] -> [0.25, 0.5, 0.75, 1.0].

2. **test_compute_calibration_perfect:** Create perfectly calibrated synthetic data where P(violation|bin) matches the bin center. ECE should be near 0.

3. **test_compute_calibration_poor:** Create poorly calibrated data (e.g., all predictions at 0.5 but actual rate varies). ECE should be substantially above 0.

4. **test_compute_calibration_insufficient_events:** Fewer than min_per_class. Should return NaN ECE.

5. **test_run_calibration_analysis_structure:** Build synthetic eval_result_data. Run run_calibration_analysis. Verify output structure has by_r_value with by_metric containing ece_by_lookback lists of correct length.

6. **test_ece_range:** ECE should always be in [0, 1].
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -m pytest tests/test_calibration.py -x -v</automated>
  </verify>
  <done>
    - _metric_to_pseudo_probability correctly converts to rank-based probabilities
    - compute_calibration_at_lookback returns correct ECE for perfect (near 0) and poor calibration
    - run_calibration_analysis returns correctly structured nested dict
    - Insufficient events produce NaN gracefully
    - ECE is always in [0, 1]
    - All tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Create reliability diagram visualization and integrate into render pipeline and HTML report</name>
  <files>
    src/visualization/calibration.py
    src/visualization/render.py
    src/reporting/single.py
    src/reporting/templates/single_report.html
    src/results/schema.py
  </files>
  <action>
**Create `src/visualization/calibration.py`:**

1. **`plot_reliability_diagram`:** Takes calibration data for a single metric across lookback distances:
   - Args: metric_name (str), lookback_data (list of dicts with fraction_of_positives, mean_predicted_value, bin_counts, ece -- one per lookback), r_value (int)
   - Create figure with 2 subplots stacked vertically (gridspec: reliability diagram on top ~70%, histogram on bottom ~30%)
   - Top subplot (reliability diagram):
     - Plot perfect-calibration diagonal as gray dashed line
     - For each lookback j that has valid data, plot (mean_predicted_value vs fraction_of_positives) as a colored line using PALETTE colors
     - Legend with lookback distance labels and ECE values: "j={j} (ECE={ece:.3f})"
     - x-label: "Mean predicted probability", y-label: "Fraction of positives"
     - Title: f"Reliability Diagram: {metric_name}"
   - Bottom subplot (histogram of predicted probabilities):
     - Show bar chart of bin_counts for the first valid lookback (representative)
     - x-label: "Predicted probability", y-label: "Count"
   - Per CONTEXT.md: one reliability diagram per metric, lookback distances as separate colored lines

2. **`plot_ece_heatmap`:** Optional compact summary -- simple table-like figure showing ECE values in a grid (metrics x lookback distances). Use `plt.imshow` with annotated cells. This provides the "ECE summary table" as a visual.

**Modify `src/visualization/render.py`:**

Add a new section after the PR curves section added in Plan 01:

```python
# -- Calibration diagnostics (PRCL-02, PRCL-03) --
calibration_data = result.get("metrics", {}).get("calibration", {})
if calibration_data:
    try:
        from src.visualization.calibration import plot_reliability_diagram

        for r_val_str, r_data in calibration_data.get("by_r_value", {}).items():
            by_metric = r_data.get("by_metric", {})
            if not by_metric:
                continue
            r_value = int(r_val_str)

            for metric_key, m_data in by_metric.items():
                # Build lookback_data list for this metric
                lookback_data = []
                ece_list = m_data.get("ece_by_lookback", [])
                # Retrieve per-lookback calibration details if stored
                for j_idx, ece_val in enumerate(ece_list):
                    lookback_data.append({"ece": ece_val})

                try:
                    fig = plot_reliability_diagram(
                        metric_name=metric_key,
                        lookback_data=lookback_data,
                        r_value=r_value,
                    )
                    safe_name = metric_key.replace(".", "_")
                    paths = save_figure(fig, figures_dir, f"calibration_{safe_name}_r{r_value}")
                    generated_files.extend(paths)
                    log.info("Generated: calibration_%s_r%d", safe_name, r_value)
                except Exception as e:
                    log.warning("Failed to generate calibration_%s_r%d: %s", safe_name, r_value, e)

    except Exception as e:
        log.warning("Failed to generate calibration plots: %s", e)
```

**Modify `src/reporting/single.py`:**

1. In `_collect_figures`, add:
   ```python
   "calibration_figures": [],
   ```
   In the for loop:
   ```python
   elif name.startswith("calibration"):
       result["calibration_figures"].append({"title": title, "data_uri": data_uri})
   ```

2. In `generate_single_report`, extract calibration data and pass to template:
   ```python
   calibration = metrics.get("calibration")
   ```
   Add to template.render():
   ```python
   calibration=calibration,
   calibration_figures=figures.get("calibration_figures", []),
   ```

**Modify `src/reporting/templates/single_report.html`:**

Add calibration section AFTER the PR Curves section (added in Plan 01), BEFORE Event-Aligned Plots:

```html
<!-- Calibration Diagnostics (PRCL-02, PRCL-03) -->
{% if calibration or calibration_figures %}
<details class="enrichment-section">
  <summary><strong>Calibration Diagnostics</strong></summary>
  <div class="section-content">
    {% if calibration %}
    <h3>ECE Summary</h3>
    <table>
      <thead>
        <tr>
          <th>r-value</th>
          <th>Metric</th>
          <th>ECE (by lookback j=1..r)</th>
        </tr>
      </thead>
      <tbody>
        {% for r_str, r_data in calibration.by_r_value|dictsort %}
        {% for metric_name, m_data in r_data.by_metric|dictsort %}
        <tr>
          <td>{{ r_str }}</td>
          <td>{{ metric_name }}</td>
          <td>{{ m_data.ece_by_lookback | map('round', 4) | join(', ') if m_data.ece_by_lookback else 'N/A' }}</td>
        </tr>
        {% endfor %}
        {% endfor %}
      </tbody>
    </table>
    {% endif %}

    {% if calibration_figures %}
    <h3>Reliability Diagrams</h3>
    {% for fig in calibration_figures %}
    <div class="figure-container">
      <div class="figure-title">{{ fig.title }}</div>
      <img src="{{ fig.data_uri }}" alt="{{ fig.title }}">
    </div>
    {% endfor %}
    {% endif %}
  </div>
</details>
{% endif %}
```

**Modify `src/results/schema.py`:**

Add optional calibration validation block after the pr_curves validation:

```python
# Optional calibration validation (Phase 13, backward compatible)
if "metrics" in result and isinstance(result["metrics"], dict):
    calibration = result["metrics"].get("calibration")
    if calibration is not None:
        if not isinstance(calibration, dict):
            errors.append("metrics.calibration must be a dict")
        else:
            if "by_r_value" not in calibration:
                errors.append("metrics.calibration missing required block: by_r_value")
            cal_config = calibration.get("config", {})
            if isinstance(cal_config, dict) and "n_bins" not in cal_config:
                errors.append("metrics.calibration.config missing field: n_bins")
```
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -m pytest tests/test_calibration.py tests/test_reporting.py -x -v</automated>
  </verify>
  <done>
    - Reliability diagram plots show per-metric calibration curves with lookback distances as colored lines
    - Perfect-calibration diagonal is visible
    - Histogram of predicted probabilities appears below reliability diagram
    - render.py generates calibration_* figures when calibration data exists
    - single.py collects calibration_* figures and passes to template
    - HTML template renders Calibration section as collapsible block after PR curves
    - schema.py validates calibration block when present, passes when absent
    - All existing tests still pass
  </done>
</task>

</tasks>

<verification>
- `pytest tests/test_calibration.py -x -v` passes all calibration tests
- `pytest tests/test_reporting.py -x -v` passes
- `pytest tests/ -x` full suite passes (no regressions)
- Reliability diagrams show correct calibration characteristics
- HTML report renders collapsible Calibration section
</verification>

<success_criteria>
- PRCL-02: Reliability diagrams with ECE generated for violation prediction
- PRCL-03 (complete): Calibration diagnostics rendered in HTML reports alongside AUROC and PR curves
- Schema validates calibration block in result.json
- All tests pass including existing suite
</success_criteria>

<output>
After completion, create `.planning/phases/13-evaluation-enrichment/13-02-SUMMARY.md`
</output>
