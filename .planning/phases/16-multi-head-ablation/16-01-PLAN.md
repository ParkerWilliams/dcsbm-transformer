---
phase: 16-multi-head-ablation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/config/experiment.py
  - src/model/attention.py
  - src/model/block.py
  - src/model/transformer.py
  - src/model/types.py
  - tests/test_multi_head.py
autonomous: true
requirements: [MHAD-01]
---

must_haves:
  truths:
    - "CausalSelfAttention accepts n_heads parameter and computes per-head QK^T of shape [B, n_heads, T, T]"
    - "d_head = d_model // n_heads is used as the per-head dimension for Q, K, V projections"
    - "Scaling factor is 1/sqrt(d_head) not 1/sqrt(d_model) -- each head scales by its own dimension"
    - "Causal mask is applied independently to each head's attention scores"
    - "AttentionInternals stores qkt [B, n_heads, T, T], attention_weights [B, n_heads, T, T], values [B, n_heads, T, d_head]"
    - "ForwardOutput stores qkt [B, n_layers, n_heads, T, T], attention_weights [B, n_layers, n_heads, T, T], values [B, n_layers, n_heads, T, d_head]"
    - "TransformerLM accepts n_heads, passes it through TransformerBlock to CausalSelfAttention"
    - "get_wvwo() returns [n_layers, n_heads, d_head, d_model] tensor for per-head OV circuits"
    - "ExperimentConfig validates n_heads in (1, 2, 4) and d_model % n_heads == 0"
    - "create_model() passes n_heads from config to TransformerLM constructor"
    - "Single-head (n_heads=1) produces identical forward pass results as v1.0 for same weights (backward compatible numerics)"
    - "All existing tests pass after the multi-head changes"
  artifacts:
    - src/config/experiment.py
    - src/model/attention.py
    - src/model/block.py
    - src/model/transformer.py
    - src/model/types.py
    - tests/test_multi_head.py
  key_links:
    - "ExperimentConfig.__post_init__ validates n_heads in (1, 2, 4) instead of n_heads == 1"
    - "CausalSelfAttention.__init__ takes n_heads parameter with default 1"
    - "TransformerBlock.__init__ takes n_heads parameter and passes to CausalSelfAttention"
    - "TransformerLM.__init__ takes n_heads parameter and passes to TransformerBlock"
    - "TransformerLM.forward() stacks per-layer outputs into [B, n_layers, n_heads, T, T]"
    - "TransformerLM.get_wvwo() iterates over heads within each layer for per-head OV circuit"
    - "create_model() reads config.model.n_heads and passes to TransformerLM"

<objective>
Extend the transformer architecture to support multi-head attention (1h/2h/4h) with per-head QK^T extraction.

Purpose: MHAD-01 requires the transformer to accept n_heads = 1, 2, or 4 with d_k held constant at 128 (d_model scales as n_heads * d_k), and per-head QK^T matrices must be extractable for SVD analysis.
Output: Modified model layer (attention, block, transformer, types) with n_heads support, updated config validation, and comprehensive tests.
</objective>

<execution_context>
@/root/.claude/get-shit-done/workflows/execute-plan.md
@/root/.claude/get-shit-done/references/tdd.md
@/root/.claude/get-shit-done/references/checkpoints.md
@/root/.claude/get-shit-done/references/model-profile-resolution.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16-multi-head-ablation/16-CONTEXT.md
@.planning/phases/16-multi-head-ablation/16-RESEARCH.md

@src/config/experiment.py
@src/model/attention.py
@src/model/block.py
@src/model/transformer.py
@src/model/types.py

<interfaces>
<!-- Key types and contracts the executor needs. -->

Current CausalSelfAttention (src/model/attention.py):
```python
class CausalSelfAttention(nn.Module):
    def __init__(self, d_model: int, max_seq_len: int, dropout: float = 0.0):
        # W_q, W_k, W_v, W_o: nn.Linear(d_model, d_model, bias=False)
        # causal_mask: [max_seq_len, max_seq_len] lower triangular bool

    def forward(self, x: [B, T, D], extract: bool) -> (y: [B, T, D], AttentionInternals | None):
        # Q = W_q(x), K = W_k(x), V = W_v(x)  -- all [B, T, D]
        # qkt_raw = (Q @ K^T) * (1/sqrt(d_model))  -- [B, T, T]
        # att_weights = softmax(masked qkt_raw)  -- [B, T, T]
        # y = W_o(att_weights_dropped @ V)  -- [B, T, D]
```

Current AttentionInternals (src/model/types.py):
```python
@dataclass
class AttentionInternals:
    qkt: torch.Tensor           # [B, T, T]
    attention_weights: torch.Tensor  # [B, T, T]
    values: torch.Tensor        # [B, T, D]
```

Current ForwardOutput (src/model/types.py):
```python
@dataclass
class ForwardOutput:
    logits: torch.Tensor              # [B, T, vocab_size]
    qkt: torch.Tensor | None          # [B, n_layers, T, T]
    attention_weights: torch.Tensor | None  # [B, n_layers, T, T]
    values: torch.Tensor | None       # [B, n_layers, T, D]
    residual_stream: torch.Tensor | None    # [B, T, n_layers+1, D]
    residual_norms: torch.Tensor | None     # [B, T, n_layers+1]
```

Current TransformerLM (src/model/transformer.py):
```python
class TransformerLM(nn.Module):
    def __init__(self, vocab_size, d_model, n_layers, max_seq_len, dropout=0.0):
        self.blocks = nn.ModuleList([TransformerBlock(d_model, max_seq_len, dropout) ...])

    def forward(self, idx, mode) -> ForwardOutput:
        # Stacks: qkt = torch.stack(all_qkt, dim=1)  # [B, n_layers, T, T]

    def get_wvwo(self) -> Tensor:  # [n_layers, D, D]
        # block.attention.W_v.weight.T @ block.attention.W_o.weight
```

Current ExperimentConfig validation (src/config/experiment.py):
```python
if self.model.n_heads != 1:
    raise ValueError("n_heads must be exactly 1 (single-head constraint)")
```

Target shapes after changes:
- AttentionInternals.qkt: [B, n_heads, T, T]
- AttentionInternals.attention_weights: [B, n_heads, T, T]
- AttentionInternals.values: [B, n_heads, T, d_head]
- ForwardOutput.qkt: [B, n_layers, n_heads, T, T]
- ForwardOutput.attention_weights: [B, n_layers, n_heads, T, T]
- ForwardOutput.values: [B, n_layers, n_heads, T, d_head]
- get_wvwo(): [n_layers, n_heads, d_head, d_model]
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update types and config</name>
  <files>
    src/model/types.py
    src/config/experiment.py
  </files>
  <action>
**src/model/types.py:**

Update docstrings in AttentionInternals to reflect new shapes:
```python
@dataclass
class AttentionInternals:
    """Per-layer attention extraction results. All tensors are detached.

    Attributes:
        qkt: Raw QK^T with zero-filled causal mask. Shape [B, n_heads, T, T].
        attention_weights: Softmax attention weights A. Shape [B, n_heads, T, T].
        values: Value matrix V before output projection. Shape [B, n_heads, T, d_head].
    """
    qkt: torch.Tensor           # [B, n_heads, T, T]
    attention_weights: torch.Tensor  # [B, n_heads, T, T]
    values: torch.Tensor        # [B, n_heads, T, d_head]
```

Update docstrings in ForwardOutput to reflect new shapes:
```python
@dataclass
class ForwardOutput:
    """Structured output from TransformerLM.forward().

    Always contains logits. Other fields populated based on ExtractionMode.

    Attributes:
        logits: Next-token prediction logits. Shape [B, T, vocab_size].
        qkt: Stacked QK^T across layers and heads, zero-filled. Shape [B, n_layers, n_heads, T, T].
        attention_weights: Stacked attention weights. Shape [B, n_layers, n_heads, T, T].
        values: Stacked value matrices. Shape [B, n_layers, n_heads, T, d_head].
        residual_stream: Per-layer residual states. Shape [B, T, n_layers+1, D].
        residual_norms: L2 norms of residual states. Shape [B, T, n_layers+1].
    """
    logits: torch.Tensor
    qkt: torch.Tensor | None = None       # [B, n_layers, n_heads, T, T]
    attention_weights: torch.Tensor | None = None  # [B, n_layers, n_heads, T, T]
    values: torch.Tensor | None = None    # [B, n_layers, n_heads, T, d_head]
    residual_stream: torch.Tensor | None = None
    residual_norms: torch.Tensor | None = None
```

**src/config/experiment.py:**

Replace the n_heads validation in `ExperimentConfig.__post_init__`:
```python
# OLD:
if self.model.n_heads != 1:
    raise ValueError("n_heads must be exactly 1 (single-head constraint)")

# NEW:
if self.model.n_heads not in (1, 2, 4):
    raise ValueError(f"n_heads must be 1, 2, or 4, got {self.model.n_heads}")
if self.model.d_model % self.model.n_heads != 0:
    raise ValueError(
        f"d_model ({self.model.d_model}) must be divisible by "
        f"n_heads ({self.model.n_heads})"
    )
```

Remove the "locked: always 1" comment from ModelConfig.n_heads and update to:
```python
n_heads: int = 1  # 1, 2, or 4 (multi-head ablation support)
```
  </action>
</task>

<task type="auto">
  <name>Task 2: Implement multi-head CausalSelfAttention</name>
  <files>
    src/model/attention.py
  </files>
  <action>
Rewrite CausalSelfAttention to support n_heads parameter:

```python
class CausalSelfAttention(nn.Module):
    """Multi-head causal self-attention with per-head QK^T extraction.

    Supports n_heads = 1 (backward compatible with v1.0), 2, or 4.
    Q, K, V projections are [B, T, d_model] -> reshaped to [B, n_heads, T, d_head].
    Per-head QK^T is [B, n_heads, T, T] for SVD analysis.

    Args:
        d_model: Model dimension (must be divisible by n_heads).
        n_heads: Number of attention heads (1, 2, or 4).
        max_seq_len: Maximum sequence length for causal mask buffer.
        dropout: Dropout rate for attention weights and residual path.
    """

    def __init__(self, d_model: int, n_heads: int, max_seq_len: int, dropout: float = 0.0):
        super().__init__()
        assert d_model % n_heads == 0, f"d_model ({d_model}) must be divisible by n_heads ({n_heads})"
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_head = d_model // n_heads

        # Projection matrices (d_model -> d_model, same as before)
        self.W_q = nn.Linear(d_model, d_model, bias=False)
        self.W_k = nn.Linear(d_model, d_model, bias=False)
        self.W_v = nn.Linear(d_model, d_model, bias=False)
        self.W_o = nn.Linear(d_model, d_model, bias=False)

        # Dropout layers
        self.attn_dropout = nn.Dropout(dropout)
        self.resid_dropout = nn.Dropout(dropout)

        # Pre-computed causal mask
        self.register_buffer(
            "causal_mask",
            torch.tril(torch.ones(max_seq_len, max_seq_len, dtype=torch.bool)),
        )

    def forward(
        self, x: torch.Tensor, extract: bool = False
    ) -> tuple[torch.Tensor, AttentionInternals | None]:
        B, T, D = x.shape

        # Project Q, K, V: [B, T, d_model] -> [B, T, d_model]
        q = self.W_q(x)
        k = self.W_k(x)
        v = self.W_v(x)

        # Reshape to multi-head: [B, T, d_model] -> [B, T, n_heads, d_head] -> [B, n_heads, T, d_head]
        q = q.view(B, T, self.n_heads, self.d_head).transpose(1, 2)
        k = k.view(B, T, self.n_heads, self.d_head).transpose(1, 2)
        v = v.view(B, T, self.n_heads, self.d_head).transpose(1, 2)

        # Scaled dot-product: per-head QK^T / sqrt(d_head)
        scale = 1.0 / math.sqrt(self.d_head)
        qkt_raw = (q @ k.transpose(-2, -1)) * scale  # [B, n_heads, T, T]

        # Causal mask
        mask = self.causal_mask[:T, :T]  # [T, T]
        att_scores = qkt_raw.masked_fill(~mask, float("-inf"))
        att_weights = F.softmax(att_scores, dim=-1)  # [B, n_heads, T, T]
        att_weights_dropped = self.attn_dropout(att_weights)

        # Compute attention output per head: [B, n_heads, T, d_head]
        attn_out = att_weights_dropped @ v

        # Concatenate heads: [B, n_heads, T, d_head] -> [B, T, n_heads, d_head] -> [B, T, d_model]
        attn_out = attn_out.transpose(1, 2).contiguous().view(B, T, D)

        # Output projection + dropout
        y = self.resid_dropout(self.W_o(attn_out))  # [B, T, D]

        if extract:
            qkt_target = qkt_raw.masked_fill(~mask, 0.0).detach()  # [B, n_heads, T, T]
            return y, AttentionInternals(
                qkt=qkt_target,
                attention_weights=att_weights.detach(),  # [B, n_heads, T, T]
                values=v.detach(),  # [B, n_heads, T, d_head]
            )

        return y, None
```

Key changes from v1.0:
1. Added n_heads parameter (default 1 for backward compat)
2. d_head = d_model // n_heads
3. Scale factor uses d_head not d_model
4. Q, K, V are reshaped to [B, n_heads, T, d_head]
5. QK^T is [B, n_heads, T, T] (per-head)
6. Heads are concatenated before W_o projection
7. Extract returns per-head tensors

**CRITICAL**: When n_heads=1, d_head = d_model, so the scale factor is 1/sqrt(d_model) -- same as v1.0. The reshape is a no-op in terms of data (just adds a size-1 dimension). Output is numerically identical.
  </action>
</task>

<task type="auto">
  <name>Task 3: Update TransformerBlock and TransformerLM</name>
  <files>
    src/model/block.py
    src/model/transformer.py
  </files>
  <action>
**src/model/block.py:**

Add n_heads parameter to TransformerBlock:
```python
class TransformerBlock(nn.Module):
    def __init__(self, d_model: int, n_heads: int, max_seq_len: int, dropout: float = 0.0):
        super().__init__()
        self.ln_1 = nn.LayerNorm(d_model)
        self.ln_2 = nn.LayerNorm(d_model)
        self.attention = CausalSelfAttention(d_model, n_heads, max_seq_len, dropout)
        self.mlp = nn.Sequential(...)  # unchanged
```

**src/model/transformer.py:**

1. Add n_heads to TransformerLM.__init__:
```python
def __init__(self, vocab_size, d_model, n_layers, n_heads, max_seq_len, dropout=0.0):
    self.n_heads = n_heads
    self.d_head = d_model // n_heads
    self.blocks = nn.ModuleList([
        TransformerBlock(d_model, n_heads, max_seq_len, dropout)
        for _ in range(n_layers)
    ])
```

2. Update forward() -- stacking now includes head dimension:
```python
if extract and all_qkt:
    qkt = torch.stack(all_qkt, dim=1)  # [B, n_layers, n_heads, T, T]
    attention_weights = torch.stack(all_attn, dim=1)  # [B, n_layers, n_heads, T, T]
    values = torch.stack(all_values, dim=1)  # [B, n_layers, n_heads, T, d_head]
```
NOTE: all_qkt entries are [B, n_heads, T, T], so stacking on dim=1 gives [B, n_layers, n_heads, T, T]. This is correct.

3. Update get_wvwo() for per-head OV circuits:
```python
def get_wvwo(self) -> torch.Tensor:
    """Return stacked per-head WvWo weight product for all layers.

    For each layer and each head, computes the OV circuit:
        W_v[:, h*d_head:(h+1)*d_head].T @ W_o[h*d_head:(h+1)*d_head, :]
    This gives a [d_head, d_model] matrix per head per layer.

    Returns:
        Tensor of shape [n_layers, n_heads, d_head, d_model], detached.
    """
    layers = []
    for block in self.blocks:
        heads = []
        Wv = block.attention.W_v.weight  # [d_model, d_model]
        Wo = block.attention.W_o.weight  # [d_model, d_model]
        for h in range(self.n_heads):
            start = h * self.d_head
            end = (h + 1) * self.d_head
            # W_v extracts head h: Wv[:, start:end].T gives [d_head, d_model]
            # But nn.Linear stores [out, in], so Wv.weight is [d_model, d_model]
            # Wv.weight.T is the actual projection matrix
            # Per-head V projection: Wv.weight.T[:, start:end] is wrong
            # Actually: W_v.weight is [d_model_out, d_model_in] = [d_model, d_model]
            # x @ W_v.weight.T gives [B, T, d_model]
            # After reshape, head h gets columns [start:end]
            # So the per-head V projection matrix is Wv[start:end, :].T = Wv[start:end, :].T
            # = [d_model, d_head].T = [d_head, d_model]
            Wv_h = Wv[start:end, :]  # [d_head, d_model]
            # W_o: nn.Linear(d_model, d_model), weight is [d_model, d_model]
            # x @ W_o.weight.T: input is [B, T, d_model], output is [B, T, d_model]
            # Head h's contribution to input of W_o is at positions [start:end]
            Wo_h = Wo[:, start:end]  # [d_model, d_head]
            # OV circuit: Wv_h.T @ Wo_h = [d_model, d_head] @ [d_head, d_model]... wait
            # Let's be precise:
            # V_full = x @ Wv.T  (nn.Linear: output = input @ weight.T)
            # V_h = V_full[:, :, start:end]  -- [B, T, d_head]
            # After attention: attn_out_h = A_h @ V_h  -- [B, T, d_head]
            # Concatenated: attn_out = [attn_out_0, ..., attn_out_{H-1}]  -- [B, T, d_model]
            # Output: y = attn_out @ Wo.T  -- [B, T, d_model]
            # Per-head contribution: y_h = attn_out_h @ Wo.T[start:end, :]
            # = attn_out_h @ Wo[:, start:end].T
            # OV circuit for head h: Wv_h.T @ Wo[:, start:end].T
            # = [d_model, d_head] @ [d_head, d_model] -> but Wv_h is [d_head, d_model]
            # Wv_h.T is [d_model, d_head]
            # Actually the full flow for a single head:
            #   input [d_model] -> Wv.T -> [d_model] -> select [start:end] -> [d_head]
            #   -> multiply by attention -> [d_head]
            #   -> placed at [start:end] in concat -> [d_model]
            #   -> Wo.T -> [d_model]
            # The value projection for head h from d_model input:
            #   v_h = x @ Wv.T[:, start:end] = x @ Wv[start:end, :].T
            # The output projection contribution for head h:
            #   y_h = concat_h @ Wo.T where concat_h has head h at [start:end]
            #   = v_h @ Wo.T[start:end, :] = v_h @ Wo[:, start:end].T
            # So OV_h = Wv[start:end, :].T @ Wo[:, start:end].T
            #         = [d_model, d_head] @ [d_head, d_model]
            #         = [d_model, d_model]
            # For SVD: this is a d_model x d_model matrix (same as single-head!)
            wvwo_h = Wv_h.T @ Wo_h.T  # [d_model, d_model]
            heads.append(wvwo_h)
        layers.append(torch.stack(heads))  # [n_heads, d_model, d_model]
    return torch.stack(layers).detach()  # [n_layers, n_heads, d_model, d_model]
```

IMPORTANT correction: The per-head WvWo is actually [d_model, d_model], not [d_head, d_model]. This is because the value projection maps from d_model to d_head, and the output projection maps from d_head back to d_model. The OV circuit maps d_model -> d_model through the head's bottleneck.

4. Update create_model() to pass n_heads:
```python
def create_model(config: "ExperimentConfig") -> TransformerLM:
    from src.config.experiment import ExperimentConfig
    return TransformerLM(
        vocab_size=config.graph.n,
        d_model=config.model.d_model,
        n_layers=config.model.n_layers,
        n_heads=config.model.n_heads,
        max_seq_len=config.training.w,
        dropout=config.model.dropout,
    )
```

5. Update W_o initialization scaling (must still reference block.attention.W_o):
The existing initialization loop accesses `block.attention.W_o.weight` -- this is unchanged since W_o is still a single [d_model, d_model] linear layer.
  </action>
</task>

<task type="auto">
  <name>Task 4: Write comprehensive multi-head tests</name>
  <files>
    tests/test_multi_head.py
  </files>
  <action>
Create `tests/test_multi_head.py` with the following test cases:

```python
"""Tests for multi-head attention support (Phase 16: MHAD-01)."""

import pytest
import torch
import numpy as np
from src.config.experiment import ExperimentConfig, ModelConfig, GraphConfig, TrainingConfig
from src.model.attention import CausalSelfAttention
from src.model.block import TransformerBlock
from src.model.transformer import TransformerLM, create_model
from src.model.types import ExtractionMode, ForwardOutput, AttentionInternals


class TestConfigValidation:
    """Config accepts n_heads 1, 2, 4 and rejects others."""

    def test_n_heads_1_accepted(self):
        config = ExperimentConfig(model=ModelConfig(n_heads=1, d_model=128))
        assert config.model.n_heads == 1

    def test_n_heads_2_accepted(self):
        config = ExperimentConfig(model=ModelConfig(n_heads=2, d_model=256))
        assert config.model.n_heads == 2

    def test_n_heads_4_accepted(self):
        config = ExperimentConfig(model=ModelConfig(n_heads=4, d_model=512))
        assert config.model.n_heads == 4

    def test_n_heads_3_rejected(self):
        with pytest.raises(ValueError, match="n_heads must be 1, 2, or 4"):
            ExperimentConfig(model=ModelConfig(n_heads=3, d_model=384))

    def test_n_heads_8_rejected(self):
        with pytest.raises(ValueError, match="n_heads must be 1, 2, or 4"):
            ExperimentConfig(model=ModelConfig(n_heads=8, d_model=1024))

    def test_d_model_not_divisible_rejected(self):
        with pytest.raises(ValueError, match="divisible"):
            ExperimentConfig(model=ModelConfig(n_heads=2, d_model=129))


class TestCausalSelfAttentionMultiHead:
    """CausalSelfAttention with n_heads > 1 produces correct shapes."""

    @pytest.fixture
    def device(self):
        return torch.device("cpu")

    @pytest.mark.parametrize("n_heads,d_model", [(1, 128), (2, 256), (4, 512)])
    def test_output_shape(self, n_heads, d_model, device):
        B, T = 2, 16
        attn = CausalSelfAttention(d_model, n_heads, max_seq_len=T).to(device)
        x = torch.randn(B, T, d_model, device=device)
        y, _ = attn(x, extract=False)
        assert y.shape == (B, T, d_model)

    @pytest.mark.parametrize("n_heads,d_model", [(1, 128), (2, 256), (4, 512)])
    def test_extraction_shapes(self, n_heads, d_model, device):
        B, T = 2, 16
        d_head = d_model // n_heads
        attn = CausalSelfAttention(d_model, n_heads, max_seq_len=T).to(device)
        x = torch.randn(B, T, d_model, device=device)
        y, internals = attn(x, extract=True)
        assert internals is not None
        assert internals.qkt.shape == (B, n_heads, T, T)
        assert internals.attention_weights.shape == (B, n_heads, T, T)
        assert internals.values.shape == (B, n_heads, T, d_head)

    def test_causal_mask_per_head(self, device):
        """Each head respects causal mask independently."""
        n_heads, d_model, T = 2, 256, 8
        attn = CausalSelfAttention(d_model, n_heads, max_seq_len=T).to(device)
        x = torch.randn(1, T, d_model, device=device)
        _, internals = attn(x, extract=True)
        # Upper triangle of QK^T should be zero (zero-filled for SVD target)
        for h in range(n_heads):
            qkt_h = internals.qkt[0, h]  # [T, T]
            upper = torch.triu(qkt_h, diagonal=1)
            assert torch.all(upper == 0), f"Head {h} has non-zero upper triangle"

    def test_attention_weights_sum_to_one(self, device):
        """Attention weights sum to 1 along last dim per head."""
        n_heads, d_model, T = 4, 512, 8
        attn = CausalSelfAttention(d_model, n_heads, max_seq_len=T).to(device)
        x = torch.randn(1, T, d_model, device=device)
        _, internals = attn(x, extract=True)
        for h in range(n_heads):
            row_sums = internals.attention_weights[0, h].sum(dim=-1)
            assert torch.allclose(row_sums, torch.ones_like(row_sums), atol=1e-5)

    def test_single_head_scale_factor(self, device):
        """Single-head uses 1/sqrt(d_model) scale (backward compat)."""
        d_model = 128
        attn = CausalSelfAttention(d_model, n_heads=1, max_seq_len=8).to(device)
        assert attn.d_head == d_model  # d_head == d_model for single head

    def test_multi_head_scale_factor(self, device):
        """Multi-head uses 1/sqrt(d_head) scale."""
        d_model, n_heads = 256, 2
        attn = CausalSelfAttention(d_model, n_heads, max_seq_len=8).to(device)
        assert attn.d_head == 128  # d_k = 128 constant

    def test_heads_produce_different_qkt(self, device):
        """Different heads should generally produce different QK^T matrices."""
        n_heads, d_model, T = 2, 256, 16
        attn = CausalSelfAttention(d_model, n_heads, max_seq_len=T).to(device)
        x = torch.randn(1, T, d_model, device=device)
        _, internals = attn(x, extract=True)
        qkt_0 = internals.qkt[0, 0]
        qkt_1 = internals.qkt[0, 1]
        # After random init, heads should differ
        assert not torch.allclose(qkt_0, qkt_1, atol=1e-3)

    def test_internals_are_detached(self, device):
        """Extracted internals should be detached from computation graph."""
        attn = CausalSelfAttention(128, 1, max_seq_len=8).to(device)
        x = torch.randn(1, 8, 128, device=device, requires_grad=True)
        _, internals = attn(x, extract=True)
        assert not internals.qkt.requires_grad
        assert not internals.attention_weights.requires_grad
        assert not internals.values.requires_grad


class TestTransformerLMMultiHead:
    """TransformerLM with multi-head support."""

    @pytest.fixture
    def device(self):
        return torch.device("cpu")

    @pytest.mark.parametrize("n_heads,d_model", [(1, 128), (2, 256), (4, 512)])
    def test_forward_output_shapes(self, n_heads, d_model, device):
        B, T, n_layers, vocab = 2, 16, 2, 100
        d_head = d_model // n_heads
        model = TransformerLM(vocab, d_model, n_layers, n_heads, T).to(device)
        idx = torch.randint(0, vocab, (B, T), device=device)
        out = model(idx, mode=ExtractionMode.SVD_TARGETS)
        assert out.logits.shape == (B, T, vocab)
        assert out.qkt.shape == (B, n_layers, n_heads, T, T)
        assert out.attention_weights.shape == (B, n_layers, n_heads, T, T)
        assert out.values.shape == (B, n_layers, n_heads, T, d_head)

    @pytest.mark.parametrize("n_heads,d_model", [(1, 128), (2, 256), (4, 512)])
    def test_get_wvwo_shape(self, n_heads, d_model, device):
        model = TransformerLM(100, d_model, 2, n_heads, 16).to(device)
        wvwo = model.get_wvwo()
        assert wvwo.shape == (2, n_heads, d_model, d_model)

    def test_create_model_passes_n_heads(self, device):
        config = ExperimentConfig(
            model=ModelConfig(n_heads=2, d_model=256),
        )
        model = create_model(config)
        assert model.n_heads == 2
        assert model.d_head == 128

    def test_no_extraction_mode_skips_internals(self, device):
        model = TransformerLM(100, 256, 2, 2, 16).to(device)
        idx = torch.randint(0, 100, (1, 16), device=device)
        out = model(idx, mode=ExtractionMode.NONE)
        assert out.qkt is None
        assert out.attention_weights is None
        assert out.values is None

    def test_residual_mode_works_with_multihead(self, device):
        B, T, n_layers = 1, 8, 2
        model = TransformerLM(100, 256, n_layers, 2, T).to(device)
        idx = torch.randint(0, 100, (B, T), device=device)
        out = model(idx, mode=ExtractionMode.RESIDUAL)
        assert out.residual_stream.shape == (B, T, n_layers + 1, 256)
        assert out.residual_norms.shape == (B, T, n_layers + 1)


class TestBackwardCompatibility:
    """Single-head behavior is numerically identical to v1.0."""

    @pytest.fixture
    def device(self):
        return torch.device("cpu")

    def test_single_head_wvwo_matches_legacy_shape(self, device):
        """Single-head get_wvwo has n_heads=1 dimension but same data."""
        model = TransformerLM(100, 128, 2, 1, 16).to(device)
        wvwo = model.get_wvwo()
        # Shape is [n_layers, 1, d_model, d_model]
        assert wvwo.shape == (2, 1, 128, 128)
```

NOTE: These tests use small configs (B=2, T=16, vocab=100) for fast execution. The exact parametrize values match the d_k=128 constant constraint from the success criteria.
  </action>
</task>

<task type="auto">
  <name>Task 5: Run existing test suite and verify backward compatibility</name>
  <files></files>
  <action>
Run the full existing test suite to ensure all existing tests pass:
```
pytest tests/ -x -v
```

If any tests fail due to shape changes (e.g., tests checking `qkt.shape == (B, T, T)` instead of `(B, n_heads, T, T)`), update those tests to handle the n_heads=1 dimension.

Common fixes needed:
- Tests indexing `output.qkt[:, layer_idx]` now get [B, n_heads, T, T] instead of [B, T, T]
- Tests checking `wvwo.shape == (n_layers, D, D)` now get `(n_layers, 1, D, D)`
- The _compute_avwo_for_layer function in pipeline.py will need updates (covered in Plan 16-02)

For this plan, ensure the MODEL layer tests pass. Pipeline tests may require Plan 16-02 changes.
  </action>
</task>

</tasks>
