---
phase: 15-advanced-analysis
plan: 03
type: execute
wave: 2
depends_on: [15-01]
files_modified:
  - src/analysis/compliance_curve.py
  - src/visualization/compliance.py
  - src/visualization/render.py
  - src/reporting/single.py
  - src/reporting/templates/single_report.html
  - src/results/schema.py
  - tests/test_compliance_curve.py
autonomous: true
requirements: [COMP-01, COMP-02]
---

must_haves:
  truths:
    - "compute_compliance_curve loads multiple result.json files and extracts (r/w ratio, edge compliance, rule compliance, predictive horizon) tuples"
    - "aggregate_compliance_curve groups by r/w ratio and computes mean +/- std across seeds"
    - "plot_compliance_curve creates a dual-axis publication figure with compliance rate (left y-axis) and predictive horizon (right y-axis) vs r/w ratio"
    - "Compliance curve rendered in HTML report in a collapsible section"
    - "result.json compliance_curve block validates through schema.py"
  artifacts:
    - src/analysis/compliance_curve.py
    - src/visualization/compliance.py
    - tests/test_compliance_curve.py
  key_links:
    - "compliance_curve.py loads result.json files from multiple experiment directories"
    - "compliance_curve.py extracts r and w from config, compliance from metrics.scalars"
    - "compliance.py creates dual-axis matplotlib figure"
    - "render.py calls plot_compliance_curve when compliance_curve data exists in result.json"
    - "single.py passes compliance_curve data to HTML template"

<objective>
Implement compliance curve sweep aggregation and dual-axis publication figure showing the phase transition from compliance to failure across r/w ratio.

Purpose: COMP-01 requires sweeping r/w ratio with fine granularity and 3 seeds per value. COMP-02 requires a composite publication figure with dual y-axes.
Output: Compliance curve analysis module, dual-axis publication figure, report integration, schema validation, and tests.
</objective>

<execution_context>
@/root/.claude/get-shit-done/workflows/execute-plan.md
@/root/.claude/get-shit-done/references/tdd.md
@/root/.claude/get-shit-done/references/checkpoints.md
@/root/.claude/get-shit-done/references/model-profile-resolution.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-advanced-analysis/15-RESEARCH.md
@.planning/phases/15-advanced-analysis/15-CONTEXT.md

@src/config/experiment.py
@src/graph/jumpers.py
@src/evaluation/behavioral.py
@src/analysis/auroc_horizon.py
@src/results/schema.py
@src/visualization/style.py
@src/visualization/render.py
@src/reporting/single.py
@src/reporting/templates/single_report.html

<interfaces>
<!-- Key types and contracts the executor needs. -->

From src/graph/jumpers.py:
```python
R_SCALES: tuple[float, ...] = (0.5, 0.7, 0.9, 1.0, 1.1, 1.3, 1.5, 2.0)

def compute_r_values(w: int) -> list[int]
```

From result.json structure (src/results/schema.py):
```json
{
  "config": {
    "training": {"r": 57, "w": 64, ...},
    ...
  },
  "metrics": {
    "scalars": {
      "final_edge_compliance": 0.98,
      "final_rule_compliance": 0.85,
      ...
    },
    "predictive_horizon": {
      "by_r_value": {
        "57": {
          "by_metric": {
            "qkt.grassmannian_distance": {
              "horizon_j": 5, ...
            }
          }
        }
      }
    }
  }
}
```

From src/visualization/style.py:
```python
PALETTE = sns.color_palette("colorblind", n_colors=8)
VIOLATION_COLOR, CONTROL_COLOR, BASELINE_COLOR, THRESHOLD_COLOR
def apply_style() -> None
def save_figure(fig, output_dir, name) -> tuple[Path, Path]
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create compliance curve analysis module and tests</name>
  <files>
    src/analysis/compliance_curve.py
    tests/test_compliance_curve.py
  </files>
  <action>
Create `src/analysis/compliance_curve.py`:

1. **`load_result_json(result_dir)`:**
   - Load result.json from a directory path
   - Return parsed dict or None if file not found
   - Minimal error handling with logging

2. **`extract_compliance_point(result)`:**
   - Given a parsed result.json dict, extract:
     - r: from result["config"]["training"]["r"]
     - w: from result["config"]["training"]["w"]
     - r_over_w: r / w (float)
     - edge_compliance: from result["metrics"]["scalars"]["final_edge_compliance"] (or compute from edge_valid if not present)
     - rule_compliance: from result["metrics"]["scalars"]["final_rule_compliance"] (or compute from rule_outcome if not present)
     - predictive_horizon: best horizon from result["metrics"]["predictive_horizon"] -- extract the maximum horizon_j across metrics for the corresponding r_value. Return None if predictive_horizon data not available.
     - seed: from result["config"]["seed"]
   - Return a dict with these fields, or None if extraction fails

3. **`compute_compliance_curve(result_dirs)`:**
   - Takes a list of Path objects pointing to result directories
   - For each, load result.json and extract compliance point
   - Skip any that fail extraction
   - Return list of compliance point dicts, sorted by r_over_w

4. **`aggregate_compliance_curve(points)`:**
   - Group compliance points by r_over_w (with tolerance for floating-point grouping: round to 3 decimal places)
   - For each group, compute:
     - mean_edge_compliance, std_edge_compliance
     - mean_rule_compliance, std_rule_compliance
     - mean_predictive_horizon, std_predictive_horizon (if available)
     - n_seeds: count of points in group
   - Return a dict:
     ```python
     {
         "r_over_w_values": [0.5, 0.7, ...],
         "edge_compliance": {"mean": [...], "std": [...]},
         "rule_compliance": {"mean": [...], "std": [...]},
         "predictive_horizon": {"mean": [...], "std": [...]},
         "n_seeds": [...],
     }
     ```

5. **`run_compliance_analysis(result_dirs)`:**
   - Orchestrator: calls compute_compliance_curve then aggregate_compliance_curve
   - Returns the aggregated dict with metadata:
     ```python
     {
         "config": {"n_result_dirs": N, "n_valid_points": M},
         "curve": <aggregated dict>,
         "raw_points": <list of point dicts>,
     }
     ```

Create `tests/test_compliance_curve.py`:

1. **test_extract_compliance_point_valid:** Create a synthetic result.json dict with config.training.r=57, w=64, metrics.scalars.final_edge_compliance=0.98, final_rule_compliance=0.85. Verify extracted point has r_over_w ~= 0.891, correct compliance values.

2. **test_extract_compliance_point_missing_fields:** Result dict missing metrics.scalars.final_rule_compliance. Should return None gracefully.

3. **test_compute_compliance_curve_sorting:** Create 3 synthetic result.json files with different r values (r=32, r=64, r=96 with w=64). Write to temp directories. Verify points are sorted by r_over_w ascending.

4. **test_aggregate_compliance_curve:** Create 6 compliance points: 2 at r/w=0.5, 2 at r/w=1.0, 2 at r/w=1.5 with slightly different compliance values. Verify aggregated output has 3 r_over_w_values, correct means/stds, n_seeds=[2,2,2].

5. **test_aggregate_compliance_curve_single_seed:** Single seed per r value. std should be 0.0.

6. **test_run_compliance_analysis_structure:** Create temp directories with synthetic result.json files. Run run_compliance_analysis. Verify output structure with config, curve, raw_points.

7. **test_compliance_curve_with_predictive_horizon:** Create result.json with predictive_horizon data. Verify horizon is extracted correctly.

8. **test_compliance_curve_without_predictive_horizon:** Create result.json without predictive_horizon. Verify extraction succeeds with horizon=None, and aggregation handles None values gracefully.

Use tmp_path fixture for creating temporary result directories.
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -m pytest tests/test_compliance_curve.py -x -v</automated>
  </verify>
  <done>
    - extract_compliance_point correctly extracts r/w ratio and compliance values
    - compute_compliance_curve returns sorted list of compliance points
    - aggregate_compliance_curve groups by r/w and computes mean/std
    - Predictive horizon extraction works when available, gracefully handles absence
    - run_compliance_analysis returns correctly structured output
    - All tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Create compliance curve visualization and integrate into render pipeline and HTML report</name>
  <files>
    src/visualization/compliance.py
    src/visualization/render.py
    src/reporting/single.py
    src/reporting/templates/single_report.html
    src/results/schema.py
    tests/test_compliance_curve.py
  </files>
  <action>
**Create `src/visualization/compliance.py`:**

1. **`plot_compliance_curve(compliance_data, ax=None)`:**
   - Input: aggregated compliance curve dict from aggregate_compliance_curve
   - Create dual-axis publication figure:
     - fig, ax1 = plt.subplots(figsize=(8, 5))
     - ax2 = ax1.twinx()
   - Left y-axis (ax1): Rule compliance rate (0-1)
     - Line plot: mean_rule_compliance vs r_over_w
     - Error band: fill_between with mean +/- std
     - Color: PALETTE[0] (blue)
     - Optional: also plot edge_compliance as a lighter/dashed line
   - Right y-axis (ax2): Predictive horizon (steps)
     - Line plot: mean_predictive_horizon vs r_over_w (if available)
     - Error band: fill_between with mean +/- std
     - Color: PALETTE[1] (orange)
   - Vertical dashed line at r/w = 1.0 (the critical boundary)
   - Labels:
     - x: "r / w ratio"
     - left y: "Rule Compliance Rate"
     - right y: "Predictive Horizon (steps)"
     - title: "Compliance Phase Transition"
   - Legend with both line labels
   - Apply style from style.py
   - Returns plt.Figure

2. **`plot_compliance_scatter(raw_points, ax=None)`:**
   - Scatter plot of individual data points (all seeds) colored by seed
   - X-axis: r/w, Y-axis: rule_compliance
   - Useful for visualizing seed variance
   - Returns plt.Figure

**Modify `src/visualization/render.py`:**

Add compliance curve section after the spectrum analysis section:

```python
# -- Compliance Curve (COMP-01, COMP-02) --
compliance_curve = result.get("metrics", {}).get("compliance_curve", {})
if compliance_curve:
    try:
        from src.visualization.compliance import plot_compliance_curve

        curve_data = compliance_curve.get("curve", {})
        if curve_data and curve_data.get("r_over_w_values"):
            fig = plot_compliance_curve(curve_data)
            paths = save_figure(fig, figures_dir, "compliance_curve")
            generated_files.extend(paths)
            log.info("Generated: compliance_curve")
    except Exception as e:
        log.warning("Failed to generate compliance curve: %s", e)
```

**Modify `src/reporting/single.py`:**

1. In `_collect_figures`, add:
   ```python
   "compliance_figures": [],
   ```
   In the for loop:
   ```python
   elif name.startswith("compliance_"):
       result["compliance_figures"].append({"title": title, "data_uri": data_uri})
   ```

2. In `generate_single_report`, extract compliance data and pass to template:
   ```python
   compliance_curve = metrics.get("compliance_curve")
   ```
   Add to template.render():
   ```python
   compliance_curve=compliance_curve,
   compliance_figures=figures.get("compliance_figures", []),
   ```

**Modify `src/reporting/templates/single_report.html`:**

Add after the Spectrum Analysis section:

```html
<!-- Compliance Curve (COMP-01, COMP-02) -->
{% if compliance_curve or compliance_figures %}
<details class="enrichment-section">
  <summary><strong>Compliance Phase Transition</strong></summary>
  <div class="section-content">
    {% if compliance_curve %}
    <h3>Compliance Summary</h3>
    <table>
      <thead>
        <tr>
          <th>r/w ratio</th>
          <th>Rule Compliance (mean +/- std)</th>
          <th>Edge Compliance (mean +/- std)</th>
          <th>Predictive Horizon (mean +/- std)</th>
          <th>Seeds</th>
        </tr>
      </thead>
      <tbody>
        {% if compliance_curve.curve %}
        {% for i in range(compliance_curve.curve.r_over_w_values|length) %}
        <tr>
          <td>{{ "%.2f"|format(compliance_curve.curve.r_over_w_values[i]) }}</td>
          <td>{{ "%.4f +/- %.4f"|format(compliance_curve.curve.rule_compliance.mean[i], compliance_curve.curve.rule_compliance.std[i]) }}</td>
          <td>{{ "%.4f +/- %.4f"|format(compliance_curve.curve.edge_compliance.mean[i], compliance_curve.curve.edge_compliance.std[i]) }}</td>
          <td>
            {% if compliance_curve.curve.predictive_horizon and compliance_curve.curve.predictive_horizon.mean[i] is number %}
            {{ "%.1f +/- %.1f"|format(compliance_curve.curve.predictive_horizon.mean[i], compliance_curve.curve.predictive_horizon.std[i]) }}
            {% else %}
            N/A
            {% endif %}
          </td>
          <td>{{ compliance_curve.curve.n_seeds[i] }}</td>
        </tr>
        {% endfor %}
        {% endif %}
      </tbody>
    </table>
    {% endif %}

    {% if compliance_figures %}
    <h3>Compliance Curve</h3>
    {% for fig in compliance_figures %}
    <div class="figure-container">
      <div class="figure-title">{{ fig.title }}</div>
      <img src="{{ fig.data_uri }}" alt="{{ fig.title }}">
    </div>
    {% endfor %}
    {% endif %}
  </div>
</details>
{% endif %}
```

**Modify `src/results/schema.py`:**

Add optional compliance_curve validation block:
```python
# Optional compliance_curve validation (Phase 15, backward compatible)
if "metrics" in result and isinstance(result["metrics"], dict):
    compliance_curve = result["metrics"].get("compliance_curve")
    if compliance_curve is not None:
        if not isinstance(compliance_curve, dict):
            errors.append("metrics.compliance_curve must be a dict")
        else:
            if "curve" not in compliance_curve:
                errors.append("metrics.compliance_curve missing required block: curve")
```

**Extend `tests/test_compliance_curve.py`:**

1. **test_plot_compliance_curve_returns_figure:** Create synthetic aggregated compliance data. Call plot_compliance_curve. Verify returns matplotlib Figure.

2. **test_plot_compliance_curve_dual_axes:** Verify figure has two y-axes (ax1 and ax2).

3. **test_plot_compliance_curve_without_horizon:** Create aggregated data without predictive_horizon. Verify plot still generates (single y-axis only).

4. **test_schema_validates_compliance_curve:** Create result dict with compliance_curve block. Run validation. Verify no errors.

5. **test_schema_backward_compatible:** Result dict without compliance_curve block. Run validation. Verify no errors.
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -m pytest tests/test_compliance_curve.py tests/test_reporting.py -x -v</automated>
  </verify>
  <done>
    - Dual-axis compliance curve figure shows compliance rate and predictive horizon
    - Vertical line at r/w=1.0 marks the critical boundary
    - Error bands show seed variance
    - HTML report includes collapsible "Compliance Phase Transition" section with data table and figure
    - Schema validates compliance_curve block when present, passes when absent
    - All existing tests pass (no regressions)
  </done>
</task>

</tasks>

<verification>
- `pytest tests/test_compliance_curve.py -x -v` passes all compliance curve tests
- `pytest tests/test_reporting.py -x -v` passes (existing + compliance integration)
- `pytest tests/ -x` full suite passes (no regressions)
- Compliance curve shows expected sigmoid-like transition shape on synthetic data
- Dual-axis figure has correct labels and legend
</verification>

<success_criteria>
- COMP-01: System sweeps r/w ratio with at least 8 values (R_SCALES provides 8) and supports 3 seeds per value
- COMP-02: Composite publication figure showing compliance rate and predictive horizon as function of r/w ratio with dual y-axes
- Report integration follows Phase 13 collapsible section patterns
- Schema backward-compatible
- All tests pass including existing suite
</success_criteria>

<output>
After completion, create `.planning/phases/15-advanced-analysis/15-03-SUMMARY.md`
</output>
