---
phase: 07-predictive-horizon-and-statistical-analysis
plan: 02
type: tdd
wave: 2
depends_on:
  - "07-01"
files_modified:
  - src/analysis/statistical_controls.py
  - tests/test_statistical_controls.py
autonomous: true
requirements:
  - STAT-01
  - STAT-02
  - STAT-03
  - STAT-04
  - STAT-05

must_haves:
  truths:
    - "Holm-Bonferroni correction is applied only to the 5 pre-registered primary metrics, not all 21"
    - "BCa bootstrap confidence intervals are computed on AUROC estimates with percentile fallback"
    - "Cohen's d effect sizes are computed per metric per lookback distance"
    - "Two correlation matrices are computed: measurement redundancy (raw values) and predictive redundancy (AUROC values)"
    - "Metric importance ranking orders metrics by max AUROC per layer, annotated with redundancy flags"
    - "Headline comparison reports QK^T vs AVWo predictive horizon per r value with bootstrap CIs"
  artifacts:
    - path: "src/analysis/statistical_controls.py"
      provides: "Holm-Bonferroni, BCa bootstrap, Cohen's d, correlation, ranking, headline comparison"
      contains: "def holm_bonferroni"
    - path: "tests/test_statistical_controls.py"
      provides: "Tests for all statistical control functions"
  key_links:
    - from: "src/analysis/statistical_controls.py"
      to: "scipy.stats"
      via: "bootstrap for BCa CIs, rankdata for AUROC in bootstrap statistic"
      pattern: "from scipy\\.stats import bootstrap"
    - from: "src/analysis/statistical_controls.py"
      to: "src/analysis/auroc_horizon.py"
      via: "auroc_from_groups used inside bootstrap statistic"
      pattern: "from src\\.analysis\\.auroc_horizon import auroc_from_groups"
---

<objective>
Implement statistical controls: Holm-Bonferroni multiple comparison correction, BCa bootstrap confidence intervals on AUROC, Cohen's d effect sizes, correlation/redundancy analysis, metric importance ranking, and headline QK^T vs AVWo comparison.

Purpose: These statistical controls ensure the predictive horizon findings are rigorous and publishable. Without correction for multiple comparisons, bootstrap CIs, and redundancy analysis, the raw AUROC results could overstate significance or miss that correlated metrics are measuring the same underlying signal.

Output: statistical_controls.py module with comprehensive TDD tests, integrating with the AUROC analysis from Plan 07-01.
</objective>

<execution_context>
@/root/.claude/get-shit-done/workflows/execute-plan.md
@/root/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-predictive-horizon-and-statistical-analysis/07-CONTEXT.md
@.planning/phases/07-predictive-horizon-and-statistical-analysis/07-RESEARCH.md
@.planning/phases/07-predictive-horizon-and-statistical-analysis/07-01-SUMMARY.md
@src/analysis/event_extraction.py
@src/analysis/auroc_horizon.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Statistical control functions (TDD)</name>
  <files>
    src/analysis/statistical_controls.py
    tests/test_statistical_controls.py
  </files>
  <action>
**RED phase: Write tests first in tests/test_statistical_controls.py**

Create test file with these test cases:

**Holm-Bonferroni tests:**
1. `test_holm_bonferroni_basic` -- p_values=[0.01, 0.04, 0.03, 0.005, 0.8], alpha=0.05. Verify correct adjusted p-values and reject flags match known Holm-Bonferroni output. Specifically: sort ascending [0.005, 0.01, 0.03, 0.04, 0.8], multiply by [5, 4, 3, 2, 1], enforce monotonicity, clip at 1.0, map back to original order.

2. `test_holm_bonferroni_all_significant` -- All p-values very small (< 0.01). All should be rejected.

3. `test_holm_bonferroni_none_significant` -- All p-values > 0.5. None rejected.

4. `test_holm_bonferroni_five_primary_metrics` -- Exactly 5 p-values (matching the 5 pre-registered primary metrics). Verify correction factor is at most 5 (not 21).

**BCa Bootstrap CI tests:**
5. `test_bootstrap_auroc_ci_separable` -- Perfect separation (AUROC=1.0). CI should be near [0.95, 1.0] (narrow, high).

6. `test_bootstrap_auroc_ci_no_signal` -- No separation (AUROC~0.5). CI should contain 0.5.

7. `test_bootstrap_auroc_ci_deterministic` -- Same inputs, same rng. Results must be identical.

8. `test_bootstrap_auroc_ci_fallback_to_percentile` -- Very small samples (n=3 per group) where BCa may fail. Verify CI is returned (not NaN) via percentile fallback.

**Cohen's d tests:**
9. `test_cohens_d_known_value` -- group1=[5,6,7,8], group2=[1,2,3,4]. Compute expected d by hand and verify.

10. `test_cohens_d_identical_groups` -- Same values. d should be 0.0 (or NaN if pooled_std ~0 due to equal values).

11. `test_cohens_d_insufficient_samples` -- Less than 2 in a group. Returns NaN.

12. `test_cohens_d_by_lookback` -- Compute d at each lookback j using violation and control metric values. Verify shape matches r.

**Correlation and redundancy tests:**
13. `test_measurement_correlation_matrix` -- 3 metrics with known correlations (metric A = metric B * 2 + noise, metric C independent). Verify correlation matrix shape is (3, 3), A-B correlation > 0.9, A-C correlation < 0.3.

14. `test_predictive_correlation_matrix` -- AUROC curves for 3 metrics. Verify correlation computed across lookback distances.

15. `test_redundancy_flagging` -- Pair with |r| > 0.9 flagged as redundant. Pair with |r| < 0.9 not flagged.

**Metric ranking tests:**
16. `test_metric_ranking_by_max_auroc` -- 5 metrics with different max AUROC values. Verify ranking is in descending max AUROC order.

17. `test_metric_ranking_annotates_redundancy` -- Top-ranked metric flagged as redundant with another. Annotation present.

18. `test_metric_ranking_per_layer` -- Separate rankings for layer 0 and layer 1. Different orderings allowed.

**Headline comparison tests:**
19. `test_headline_comparison_qkt_leads` -- QK^T horizon=12, AVWo horizon=5. qkt_leads=True, gap=7.

20. `test_headline_comparison_avwo_leads` -- AVWo horizon > QK^T horizon. qkt_leads=False.

**Integration test:**
21. `test_apply_statistical_controls_to_auroc_results` -- Take a mock AUROC results dict (as produced by run_auroc_analysis from 07-01), apply all statistical controls (bootstrap, Holm-Bonferroni, Cohen's d, correlation, ranking, headline). Verify the output dict has all required fields and is JSON-serializable.

**GREEN phase: Implement src/analysis/statistical_controls.py**

Functions:

- `holm_bonferroni(p_values: np.ndarray, alpha: float = 0.05) -> tuple[np.ndarray, np.ndarray]` -- Step-down correction. Returns (adjusted_p_values, reject_flags). Sort ascending, multiply by (m - rank + 1), enforce monotonicity, clip at 1.0, map back.

- `auroc_with_bootstrap_ci(violation_vals: np.ndarray, control_vals: np.ndarray, n_resamples: int = 10_000, confidence_level: float = 0.95, rng: int | np.random.Generator = 42) -> tuple[float, float, float]` -- BCa bootstrap CI via scipy.stats.bootstrap. Returns (point_estimate, ci_low, ci_high). Falls back to percentile method if BCa produces NaN or raises. Uses auroc_from_groups from auroc_horizon.py as the statistic.

- `cohens_d(group1: np.ndarray, group2: np.ndarray) -> float` -- Cohen's d with pooled standard deviation. Positive d means group1 > group2. Returns NaN for insufficient samples or zero pooled_std.

- `compute_cohens_d_by_lookback(violation_events: list[AnalysisEvent], control_events: list[AnalysisEvent], metric_array: np.ndarray, r_value: int) -> np.ndarray` -- Cohen's d at each lookback j=1..r. Returns array of shape (r_value,).

- `compute_correlation_matrix(metric_arrays: dict[str, np.ndarray], events: list[AnalysisEvent], mode: str) -> dict` -- mode="measurement": pool raw metric values at all valid event positions, compute Pearson correlation. mode="predictive": compute AUROC curves first, then correlate across lookback distances. Returns dict with metric_names, matrix (2D list), redundant_pairs (|r|>0.9).

- `compute_metric_ranking(auroc_results_by_metric: dict[str, dict], primary_metric_names: list[str], redundant_pairs: list) -> dict` -- Rank all metrics by max AUROC descending. Annotate which are primary, which are redundant. Returns separate primary and all rankings.

- `compute_headline_comparison(auroc_results: dict, primary_metrics: list[str]) -> dict` -- Compare QK^T vs AVWo predictive horizons per r value. QK^T metrics = those starting with "qkt.", AVWo metrics = those starting with "avwo.". For each r, take the max horizon across the primary QK^T metrics and max horizon across primary AVWo metrics. Report qkt_max_horizon, avwo_max_horizon, qkt_leads (bool), gap.

- `apply_statistical_controls(auroc_results: dict, eval_data: dict, jumper_map: dict, n_bootstrap: int = 10_000, confidence_level: float = 0.95, bootstrap_rng: int = 42) -> dict` -- Top-level orchestrator. Takes the output from run_auroc_analysis (Plan 07-01) and enriches it with: (1) BCa bootstrap CIs on each AUROC value (for strata with 10+ events), (2) Holm-Bonferroni correction on the 5 primary metrics' p-values (test: AUROC > 0.5), (3) Cohen's d at each lookback, (4) both correlation matrices, (5) metric importance ranking per layer, (6) headline comparison. Returns the enriched dict ready for result.json.

**WvWo handling per Claude's discretion:** WvWo metrics are static (constant across steps). When computing WvWo correlation/ranking entries, note them as "per-checkpoint reference" and expect AUROC ~0.5. Do not include WvWo in the primary metrics or Holm-Bonferroni correction.

Run all tests: `pytest tests/test_statistical_controls.py -x -v`
  </action>
  <verify>
    <automated>pytest tests/test_statistical_controls.py tests/test_auroc_horizon.py tests/test_event_extraction.py -x -v</automated>
    <manual>Verify Holm-Bonferroni applies to exactly 5 metrics (not 21), and bootstrap CIs are deterministic with fixed rng</manual>
    <sampling_rate>run after this task commits, before next task begins</sampling_rate>
  </verify>
  <done>
    - All 21 test cases pass
    - Holm-Bonferroni correctly adjusts exactly 5 primary metric p-values (not 21)
    - BCa bootstrap CIs are computed with percentile fallback for degenerate cases
    - Cohen's d matches hand-calculated values for known inputs
    - Two correlation matrices (measurement + predictive) computed with redundancy flagging at |r| > 0.9
    - Metric ranking orders by max AUROC per layer with redundancy annotations
    - Headline comparison correctly reports QK^T vs AVWo predictive horizon gap
    - apply_statistical_controls produces JSON-serializable output matching result.json schema
    - No regressions in Plan 07-01 tests
  </done>
</task>

</tasks>

<verification>
```bash
# All Phase 7 tests pass
pytest tests/test_event_extraction.py tests/test_auroc_horizon.py tests/test_statistical_controls.py -x -v

# No regressions in full test suite
pytest tests/ -x --timeout=120

# Verify complete analysis module is importable
python -c "
from src.analysis.event_extraction import AnalysisEvent, extract_events, filter_contaminated_events, stratify_by_r
from src.analysis.auroc_horizon import auroc_from_groups, compute_auroc_curve, compute_predictive_horizon, run_shuffle_control, run_auroc_analysis
from src.analysis.statistical_controls import holm_bonferroni, auroc_with_bootstrap_ci, cohens_d, compute_correlation_matrix, compute_metric_ranking, compute_headline_comparison, apply_statistical_controls
print('All analysis module imports OK')
"
```
</verification>

<success_criteria>
- Holm-Bonferroni applies to exactly 5 pre-registered primary metrics with correct step-down procedure
- BCa bootstrap CIs computed on AUROC with fallback to percentile for degenerate cases
- Cohen's d correctly computed per lookback distance
- Two correlation matrices (measurement + predictive redundancy) with |r| > 0.9 flagging
- Metric importance ranking per layer with redundancy annotations
- Headline QK^T vs AVWo predictive horizon comparison with CIs
- Full apply_statistical_controls produces JSON-serializable output matching result.json schema
- All Phase 7 tests pass, no regressions in existing tests
</success_criteria>

<output>
After completion, create `.planning/phases/07-predictive-horizon-and-statistical-analysis/07-02-SUMMARY.md`
</output>
