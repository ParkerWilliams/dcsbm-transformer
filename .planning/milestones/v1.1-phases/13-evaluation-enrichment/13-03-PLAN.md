---
phase: 13-evaluation-enrichment
plan: 03
type: execute
wave: 3
depends_on: [13-02]
files_modified:
  - src/analysis/svd_benchmark.py
  - src/visualization/svd_benchmark.py
  - src/visualization/render.py
  - src/reporting/single.py
  - src/reporting/templates/single_report.html
  - src/results/schema.py
  - tests/test_svd_benchmark.py
autonomous: true
requirements: [OVHD-01, OVHD-02, OVHD-03]

must_haves:
  truths:
    - "benchmark_svd_methods returns timing (ms per call) for full, randomized, and values-only SVD for each target at actual matrix dimensions"
    - "accuracy comparison reports relative Frobenius error and singular value correlation for randomized and values-only SVD vs full SVD"
    - "Cost summary table shows matrix size, time per step, and percentage of total evaluation time per target"
    - "Grouped bar chart shows targets on x-axis with SVD methods as colored groups"
    - "SVD benchmark results are stored in result.json svd_benchmark block and rendered in HTML report"
  artifacts:
    - src/analysis/svd_benchmark.py
    - src/visualization/svd_benchmark.py
    - tests/test_svd_benchmark.py
  key_links:
    - "svd_benchmark.py uses torch.cuda.Event for GPU timing with 5 warmup + 20 timed iterations"
    - "render.py calls plot_svd_benchmark and saves svd_benchmark_* figures"
    - "single.py collects svd_benchmark_* figures and passes to template"
    - "single_report.html renders SVD Cost section as collapsible details/summary block"
---

<objective>
Implement SVD computational overhead benchmarking comparing full SVD vs randomized SVD vs values-only SVD, with cost summary table and HTML report integration.

Purpose: OVHD-01 requires wall-clock benchmarking by target and matrix dimension. OVHD-02 requires accuracy-cost tradeoff comparison. OVHD-03 requires cost summary in HTML reports.
Output: SVD benchmark module, grouped bar chart visualization, report section, schema validation, and tests.
</objective>

<execution_context>
@/root/.claude/get-shit-done/workflows/execute-plan.md
@/root/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-evaluation-enrichment/13-RESEARCH.md
@.planning/phases/13-evaluation-enrichment/13-01-SUMMARY.md
@.planning/phases/13-evaluation-enrichment/13-02-SUMMARY.md

@src/evaluation/svd_metrics.py
@src/evaluation/pipeline.py
@src/config/experiment.py
@src/visualization/style.py
@src/visualization/render.py
@src/reporting/single.py
@src/reporting/templates/single_report.html
@src/results/schema.py

<interfaces>
<!-- Key types and contracts the executor needs. -->

From src/evaluation/pipeline.py:
```python
SVD_TARGETS = ["qkt", "wvwo", "avwo"]
# QK^T matrix shape: [B, w, w] (context window)
# WvWo matrix shape: [d_model, d_model] (weight matrices)
# AVWo matrix shape: [B, w, d_model] (residual update)
```

From src/config/experiment.py:
```python
@dataclass
class ModelConfig:
    d_model: int = 128
    n_layers: int = 2
    n_heads: int = 1
    vocab_size: int = 50

@dataclass
class TrainingConfig:
    w: int = 64  # context window
    ...

@dataclass
class ExperimentConfig:
    model: ModelConfig
    training: TrainingConfig
    ...
```

From src/evaluation/svd_metrics.py:
```python
def guard_matrix_for_svd(M: torch.Tensor) -> tuple[torch.Tensor, bool]
def compute_all_metrics(S, U=None, Vh=None) -> dict[str, torch.Tensor]
```

From src/visualization/style.py:
```python
PALETTE = sns.color_palette("colorblind", n_colors=8)
def apply_style() -> None
def save_figure(fig, output_dir, name) -> tuple[Path, Path]
```

Collapsible section pattern from Plans 01/02:
```html
<details class="enrichment-section">
  <summary><strong>Section Title</strong></summary>
  <div class="section-content">...</div>
</details>
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create SVD benchmark module and tests</name>
  <files>
    src/analysis/svd_benchmark.py
    tests/test_svd_benchmark.py
  </files>
  <action>
Create `src/analysis/svd_benchmark.py`:

1. **`_time_svd_method`:** Low-level timing function.
   - Args: matrix (torch.Tensor), method_fn (callable), n_warmup=5, n_timed=20, device=None
   - If CUDA available and matrix is on CUDA:
     - Run n_warmup iterations of method_fn(matrix) for warmup
     - torch.cuda.synchronize()
     - Create start/end CUDA events with enable_timing=True
     - Record start event, run n_timed iterations, record end event
     - torch.cuda.synchronize()
     - Return elapsed_time(end) / n_timed in milliseconds
   - If CPU (no CUDA):
     - Run n_warmup iterations for warmup
     - Use time.perf_counter() for timing
     - Return average time per iteration in milliseconds
   - Return float: ms per call

2. **`_full_svd`:** Wrapper: `torch.linalg.svd(M, full_matrices=False)` returning (U, S, Vh).

3. **`_randomized_svd`:** Wrapper: `torch.svd_lowrank(M, q=min(M.shape[-2], M.shape[-1]))` returning (U, S, V). Note: torch.svd_lowrank returns V, not Vh.

4. **`_values_only_svd`:** Wrapper: `torch.linalg.svdvals(M)` returning S only.

5. **`_compare_accuracy`:** Given reference_S (from full SVD), approx_S (from randomized or values-only):
   - Compute relative Frobenius error: `torch.norm(reference_S - approx_S) / torch.norm(reference_S)`
   - Compute singular value correlation: `torch.corrcoef(torch.stack([reference_S, approx_S]))[0, 1]`
   - Handle edge cases: if reference_S is all zeros, error is NaN
   - Return dict: {"frob_error": float, "sv_correlation": float}

6. **`benchmark_svd_for_target`:** Benchmark a single target at given matrix dimensions.
   - Args: target_name (str), matrix_shape (tuple), device, n_warmup=5, n_timed=20, seed=42
   - Generate random matrix of given shape using torch.randn with seed for reproducibility
   - Time all three methods using _time_svd_method
   - Compute accuracy comparison using full SVD as reference
   - Return dict:
     ```python
     {
         "target": target_name,
         "matrix_shape": list(matrix_shape),
         "full_svd_ms": float,
         "randomized_svd_ms": float,
         "values_only_ms": float,
         "randomized_frob_error": float,
         "randomized_sv_correlation": float,
         "values_only_sv_correlation": float,  # should be ~1.0 since same algorithm class
     }
     ```

7. **`run_svd_benchmark`:** Orchestrator function.
   - Args: config (ExperimentConfig), device (torch.device), n_warmup=5, n_timed=20
   - Determine matrix dimensions from config:
     - QK^T: (config.training.w, config.training.w)
     - WvWo: (config.model.d_model, config.model.d_model)
     - AVWo: (config.training.w, config.model.d_model)
   - Benchmark each target
   - Compute total evaluation estimate: sum of per-target times * n_layers * expected_steps
   - Return dict:
     ```python
     {
         "config": {"n_warmup": 5, "n_timed": 20, "device": str(device)},
         "by_target": {
             "qkt": {...},
             "wvwo": {...},
             "avwo": {...},
         },
         "summary": {
             "total_svd_ms_per_step": float,  # sum of all targets * n_layers
             "fastest_method": str,  # method with lowest total time
         }
     }
     ```

Create `tests/test_svd_benchmark.py`:

1. **test_time_svd_method_cpu:** Time full SVD on CPU. Verify returns positive float ms.

2. **test_full_svd_wrapper:** Verify _full_svd returns (U, S, Vh) with correct shapes.

3. **test_randomized_svd_wrapper:** Verify _randomized_svd returns (U, S, V) with expected shapes.

4. **test_values_only_svd_wrapper:** Verify _values_only_svd returns S with correct length.

5. **test_compare_accuracy_identical:** When reference_S == approx_S, frob_error should be 0 and correlation should be 1.

6. **test_compare_accuracy_different:** When approx_S is perturbed, frob_error > 0 and correlation < 1.

7. **test_benchmark_svd_for_target_structure:** Run benchmark_svd_for_target on CPU with small matrix. Verify output dict has all required keys with correct types.

8. **test_run_svd_benchmark_structure:** Create a minimal ExperimentConfig, run run_svd_benchmark on CPU. Verify output has by_target with all three targets.

Mark CUDA-specific tests with `@pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")` for GPU timing tests.
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -m pytest tests/test_svd_benchmark.py -x -v</automated>
  </verify>
  <done>
    - _time_svd_method returns positive timing values on CPU
    - SVD method wrappers produce correct output shapes
    - Accuracy comparison returns correct frob_error and sv_correlation
    - benchmark_svd_for_target returns complete result dict
    - run_svd_benchmark benchmarks all three targets
    - All tests pass (CUDA tests skipped when no GPU)
  </done>
</task>

<task type="auto">
  <name>Task 2: Create SVD benchmark visualization and integrate into render pipeline and HTML report</name>
  <files>
    src/visualization/svd_benchmark.py
    src/visualization/render.py
    src/reporting/single.py
    src/reporting/templates/single_report.html
    src/results/schema.py
  </files>
  <action>
**Create `src/visualization/svd_benchmark.py`:**

1. **`plot_svd_benchmark_bars`:** Grouped bar chart.
   - Args: benchmark_data (dict with by_target)
   - Create figure with grouped bar chart:
     - x-axis: targets (QK^T, WvWo, AVWo)
     - Groups: Full SVD, Randomized SVD, Values-only SVD
     - y-axis: Time (ms per call)
     - Colors: First 3 colors from PALETTE
     - Bar width: 0.25, positioned with offsets for grouping
     - Add value labels on top of each bar (formatted as "{:.2f} ms")
     - Title: "SVD Computation Cost by Target and Method"
     - Legend with method names
   - Returns plt.Figure

2. **`plot_svd_accuracy_tradeoff`:** Scatter plot of accuracy vs cost.
   - Args: benchmark_data (dict with by_target)
   - Create figure showing each (target, method) combination as a point
   - x-axis: Time (ms), y-axis: Relative Frobenius Error (log scale if > 0)
   - Full SVD points at error=0 (reference), randomized and values-only at their measured error
   - Color by target, marker shape by method
   - Title: "SVD Accuracy-Cost Tradeoff"
   - Returns plt.Figure

**Modify `src/visualization/render.py`:**

Add a new section after the calibration section added in Plan 02:

```python
# -- SVD Benchmark (OVHD-01, OVHD-02, OVHD-03) --
svd_benchmark = result.get("metrics", {}).get("svd_benchmark", {})
if svd_benchmark and svd_benchmark.get("by_target"):
    try:
        from src.visualization.svd_benchmark import (
            plot_svd_benchmark_bars,
            plot_svd_accuracy_tradeoff,
        )

        fig = plot_svd_benchmark_bars(svd_benchmark)
        paths = save_figure(fig, figures_dir, "svd_benchmark_bars")
        generated_files.extend(paths)
        log.info("Generated: svd_benchmark_bars")

        fig = plot_svd_accuracy_tradeoff(svd_benchmark)
        paths = save_figure(fig, figures_dir, "svd_benchmark_tradeoff")
        generated_files.extend(paths)
        log.info("Generated: svd_benchmark_tradeoff")

    except Exception as e:
        log.warning("Failed to generate SVD benchmark plots: %s", e)
```

**Modify `src/reporting/single.py`:**

1. In `_collect_figures`, add:
   ```python
   "svd_benchmark_figures": [],
   ```
   In the for loop:
   ```python
   elif name.startswith("svd_benchmark"):
       result["svd_benchmark_figures"].append({"title": title, "data_uri": data_uri})
   ```

2. In `generate_single_report`, extract svd_benchmark data and pass to template:
   ```python
   svd_benchmark = metrics.get("svd_benchmark")
   ```
   Add to template.render():
   ```python
   svd_benchmark=svd_benchmark,
   svd_benchmark_figures=figures.get("svd_benchmark_figures", []),
   ```

**Modify `src/reporting/templates/single_report.html`:**

Add SVD Cost section AFTER the Calibration section (added in Plan 02), BEFORE Event-Aligned Plots. This is the last enrichment section per CONTEXT.md ordering (AUROC -> PR -> Calibration -> SVD Cost):

```html
<!-- SVD Computational Cost (OVHD-01, OVHD-02, OVHD-03) -->
{% if svd_benchmark %}
<details class="enrichment-section">
  <summary><strong>SVD Computational Cost</strong></summary>
  <div class="section-content">
    <h3>Cost Summary</h3>
    <table>
      <thead>
        <tr>
          <th>Target</th>
          <th>Matrix Size</th>
          <th>Full SVD (ms)</th>
          <th>Randomized SVD (ms)</th>
          <th>Values-only SVD (ms)</th>
          <th>Randomized Frob. Error</th>
          <th>Randomized SV Corr.</th>
        </tr>
      </thead>
      <tbody>
        {% for target_name, t_data in svd_benchmark.by_target|dictsort %}
        <tr>
          <td>{{ target_name }}</td>
          <td>{{ t_data.matrix_shape | join(' x ') if t_data.matrix_shape else 'N/A' }}</td>
          <td>{{ "%.3f"|format(t_data.full_svd_ms) if t_data.full_svd_ms is number else 'N/A' }}</td>
          <td>{{ "%.3f"|format(t_data.randomized_svd_ms) if t_data.randomized_svd_ms is number else 'N/A' }}</td>
          <td>{{ "%.3f"|format(t_data.values_only_ms) if t_data.values_only_ms is number else 'N/A' }}</td>
          <td>{{ "%.6f"|format(t_data.randomized_frob_error) if t_data.randomized_frob_error is number else 'N/A' }}</td>
          <td>{{ "%.6f"|format(t_data.randomized_sv_correlation) if t_data.randomized_sv_correlation is number else 'N/A' }}</td>
        </tr>
        {% endfor %}
      </tbody>
    </table>

    {% if svd_benchmark.summary %}
    <p>
      <strong>Total SVD cost per step:</strong> {{ "%.3f"|format(svd_benchmark.summary.total_svd_ms_per_step) }} ms.
      <strong>Fastest method:</strong> {{ svd_benchmark.summary.fastest_method }}.
    </p>
    {% endif %}

    {% if svd_benchmark_figures %}
    <h3>Benchmark Plots</h3>
    {% for fig in svd_benchmark_figures %}
    <div class="figure-container">
      <div class="figure-title">{{ fig.title }}</div>
      <img src="{{ fig.data_uri }}" alt="{{ fig.title }}">
    </div>
    {% endfor %}
    {% endif %}
  </div>
</details>
{% endif %}
```

**Modify `src/results/schema.py`:**

Add optional svd_benchmark validation block after the calibration validation:

```python
# Optional svd_benchmark validation (Phase 13, backward compatible)
if "metrics" in result and isinstance(result["metrics"], dict):
    svd_bench = result["metrics"].get("svd_benchmark")
    if svd_bench is not None:
        if not isinstance(svd_bench, dict):
            errors.append("metrics.svd_benchmark must be a dict")
        else:
            if "by_target" not in svd_bench:
                errors.append("metrics.svd_benchmark missing required block: by_target")
            by_target = svd_bench.get("by_target", {})
            if isinstance(by_target, dict):
                for target_name, t_data in by_target.items():
                    if not isinstance(t_data, dict):
                        errors.append(f"metrics.svd_benchmark.by_target.{target_name} must be a dict")
                    else:
                        for field in ["matrix_shape", "full_svd_ms"]:
                            if field not in t_data:
                                errors.append(f"metrics.svd_benchmark.by_target.{target_name} missing field: {field}")
```
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -m pytest tests/test_svd_benchmark.py tests/test_reporting.py -x -v</automated>
  </verify>
  <done>
    - Grouped bar chart shows all three targets with three SVD methods
    - Accuracy-cost tradeoff scatter plot renders correctly
    - render.py generates svd_benchmark_* figures when benchmark data exists
    - single.py collects svd_benchmark_* figures and passes to template
    - HTML template renders SVD Cost section as collapsible block with cost summary table
    - schema.py validates svd_benchmark block when present, passes when absent
    - All existing tests still pass
  </done>
</task>

</tasks>

<verification>
- `pytest tests/test_svd_benchmark.py -x -v` passes all benchmark tests
- `pytest tests/test_reporting.py -x -v` passes
- `pytest tests/ -x` full suite passes (no regressions)
- SVD benchmark compares all three methods with correct timing
- Accuracy metrics (Frobenius error, SV correlation) are computed correctly
- HTML report renders collapsible SVD Cost section with cost summary table
</verification>

<success_criteria>
- OVHD-01: Wall-clock SVD cost benchmarked by target and matrix dimension using CUDA events with warmup
- OVHD-02: Full vs randomized vs values-only compared with accuracy-cost tradeoff
- OVHD-03: Cost summary table included in HTML reports
- Schema validates svd_benchmark block in result.json
- All tests pass including existing suite
</success_criteria>

<output>
After completion, create `.planning/phases/13-evaluation-enrichment/13-03-SUMMARY.md`
</output>
