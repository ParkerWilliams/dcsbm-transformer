---
phase: 12-null-model-baseline
plan: 02
type: execute
wave: 2
depends_on:
  - "12-01"
files_modified:
  - src/analysis/null_model.py
  - src/visualization/null_overlay.py
  - src/visualization/mp_histogram.py
  - src/visualization/render.py
  - src/visualization/event_aligned.py
  - src/reporting/single.py
  - src/reporting/templates/single_report.html
  - src/results/schema.py
  - tests/test_null_model.py
autonomous: true
requirements:
  - NULL-02
  - NULL-04

must_haves:
  truths:
    - "compare_null_vs_violation() computes Mann-Whitney U and Cohen's d at each lookback distance j=1..r, with minimum sample size check (n >= 5)"
    - "Holm-Bonferroni correction is applied across lookback distances within the null comparison as a SEPARATE family (not merged with primary metrics family)"
    - "run_null_analysis() is a top-level orchestrator that generates null walks, evaluates them, extracts drift, runs MW-U, computes Cohen's d, applies Holm-Bonferroni, computes MP KS test, and returns a dict ready for result.json['null_model']"
    - "result.json null_model block contains per-lookback stats (null_mean, null_std, violation_mean, violation_std, U statistic, raw p-value, adjusted p-value, Cohen's d, reject flag) and aggregate summary"
    - "Event-aligned plots render null distribution as gray 95% CI band with solid gray median line overlaid on existing violation/control traces"
    - "MP histogram plot overlays theoretical MP density curve on empirical QK^T singular value histogram"
    - "Single-experiment HTML report includes a Null Model Baseline section with statistical summary table and embedded null overlay figures"
    - "validate_result() accepts optional null_model block without requiring it (backward compatible)"
  artifacts:
    - path: "src/analysis/null_model.py"
      provides: "compare_null_vs_violation, run_null_analysis orchestrator added to existing module"
      exports: ["compare_null_vs_violation", "run_null_analysis"]
    - path: "src/visualization/null_overlay.py"
      provides: "Null CI band and median overlay on event-aligned plots"
      exports: ["plot_event_aligned_with_null", "compute_null_distribution_stats"]
    - path: "src/visualization/mp_histogram.py"
      provides: "MP density overlay on empirical SV histogram"
      exports: ["plot_mp_histogram"]
    - path: "src/results/schema.py"
      provides: "Optional null_model block validation in validate_result()"
    - path: "src/reporting/single.py"
      provides: "Null Model Baseline section in HTML report"
    - path: "src/reporting/templates/single_report.html"
      provides: "Template block for null model section with statistical table and figure slots"
  key_links:
    - from: "src/analysis/null_model.py"
      to: "src/analysis/statistical_controls.py"
      via: "holm_bonferroni and cohens_d reuse"
      pattern: "from src\\.analysis\\.statistical_controls import holm_bonferroni, cohens_d"
    - from: "src/analysis/null_model.py"
      to: "scipy.stats.mannwhitneyu"
      via: "Mann-Whitney U test for null vs violation comparison"
      pattern: "from scipy\\.stats import mannwhitneyu"
    - from: "src/visualization/null_overlay.py"
      to: "src/visualization/event_aligned.py"
      via: "Extends plot_event_aligned with null overlay data"
      pattern: "from src\\.visualization\\.event_aligned import plot_event_aligned"
    - from: "src/analysis/null_model.py"
      to: "results/{exp}/null_token_metrics.npz"
      via: "run_null_analysis() writes null metric arrays to NPZ when output_dir is provided"
      pattern: "np\\.savez_compressed.*null_token_metrics\\.npz"
    - from: "src/visualization/render.py"
      to: "results/{exp}/null_token_metrics.npz"
      via: "Render orchestrator loads null NPZ for null overlay and MP histogram plots"
      pattern: "null_token_metrics\\.npz"
    - from: "src/visualization/render.py"
      to: "src/visualization/null_overlay.py"
      via: "Render orchestrator calls null overlay when null_model data exists"
      pattern: "from src\\.visualization\\.null_overlay import"
    - from: "src/reporting/single.py"
      to: "src/reporting/templates/single_report.html"
      via: "Jinja2 template rendering with null_model context variables"
      pattern: "null_model"
---

<objective>
Implement the statistical comparison pipeline (Mann-Whitney U, Cohen's d, Holm-Bonferroni), result.json storage, null overlay visualization, MP histogram, and report integration for the null model baseline.

Purpose: Complete Phase 12 by (1) quantifying whether the Grassmannian drift signal exceeds the null noise floor with proper statistical rigor, (2) storing all results in a structured null_model block in result.json, (3) rendering null distribution overlays on event-aligned plots, and (4) adding a dedicated Null Model Baseline section to the HTML report.

Output: Extended `src/analysis/null_model.py` with statistical comparison and orchestrator functions, new `src/visualization/null_overlay.py` and `src/visualization/mp_histogram.py`, modified report template and render orchestrator, extended result schema.
</objective>

<execution_context>
@/root/.claude/get-shit-done/workflows/execute-plan.md
@/root/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-null-model-baseline/12-CONTEXT.md
@.planning/phases/12-null-model-baseline/12-RESEARCH.md
@.planning/phases/12-null-model-baseline/12-01-SUMMARY.md
@src/analysis/null_model.py
@src/analysis/statistical_controls.py
@src/visualization/event_aligned.py
@src/visualization/style.py
@src/visualization/render.py
@src/reporting/single.py
@src/reporting/templates/single_report.html
@src/results/schema.py
@src/evaluation/pipeline.py

<interfaces>
<!-- Key types and contracts the executor needs. Extracted from codebase. -->

From src/analysis/null_model.py (created in Plan 12-01):
```python
def generate_null_walks(graph_data, jumpers, config, n_walks, seed=9999) -> np.ndarray
    # Returns [n_walks, walk_length] int32 array of jumper-free walks

def run_null_evaluation(model, null_walks, graph_data, jumpers, config, device, batch_size=32) -> EvaluationResult
    # Returns EvaluationResult with SVD metrics for null walks

def extract_position_matched_drift(metric_array, event_positions, max_lookback) -> dict[int, np.ndarray]
    # Returns {lookback_j: array_of_metric_values} NaN-filtered

def marchenko_pastur_pdf(x, gamma, sigma2=1.0) -> float
def marchenko_pastur_cdf(x, gamma, sigma2=1.0) -> float
def run_mp_ks_test(singular_values, gamma) -> dict
    # Returns {ks_statistic, ks_p_value, gamma, sigma2, lambda_minus, lambda_plus}
```

From src/analysis/statistical_controls.py:
```python
def holm_bonferroni(p_values: np.ndarray, alpha: float = 0.05) -> tuple[np.ndarray, np.ndarray]
    # Returns (adjusted_p_values, reject_flags)
def cohens_d(group1: np.ndarray, group2: np.ndarray) -> float
    # Returns Cohen's d (positive = group1 > group2)
```

From src/visualization/style.py:
```python
VIOLATION_COLOR = PALETTE[3]    # red-ish
CONTROL_COLOR = PALETTE[0]      # blue-ish
BASELINE_COLOR = PALETTE[2]     # green-ish
THRESHOLD_COLOR = (0.5, 0.5, 0.5)  # gray

def apply_style() -> None
def save_figure(fig, output_dir, name) -> tuple[Path, Path]
```

From src/visualization/event_aligned.py:
```python
def plot_event_aligned(
    metric_values: np.ndarray,   # [n_sequences, max_steps]
    events: list[AnalysisEvent],
    window: int = 10,
    metric_name: str = "SVD metric",
    ax: plt.Axes | None = None,
) -> plt.Figure
```

From src/visualization/render.py:
```python
def load_result_data(result_dir) -> dict
    # Returns {result: dict, metric_arrays: dict, curves: dict}
def render_all(result_dir) -> list[Path]
```

From src/reporting/single.py:
```python
def generate_single_report(result_dir, output_path=None) -> Path
# Template variables passed to single_report.html:
#   experiment_id, timestamp, description, config_tables, scalar_metrics,
#   training_curves_figure, confusion_matrix_figure, auroc_figures,
#   event_aligned_figures, distribution_figures, statistical_tests,
#   predictive_horizon, sequence_analysis, reproduction
```

result.json null_model block structure (per CONTEXT.md):
```python
{
    "null_model": {
        "config": {
            "n_null_walks": 500,
            "n_violation_walks": 100,
            "null_seed": 9999,
            "alpha": 0.05,
            "cohens_d_threshold": 0.5,
        },
        "by_lookback": {
            "1": {
                "null_mean": ..., "null_std": ...,
                "violation_mean": ..., "violation_std": ...,
                "mann_whitney_U": ..., "p_value_raw": ...,
                "p_value_adjusted": ...,  # Holm-Bonferroni within null family
                "cohens_d": ..., "reject": ...,
                "n_null_valid": ..., "n_violation_valid": ...,
            },
        },
        "aggregate": {
            "n_lookbacks_tested": ..., "n_lookbacks_rejected": ...,
            "max_cohens_d": ..., "max_cohens_d_lookback": ...,
            "signal_exceeds_noise": ...,  # at least one reject with d >= 0.5
        },
        "marchenko_pastur": {
            "gamma": ..., "sigma2": ...,
            "lambda_minus": ..., "lambda_plus": ...,
            "anchor_positions": {
                "event": {"ks_statistic": ..., "ks_p_value": ...},
                "pre_event_5": {"ks_statistic": ..., "ks_p_value": ...},
                "post_event_5": {"ks_statistic": ..., "ks_p_value": ...},
            },
        },
    }
}
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement statistical comparison functions and run_null_analysis orchestrator</name>
  <files>
    src/analysis/null_model.py
    tests/test_null_model.py
  </files>
  <action>
**Extend `src/analysis/null_model.py`** with three new functions. Add these imports at the top (in addition to existing):

```python
from scipy.stats import mannwhitneyu
from src.analysis.statistical_controls import cohens_d, holm_bonferroni
from src.analysis.event_extraction import AnalysisEvent, extract_events, filter_contaminated_events
from src.evaluation.behavioral import RuleOutcome
```

**Function 1: `compare_null_vs_violation()`**

```python
def compare_null_vs_violation(
    null_drift: dict[int, np.ndarray],
    violation_drift: dict[int, np.ndarray],
    alpha: float = 0.05,
) -> dict:
    """Position-matched Mann-Whitney U and Cohen's d at each lookback distance.

    Compares null Grassmannian drift against violation Grassmannian drift.
    Applies Holm-Bonferroni correction across lookback distances as a
    SEPARATE family (per CONTEXT.md: independent of primary metrics family).

    Args:
        null_drift: Dict mapping lookback j -> array of null metric values.
        violation_drift: Dict mapping lookback j -> array of violation metric values.
        alpha: Family-wise error rate for Holm-Bonferroni.

    Returns:
        Dict with 'by_lookback' (per-j results) and 'aggregate' summary.
    """
```

Implementation:
1. For each lookback j present in both dicts:
   - Check minimum sample size: if len(null) < 5 or len(violation) < 5, mark as "insufficient_samples" and skip
   - Compute Mann-Whitney U: `mannwhitneyu(violation, null, alternative='two-sided', method='auto')`
   - Compute Cohen's d: `cohens_d(violation, null)` -- positive d means violation > null
   - Store: null_mean, null_std, violation_mean, violation_std, U, p_value_raw, cohens_d, n_null_valid, n_violation_valid
2. Collect all raw p-values for tested lookbacks
3. Apply `holm_bonferroni()` to get adjusted p-values and reject flags
4. Map adjusted p-values back to per-lookback results
5. Compute aggregate: n_lookbacks_tested, n_lookbacks_rejected, max_cohens_d, max_cohens_d_lookback, signal_exceeds_noise (at least one reject with d >= 0.5)

**Function 2: `run_null_analysis()`**

```python
def run_null_analysis(
    model: nn.Module,
    graph_data: GraphData,
    jumpers: list[JumperInfo],
    config: ExperimentConfig,
    eval_result: EvaluationResult,
    device: torch.device,
    metric_key: str = "qkt.layer_0.grassmannian_distance",
    null_seed: int = 9999,
    batch_size: int = 32,
) -> dict:
    """Top-level null model analysis orchestrator.

    Generates null walks, evaluates them through the model, extracts
    position-matched Grassmannian drift, compares against violation drift
    with Mann-Whitney U + Cohen's d + Holm-Bonferroni, and computes
    Marchenko-Pastur KS test at anchor positions.

    Returns a dict ready for result.json["null_model"].

    Args:
        model: Trained TransformerLM.
        graph_data: DCSBM graph.
        jumpers: Jumper info list.
        config: ExperimentConfig.
        eval_result: EvaluationResult from the violation experiment.
        device: Computation device.
        metric_key: SVD metric key for Grassmannian drift comparison.
        null_seed: Seed for null walk generation.
        batch_size: Batch size for null walk evaluation.

    Returns:
        Dict with 'config', 'by_lookback', 'aggregate', 'marchenko_pastur' blocks.
    """
```

Implementation:
1. Extract violation events from eval_result using `extract_events()` and `filter_contaminated_events()`
2. Build jumper_map from jumpers list
3. Get violation event positions: `[e.resolution_step for e in violations]`
4. Determine max_lookback from r values: `max(e.r_value for e in violations)`
5. Count violation walks (those with failure_index >= 0)
6. Generate null walks: `n_null = 5 * n_violation_walks` (per CONTEXT.md: 5x)
7. Run null evaluation via `run_null_evaluation()`
8. Extract violation drift: `extract_position_matched_drift(eval_result.svd_metrics[metric_key], event_positions, max_lookback)`
9. Extract null drift at the SAME positions: `extract_position_matched_drift(null_eval_result.svd_metrics[metric_key], event_positions, max_lookback)`
10. Compare: `compare_null_vs_violation(null_drift, violation_drift)`
11. MP analysis at anchor positions (event, pre_event_5, post_event_5):
    - For each anchor: extract QK^T singular values from null evaluation at that position
    - Run `run_mp_ks_test()` with gamma = config.training.w / config.model.d_model
    - Compute gamma and aggregate MP results
12. Assemble and return the full null_model dict matching the schema in CONTEXT.md

**Extend `tests/test_null_model.py`** with additional tests for the new functions:

17. `test_compare_null_vs_violation_basic` -- Create synthetic null drift (low values, small variance) and violation drift (higher values, larger variance) at 5 lookback distances. Call `compare_null_vs_violation()`. Assert that results contain by_lookback and aggregate keys, that each lookback has the expected fields, and that the aggregate summary is consistent.

18. `test_compare_null_vs_violation_separate_family` -- Verify that `compare_null_vs_violation()` applies Holm-Bonferroni ONLY across the lookback distances in the null comparison dict, NOT mixed with any external p-values. Check that the number of adjusted p-values equals the number of tested lookback distances.

19. `test_compare_null_vs_violation_insufficient_samples` -- Provide fewer than 5 values for one lookback. Assert that lookback is marked with insufficient_samples and excluded from Holm-Bonferroni correction.

20. `test_compare_null_vs_violation_signal_exceeds_noise` -- Create data where violation drift clearly exceeds null (Cohen's d > 0.5). Assert `aggregate["signal_exceeds_noise"] is True`.

21. `test_compare_null_vs_violation_no_signal` -- Create data where violation and null distributions are identical. Assert `aggregate["signal_exceeds_noise"] is False`.

Run all tests:
```bash
pytest tests/test_null_model.py -x -v
```
  </action>
  <verify>
    <automated>pytest tests/test_null_model.py -x -v</automated>
  </verify>
  <done>
    - compare_null_vs_violation() returns per-lookback MW-U, Cohen's d, and Holm-Bonferroni corrected p-values
    - Holm-Bonferroni is applied as SEPARATE family (across lookback distances only)
    - Insufficient samples (< 5) are detected and excluded from correction
    - Aggregate summary correctly computes signal_exceeds_noise flag (d >= 0.5 + reject)
    - run_null_analysis() orchestrates the full pipeline: null walk gen -> eval -> drift extraction -> MW-U -> Cohen's d -> HB -> MP KS test
    - All 21 tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement null overlay visualization, MP histogram, and render integration</name>
  <files>
    src/analysis/null_model.py
    src/visualization/null_overlay.py
    src/visualization/mp_histogram.py
    src/visualization/event_aligned.py
    src/visualization/render.py
  </files>
  <action>
**Create `src/visualization/null_overlay.py`:**

```python
"""Null distribution overlay on event-aligned SVD metric plots.

Renders a gray 95% CI band and solid gray median line representing the
null (jumper-free) Grassmannian drift distribution, overlaid on the
standard event-aligned violation/control plot.

Color scheme per CONTEXT.md: light gray band, solid gray median line.
"""
```

Constants (per CONTEXT.md locked decision):
```python
NULL_BAND_COLOR = (0.7, 0.7, 0.7, 0.3)   # light gray with alpha
NULL_MEDIAN_COLOR = (0.5, 0.5, 0.5, 1.0)  # solid gray
```

**Function 1: `compute_null_distribution_stats()`**

```python
def compute_null_distribution_stats(
    null_metric_array: np.ndarray,
    event_positions: list[int],
    window: int = 10,
) -> dict[int, dict[str, float]]:
    """Compute null distribution statistics at each position relative to events.

    For each relative position in [-window, +window], computes the median
    and 95% CI (2.5th and 97.5th percentiles) of the null metric values
    at the corresponding absolute positions.

    Args:
        null_metric_array: Shape [n_null_sequences, max_steps-1].
        event_positions: Absolute positions from violation events.
        window: Number of steps before/after event to include.

    Returns:
        Dict mapping relative position -> {"median": float, "ci_low": float, "ci_high": float}.
    """
```

Implementation:
1. For each relative position p in range(-window, window+1):
   - For each event_position: compute absolute_pos = event_position + p
   - Extract null_metric_array[:, absolute_pos] for all valid positions
   - Pool across all event positions
   - Compute median, 2.5th percentile, 97.5th percentile of the pooled NaN-filtered values
   - Store in result dict

**Function 2: `plot_event_aligned_with_null()`**

```python
def plot_event_aligned_with_null(
    metric_values: np.ndarray,
    events: list[AnalysisEvent],
    null_stats: dict[int, dict[str, float]],
    window: int = 10,
    metric_name: str = "SVD metric",
    ax: plt.Axes | None = None,
) -> plt.Figure:
    """Event-aligned plot with null distribution overlay.

    Renders the standard violation/control traces from plot_event_aligned(),
    then overlays the null distribution as:
    - Gray 95% CI band (shaded)
    - Solid gray median line

    Args:
        metric_values: Metric array [n_sequences, max_steps].
        events: List of AnalysisEvent records.
        null_stats: Output from compute_null_distribution_stats().
        window: Number of steps before/after event.
        metric_name: Label for Y-axis.
        ax: Optional axes.

    Returns:
        Matplotlib Figure.
    """
```

Implementation:
1. Call `plot_event_aligned(metric_values, events, window, metric_name, ax)` to render base plot
2. Get the axes from the figure
3. Build position, median, ci_low, ci_high arrays from null_stats
4. `ax.fill_between(positions, ci_low, ci_high, color=NULL_BAND_COLOR, label="Null 95% CI", zorder=1)`
5. `ax.plot(positions, medians, color=NULL_MEDIAN_COLOR, linewidth=1.5, linestyle='-', label="Null median", zorder=2)`
6. Update legend
7. Return fig

**Create `src/visualization/mp_histogram.py`:**

```python
"""Marchenko-Pastur density overlay on empirical QK^T singular value histogram.

Overlays the theoretical MP PDF curve on a histogram of empirical squared
singular values from QK^T, with KS test annotation.
"""
```

**Function: `plot_mp_histogram()`**

```python
def plot_mp_histogram(
    singular_values: np.ndarray,
    gamma: float,
    mp_result: dict,
    position_label: str = "event",
    ax: plt.Axes | None = None,
) -> plt.Figure:
    """Plot empirical SV histogram with MP density overlay.

    Args:
        singular_values: 1D array of empirical singular values from QK^T.
        gamma: Aspect ratio w / d_model.
        mp_result: Dict from run_mp_ks_test() with sigma2, lambda_minus, lambda_plus, ks_statistic, ks_p_value.
        position_label: Label for the position (e.g., "event", "pre_event_5").
        ax: Optional axes.

    Returns:
        Matplotlib Figure.
    """
```

Implementation:
1. Create figure/axes if needed
2. Plot histogram of `singular_values ** 2` (squared SVs = eigenvalue distribution), normalized (density=True), bins=30
3. Overlay MP PDF curve: compute x_range from lambda_minus to lambda_plus, evaluate `marchenko_pastur_pdf()` at 200 points, plot as solid line
4. Add text annotation: "KS = {ks_statistic:.3f}, p = {ks_p_value:.3f}"
5. Labels: x="Squared singular value", y="Density", title=f"QK^T SVs vs Marchenko-Pastur ({position_label})"

**Modify `src/visualization/event_aligned.py`:**

No structural changes needed. The existing `plot_event_aligned()` function is called by `plot_event_aligned_with_null()` without modification. However, ensure the function returns the figure and that the axes are accessible for overlay. Verify the existing function already works correctly for this purpose (it does -- it returns `fig` and `ax` is accessible via `fig.axes[0]`).

**Modify `src/visualization/render.py`:**

Add a new section after the existing event-aligned plots section (after the `PLOT-01` block). This section checks for `null_model` data in result.json and, if present, generates null overlay and MP histogram figures.

Add after the PLOT-01 event-aligned section (around line 184):

```python
    # ── NULL MODEL: Null overlay on event-aligned plots ───────────────
    null_model = result.get("metrics", {}).get("null_model")
    if null_model and failure_index is not None:
        try:
            from src.visualization.null_overlay import (
                plot_event_aligned_with_null,
                compute_null_distribution_stats,
            )
            # ... render null overlay figures for primary metrics
        except Exception as e:
            log.warning("Failed to generate null overlay plots: %s", e)

    # ── NULL MODEL: MP histogram ──────────────────────────────────────
    if null_model and "marchenko_pastur" in null_model:
        try:
            from src.visualization.mp_histogram import plot_mp_histogram
            # ... render MP histogram figures at anchor positions
        except Exception as e:
            log.warning("Failed to generate MP histogram: %s", e)
```

The null overlay rendering needs the null metric arrays, which are NOT stored in the main token_metrics.npz (null walks are standalone). Instead, the render function should:
- Check if a `null_token_metrics.npz` file exists in the result directory (written by run_null_analysis)
- If it exists, load null metric arrays and compute null_stats for each primary metric
- Generate `plot_event_aligned_with_null()` for each primary metric

**Important implementation note for run_null_analysis (Task 1):** When run_null_analysis completes, it should save `null_token_metrics.npz` in the experiment output directory so the render orchestrator can access null metric arrays. Add this to run_null_analysis's behavior -- it should accept an optional `output_dir` parameter and write null_token_metrics.npz there. If output_dir is None, skip writing (for testing).

Add `null_token_metrics.npz` writing to `run_null_analysis()` in `src/analysis/null_model.py`:
```python
def run_null_analysis(
    ...,
    output_dir: str | Path | None = None,
) -> dict:
    ...
    # Save null metric arrays for visualization
    if output_dir is not None:
        output_path = Path(output_dir)
        null_npz_data = {}
        for key, arr in null_eval_result.svd_metrics.items():
            null_npz_data[key] = arr
        np.savez_compressed(str(output_path / "null_token_metrics.npz"), **null_npz_data)
```

For render.py, load the null NPZ:
```python
    null_npz_path = result_dir / "null_token_metrics.npz"
    null_metric_arrays = {}
    if null_npz_path.exists():
        null_npz = np.load(str(null_npz_path), allow_pickle=False)
        null_metric_arrays = dict(null_npz)
```

Then for each primary metric:
```python
    if metric_key in null_metric_arrays:
        null_stats = compute_null_distribution_stats(
            null_metric_arrays[metric_key], event_positions, window=10
        )
        fig = plot_event_aligned_with_null(
            metric_arrays[metric_key], events, null_stats,
            window=10, metric_name=metric_key
        )
        safe_name = metric_key.replace(".", "_")
        paths = save_figure(fig, figures_dir, f"null_overlay_{safe_name}")
        generated_files.extend(paths)
```

For MP histogram, the run_mp_ks_test results are in result.json under `null_model.marchenko_pastur`. The raw singular values at anchor positions should also be saved in the null NPZ (add keys like `mp_svs_event`, `mp_svs_pre_event_5`, `mp_svs_post_event_5`).

Run `pytest tests/test_visualization.py -x -v` to check for regressions.
  </action>
  <verify>
    <automated>python -c "from src.visualization.null_overlay import plot_event_aligned_with_null, compute_null_distribution_stats; from src.visualization.mp_histogram import plot_mp_histogram; print('Visualization imports OK')" && pytest tests/test_visualization.py -x -v 2>&1 | tail -5</automated>
  </verify>
  <done>
    - null_overlay.py exports compute_null_distribution_stats() and plot_event_aligned_with_null()
    - Null overlay renders gray CI band + gray median line on event-aligned plots (per CONTEXT.md color scheme)
    - mp_histogram.py exports plot_mp_histogram() with MP density curve overlay and KS annotation
    - render.py loads null_token_metrics.npz when present and generates null overlay + MP histogram figures
    - No regressions in existing visualization tests
  </done>
</task>

<task type="auto">
  <name>Task 3: Extend result schema validation and HTML report with null model section</name>
  <files>
    src/results/schema.py
    src/reporting/single.py
    src/reporting/templates/single_report.html
  </files>
  <action>
**Modify `src/results/schema.py`:**

Add optional validation for the `null_model` block in `validate_result()`. This must be backward compatible -- if null_model is absent, no error. If present, validate structure.

Add after the `split_assignment` validation block (after line ~96):

```python
    # Optional null_model validation (Phase 12, backward compatible)
    if "metrics" in result and isinstance(result["metrics"], dict):
        null_model = result["metrics"].get("null_model")
        if null_model is not None:
            if not isinstance(null_model, dict):
                errors.append("metrics.null_model must be a dict")
            else:
                # Validate required sub-blocks
                for block_name in ["config", "by_lookback", "aggregate"]:
                    if block_name not in null_model:
                        errors.append(
                            f"metrics.null_model missing required block: {block_name}"
                        )
                # Validate config fields
                nm_config = null_model.get("config", {})
                if isinstance(nm_config, dict):
                    for field in ["n_null_walks", "n_violation_walks", "null_seed", "alpha"]:
                        if field not in nm_config:
                            errors.append(
                                f"metrics.null_model.config missing field: {field}"
                            )
                # Validate aggregate fields
                nm_agg = null_model.get("aggregate", {})
                if isinstance(nm_agg, dict):
                    for field in ["n_lookbacks_tested", "signal_exceeds_noise"]:
                        if field not in nm_agg:
                            errors.append(
                                f"metrics.null_model.aggregate missing field: {field}"
                            )
```

**Modify `src/reporting/templates/single_report.html`:**

Add a new "Null Model Baseline" section after the existing "Statistical Tests" section. This section should contain:
1. A summary paragraph indicating signal_exceeds_noise outcome
2. A statistical summary table with columns: Lookback j, Null Mean +/- Std, Violation Mean +/- Std, Mann-Whitney U, p-value (raw), p-value (adjusted), Cohen's d, Reject?
3. Aggregate summary: max Cohen's d, number of rejected lookbacks
4. Marchenko-Pastur subsection with gamma, lambda bounds, and KS test results per anchor position
5. Figure slots for null overlay plots and MP histogram

Template block (Jinja2):
```html
{% if null_model %}
<h2>Null Model Baseline</h2>

<p class="summary">
  {% if null_model.aggregate.signal_exceeds_noise %}
  <strong>Signal exceeds noise:</strong> The Grassmannian drift signal at violation events
  is statistically distinguishable from the null (jumper-free) distribution.
  {{ null_model.aggregate.n_lookbacks_rejected }} of {{ null_model.aggregate.n_lookbacks_tested }}
  lookback distances rejected (Holm-Bonferroni corrected, &alpha; = {{ null_model.config.alpha }}).
  Maximum Cohen's d = {{ "%.3f"|format(null_model.aggregate.max_cohens_d) }} at lookback j={{ null_model.aggregate.max_cohens_d_lookback }}.
  {% else %}
  <strong>Signal does not exceed noise:</strong> The Grassmannian drift at violation events
  is not statistically distinguishable from the null distribution with Cohen's d &ge; 0.5.
  {% endif %}
</p>

<h3>Per-Lookback Statistical Comparison</h3>
<table>
  <thead>
    <tr>
      <th>Lookback j</th>
      <th>Null (mean &plusmn; std)</th>
      <th>Violation (mean &plusmn; std)</th>
      <th>Mann-Whitney U</th>
      <th>p (raw)</th>
      <th>p (adjusted)</th>
      <th>Cohen's d</th>
      <th>Reject?</th>
    </tr>
  </thead>
  <tbody>
    {% for j, data in null_model.by_lookback|dictsort %}
    <tr>
      <td>{{ j }}</td>
      <td>{{ "%.4f"|format(data.null_mean) }} &plusmn; {{ "%.4f"|format(data.null_std) }}</td>
      <td>{{ "%.4f"|format(data.violation_mean) }} &plusmn; {{ "%.4f"|format(data.violation_std) }}</td>
      <td>{{ "%.1f"|format(data.mann_whitney_U) }}</td>
      <td>{{ "%.4f"|format(data.p_value_raw) }}</td>
      <td>{{ "%.4f"|format(data.p_value_adjusted) }}</td>
      <td>{{ "%.3f"|format(data.cohens_d) }}</td>
      <td>{{ "Yes" if data.reject else "No" }}</td>
    </tr>
    {% endfor %}
  </tbody>
</table>

{% if null_model.marchenko_pastur %}
<h3>Marchenko-Pastur Reference</h3>
<p>
  Aspect ratio &gamma; = {{ "%.3f"|format(null_model.marchenko_pastur.gamma) }},
  &sigma;&sup2; = {{ "%.6f"|format(null_model.marchenko_pastur.sigma2) }},
  &lambda;<sub>&minus;</sub> = {{ "%.6f"|format(null_model.marchenko_pastur.lambda_minus) }},
  &lambda;<sub>+</sub> = {{ "%.6f"|format(null_model.marchenko_pastur.lambda_plus) }}.
</p>

<table>
  <thead>
    <tr><th>Anchor Position</th><th>KS Statistic</th><th>KS p-value</th></tr>
  </thead>
  <tbody>
    {% for pos, data in null_model.marchenko_pastur.anchor_positions|dictsort %}
    <tr>
      <td>{{ pos }}</td>
      <td>{{ "%.4f"|format(data.ks_statistic) }}</td>
      <td>{{ "%.4f"|format(data.ks_p_value) }}</td>
    </tr>
    {% endfor %}
  </tbody>
</table>
{% endif %}

{% if null_overlay_figures %}
<h3>Null Overlay Plots</h3>
{% for fig in null_overlay_figures %}
<figure>
  <img src="{{ fig.data_uri }}" alt="{{ fig.title }}">
  <figcaption>{{ fig.title }}</figcaption>
</figure>
{% endfor %}
{% endif %}

{% if mp_histogram_figures %}
<h3>Marchenko-Pastur Histograms</h3>
{% for fig in mp_histogram_figures %}
<figure>
  <img src="{{ fig.data_uri }}" alt="{{ fig.title }}">
  <figcaption>{{ fig.title }}</figcaption>
</figure>
{% endfor %}
{% endif %}

{% endif %}
```

**Modify `src/reporting/single.py`:**

1. In `_collect_figures()`, add new figure categories for null overlay and MP histogram:
```python
result["null_overlay_figures"] = []
result["mp_histogram_figures"] = []
```
And in the PNG scanning loop:
```python
elif name.startswith("null_overlay"):
    result["null_overlay_figures"].append({"title": title, "data_uri": data_uri})
elif name.startswith("mp_histogram"):
    result["mp_histogram_figures"].append({"title": title, "data_uri": data_uri})
```

2. In `generate_single_report()`, extract null_model data and pass to template:
```python
    # Null model data (Phase 12)
    null_model = metrics.get("null_model")
```

3. Add null_model and the new figure lists to the template.render() call:
```python
    html = template.render(
        ...,
        null_model=null_model,
        null_overlay_figures=figures.get("null_overlay_figures", []),
        mp_histogram_figures=figures.get("mp_histogram_figures", []),
    )
```

Run tests:
```bash
pytest tests/test_results.py tests/test_reporting.py -x -v
```
  </action>
  <verify>
    <automated>pytest tests/test_results.py tests/test_reporting.py -x -v 2>&1 | tail -10</automated>
  </verify>
  <done>
    - validate_result() accepts null_model block when present, validates structure (config, by_lookback, aggregate)
    - validate_result() allows null_model to be absent (backward compatible)
    - single_report.html template renders Null Model Baseline section with statistical summary table and figure slots
    - generate_single_report() passes null_model data and figure lists to template
    - _collect_figures() categorizes null_overlay_* and mp_histogram_* figures correctly
    - No regressions in existing result or reporting tests
  </done>
</task>

</tasks>

<verification>
```bash
# All null model tests pass (including new statistical comparison tests)
pytest tests/test_null_model.py -x -v

# No regressions in full test suite
pytest tests/ -x --timeout=120

# All expected exports are available
python -c "
from src.analysis.null_model import (
    generate_null_walks, run_null_evaluation, extract_position_matched_drift,
    marchenko_pastur_pdf, marchenko_pastur_cdf, run_mp_ks_test,
    compare_null_vs_violation, run_null_analysis,
)
from src.visualization.null_overlay import plot_event_aligned_with_null, compute_null_distribution_stats
from src.visualization.mp_histogram import plot_mp_histogram
print('All exports OK')
"

# Schema accepts null_model block
python -c "
from src.results.schema import validate_result
result = {
    'schema_version': '1.0', 'experiment_id': 'test', 'timestamp': '2026-02-26T00:00:00',
    'description': 'test', 'tags': [], 'config': {}, 'metrics': {'scalars': {},
    'null_model': {
        'config': {'n_null_walks': 500, 'n_violation_walks': 100, 'null_seed': 9999, 'alpha': 0.05},
        'by_lookback': {}, 'aggregate': {'n_lookbacks_tested': 5, 'signal_exceeds_noise': True,
        'n_lookbacks_rejected': 3, 'max_cohens_d': 1.2, 'max_cohens_d_lookback': 3},
    }}
}
errors = validate_result(result)
assert not errors, f'Validation errors: {errors}'
print('Schema validation OK')
"

# Schema still accepts results WITHOUT null_model (backward compat)
python -c "
from src.results.schema import validate_result
result = {
    'schema_version': '1.0', 'experiment_id': 'test', 'timestamp': '2026-02-26T00:00:00',
    'description': 'test', 'tags': [], 'config': {}, 'metrics': {'scalars': {}}
}
errors = validate_result(result)
assert not errors, f'Validation errors: {errors}'
print('Backward compat OK')
"

# Template renders without errors (dry-run with minimal data)
python -c "
from jinja2 import Environment, FileSystemLoader, select_autoescape
from pathlib import Path
template_dir = Path('src/reporting/templates')
env = Environment(loader=FileSystemLoader(str(template_dir)), autoescape=select_autoescape(['html']))
template = env.get_template('single_report.html')
html = template.render(
    experiment_id='test', timestamp='', description='',
    config_tables={}, scalar_metrics={},
    training_curves_figure=None, confusion_matrix_figure=None,
    auroc_figures=[], event_aligned_figures=[], distribution_figures=[],
    statistical_tests=[], predictive_horizon=[], sequence_analysis=None,
    reproduction='', null_model=None, null_overlay_figures=[], mp_histogram_figures=[],
)
assert 'Null Model' not in html  # null_model is None, section should not render
print('Template render OK (no null model)')
"
```
</verification>

<success_criteria>
- compare_null_vs_violation() computes MW-U and Cohen's d at each lookback with proper Holm-Bonferroni (separate family)
- run_null_analysis() orchestrates the full null model pipeline and returns result.json-ready dict
- Null overlay renders gray CI band + median line on event-aligned plots
- MP histogram overlays theoretical density on empirical SV histogram with KS annotation
- result.json null_model block validates with proper structure
- HTML report includes Null Model Baseline section when null_model data present
- All tests pass; no regressions; backward compatible (works without null_model)
</success_criteria>

<output>
After completion, create `.planning/phases/12-null-model-baseline/12-02-SUMMARY.md`
</output>
