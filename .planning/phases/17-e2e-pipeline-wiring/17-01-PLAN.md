---
phase: 17
plan: 1
title: "Wire run_experiment.py end-to-end pipeline"
wave: 1
depends_on: []
files_modified:
  - run_experiment.py
  - src/visualization/__init__.py
  - Makefile
  - tests/test_e2e_pipeline.py
autonomous: true
requirements:
  - "Gap closure for v1.0 audit integration breaks"
---

# Plan 17-01: Wire run_experiment.py end-to-end pipeline

## Objective

Replace the stub in `run_experiment.py` with a complete E2E pipeline that chains all experiment stages — graph generation, walk generation, model creation, training, evaluation, analysis, visualization, and reporting — into a single executable command. Close all P0/P1/P2 integration gaps from the v1.0 milestone audit.

## must_haves
- `python run_experiment.py --config config.json` executes full pipeline from config to HTML report
- `set_seed(config.seed)` called at pipeline start before any stochastic operations
- Predictive horizon analysis results written to result.json `predictive_horizon` block
- NPZ keys consistent between write_result and save_evaluation_results (dual-key emission verified)
- Stage banners printed with elapsed time
- Partial results saved before pipeline failure
- `--verbose` flag for DEBUG-level logging
- `--dry-run` shows full pipeline plan with expected output paths
- Config copied into output directory for reproducibility
- `src/visualization/__init__.py` exports public API via `__all__`
- Makefile `pdf` target calls `generate_math_pdf`; `report` target runs full pipeline

## Tasks

<task id="1">
<title>Implement full pipeline in run_experiment.py</title>
<type>code</type>
<files>
  - run_experiment.py
</files>
<description>
Replace the stub in `run_experiment.py` with the full E2E pipeline. The pipeline function should:

**1. Argument parsing enhancements:**
- Keep existing `--config` and `--dry-run` flags
- Add `--verbose` flag that sets logging to DEBUG level

**2. Pipeline initialization:**
```python
import logging
import time
import json
import shutil
from pathlib import Path

from src.reproducibility import set_seed, get_git_hash
from src.config import config_from_json, full_config_hash, config_hash
from src.results import generate_experiment_id, write_result, load_result, validate_result
from src.graph import generate_or_load_graph
from src.walk import generate_or_load_walks
from src.model import create_model, TransformerLM
from src.training import run_training_pipeline
from src.evaluation import fused_evaluate, save_evaluation_results
from src.evaluation.split import assign_split
from src.analysis.auroc_horizon import run_auroc_analysis
from src.analysis.statistical_controls import apply_statistical_controls
from src.visualization.render import render_all
from src.reporting import generate_single_report
```

**3. Logging setup:**
- Default: INFO level with format `"%(asctime)s %(levelname)s %(name)s: %(message)s"`
- `--verbose`: DEBUG level
- Log seed, experiment_id, git hash at startup

**4. Stage execution with timing banners:**
Create a context manager or helper that prints:
```
=== {Stage Name} ===
... done in {elapsed:.1f}s
```

**5. Pipeline stages (in order):**

a. **Seed:** `set_seed(config.seed)` — MUST be before any model/data creation
   Log: `"Seed: {config.seed}"`

b. **Device selection:** `torch.device("cuda" if torch.cuda.is_available() else "cpu")`

c. **Graph generation:** `graph_data, jumpers = generate_or_load_graph(config)`

d. **Walk generation:**
   ```python
   from src.walk.corpus import generate_corpus, TRAIN_SEED_OFFSET, EVAL_SEED_OFFSET
   train_result = generate_corpus(config, graph_data, jumpers, seed_offset=TRAIN_SEED_OFFSET)
   eval_result = generate_corpus(config, graph_data, jumpers, seed_offset=EVAL_SEED_OFFSET)
   train_walks = train_result.walks
   eval_walks = eval_result.walks
   ```
   Note: Check if `generate_or_load_walks` exists in the walk cache module. If it does, use it instead. If not, use `generate_corpus` directly. The walk module exports `generate_or_load_walks` — use that.

e. **Output directory setup:**
   ```python
   experiment_id = generate_experiment_id(config)
   results_dir = Path("results")
   output_dir = results_dir / experiment_id
   output_dir.mkdir(parents=True, exist_ok=True)
   ```

f. **Copy config to output dir:**
   ```python
   shutil.copy2(config_path, output_dir / "config.json")
   ```

g. **Model creation:**
   ```python
   model = create_model(config)
   model = model.to(device)
   ```

h. **Training:**
   ```python
   training_result = run_training_pipeline(
       model=model,
       train_walks=train_walks,
       eval_walks=eval_walks,
       graph_data=graph_data,
       jumpers=jumpers,
       config=config,
       device=device,
       results_dir=str(results_dir),
   )
   ```
   If gate fails: log warning but continue (evaluation still useful for analysis)

i. **Evaluation:**
   ```python
   model.eval()
   eval_result = fused_evaluate(
       model=model,
       eval_walks=eval_walks,
       graph_data=graph_data,
       jumpers=jumpers,
       config=config,
       device=device,
   )
   ```

j. **Split assignment:**
   ```python
   split_labels = assign_split(eval_result.failure_index)
   ```

k. **Save evaluation results (writes NPZ):**
   ```python
   eval_summary = save_evaluation_results(
       eval_result,
       output_dir=str(output_dir),
       split_labels=split_labels,
   )
   ```

l. **AUROC analysis:**
   ```python
   # Get metric keys from the NPZ data
   metric_keys = [
       k for k in eval_result.svd_metrics.keys()
       if "." in k and k.split(".")[0] in ("qkt", "avwo", "wvwo")
   ]
   jumper_map = {j.vertex_id: j for j in jumpers}

   # Build eval_result_data dict expected by run_auroc_analysis
   eval_result_data = {
       "generated": eval_result.generated,
       "rule_outcome": eval_result.rule_outcome,
       "failure_index": eval_result.failure_index,
       "sequence_lengths": eval_result.sequence_lengths,
   }
   eval_result_data.update(eval_result.svd_metrics)

   predictive_horizon = run_auroc_analysis(
       eval_result_data=eval_result_data,
       jumper_map=jumper_map,
       metric_keys=metric_keys,
   )
   ```

m. **Statistical controls:**
   ```python
   statistical_controls = apply_statistical_controls(
       auroc_results=predictive_horizon,
       eval_result_data=eval_result_data,
       metric_keys=metric_keys,
   )
   ```

n. **Update result.json with analysis blocks:**
   ```python
   result_json_path = output_dir / "result.json"
   with open(result_json_path) as f:
       result_data = json.load(f)

   result_data["metrics"]["predictive_horizon"] = predictive_horizon
   result_data["metrics"]["statistical_controls"] = statistical_controls
   result_data["metrics"].update(eval_summary)
   result_data["metadata"]["seed"] = config.seed
   result_data["metadata"]["git_hash"] = get_git_hash()

   with open(result_json_path, "w") as f:
       json.dump(result_data, f, indent=2)
   ```

o. **Visualization:**
   ```python
   figures = render_all(str(output_dir))
   ```

p. **Reporting:**
   ```python
   report_path = generate_single_report(str(output_dir))
   ```

q. **Final summary:**
   Print total elapsed time, output directory path, number of figures generated, report path.

**6. Error handling:**
- Wrap each stage in try/except
- On error: save partial results (whatever has been written so far), print traceback, re-raise
- Use `logging` module throughout

**7. Dry-run mode:**
Enhance `--dry-run` to show:
```
Pipeline plan for experiment {experiment_id}:
  1. Set seed: {config.seed}
  2. Graph generation: n={n}, K={K}, ...
  3. Walk generation: corpus_size={corpus_size}, walk_length={walk_length}
  4. Model creation: d_model={d_model}, n_layers={n_layers}, n_heads={n_heads}
  5. Training: max_steps={max_steps}, batch_size={batch_size}
  6. Evaluation: fused SVD + behavioral
  7. Analysis: AUROC horizon + statistical controls
  8. Visualization: all plots to figures/
  9. Reporting: HTML report

Output: results/{experiment_id}/
  - result.json
  - token_metrics.npz
  - figures/ (PNG + SVG)
  - report.html
  - config.json (copy)
```

**Implementation notes:**
- The training pipeline (`run_training_pipeline`) already writes an initial result.json. The evaluation/analysis stages update this file in-place by loading, merging, and re-writing.
- `save_evaluation_results` writes `token_metrics.npz` to the output directory.
- `render_all` reads from `result.json` and `token_metrics.npz` in the directory.
- `generate_single_report` reads the same data plus figures.
- Check `generate_or_load_walks` in `src/walk/cache.py` for exact API. It likely takes `(config, graph_data, jumpers)` and returns train/eval walks.
</description>
</task>

<task id="2">
<title>Fix visualization __init__.py and Makefile targets</title>
<type>code</type>
<files>
  - src/visualization/__init__.py
  - Makefile
</files>
<description>
**1. Fix `src/visualization/__init__.py`:**

Replace the single-line docstring with proper exports:

```python
"""Publication-quality static figure generation for DCSBM-transformer experiments.

Provides render_all() to generate all figures for a single experiment,
with individual plot modules for each visualization type.
"""

from src.visualization.render import load_result_data, render_all
from src.visualization.style import apply_style, save_figure

__all__ = [
    "render_all",
    "load_result_data",
    "apply_style",
    "save_figure",
]
```

**2. Wire Makefile targets:**

Update the Makefile to:

```makefile
.PHONY: test run sweep pdf report clean

# Run all tests
test:
	python -m pytest tests/ -v

# Run anchor config experiment (requires config.json)
run:
	python run_experiment.py --config config.json

# Run parameter sweep (placeholder -- implemented in Phase 10)
sweep:
	@echo "Sweep not yet implemented (Phase 10)"

# Generate math verification PDF
pdf:
	python -c "from src.reporting import generate_math_pdf; generate_math_pdf()"

# Run full pipeline (alias for run)
report:
	python run_experiment.py --config config.json

# Clean generated artifacts
clean:
	find . -type d -name __pycache__ -exec rm -rf {} + 2>/dev/null || true
	find . -type d -name .pytest_cache -exec rm -rf {} + 2>/dev/null || true
	rm -rf results/
```
</description>
</task>

<task id="3">
<title>Write E2E pipeline integration test</title>
<type>test</type>
<files>
  - tests/test_e2e_pipeline.py
</files>
<description>
Create a comprehensive integration test for the E2E pipeline. Use tiny config values to keep tests fast (under 30 seconds).

```python
"""Integration tests for the end-to-end experiment pipeline.

Tests the full pipeline from config loading through report generation
using tiny configurations for fast execution.
"""

import json
import subprocess
import sys
from pathlib import Path

import numpy as np
import pytest


# Tiny config for fast E2E testing
TINY_CONFIG = {
    "graph": {
        "n": 20,
        "K": 2,
        "p_in": 0.5,
        "p_out": 0.05,
        "n_jumpers_per_block": 1,
    },
    "model": {
        "d_model": 16,
        "n_layers": 1,
        "n_heads": 1,
        "dropout": 0.0,
    },
    "training": {
        "w": 8,
        "walk_length": 16,
        "corpus_size": 2000,
        "r": 5,
        "learning_rate": 3e-4,
        "batch_size": 16,
        "max_steps": 50000,
        "eval_interval": 1000,
        "checkpoint_interval": 5000,
    },
    "seed": 42,
    "description": "E2E pipeline test",
    "tags": ["test", "e2e"],
}
```

**Test cases:**

1. **test_dry_run**: Write tiny config to tmp dir, run `python run_experiment.py --config {path} --dry-run`, assert exit code 0, assert output contains "Pipeline plan" and stage names.

2. **test_set_seed_called**: Import `run_experiment` module, verify `set_seed` is called before model creation by checking the pipeline function imports and flow. (Or: run with tiny config and verify deterministic behavior by running twice with same seed.)

3. **test_full_pipeline**: Write config to tmpdir, run the pipeline programmatically (import main function or use subprocess). Verify:
   - result.json exists in `results/{experiment_id}/`
   - result.json validates (no schema errors)
   - result.json has `predictive_horizon` key in metrics
   - result.json has `metadata.seed` field
   - token_metrics.npz exists with SVD metric keys
   - figures/ directory exists with at least 1 figure
   - report.html exists
   - config.json copy exists in output dir

4. **test_config_copy**: Verify config.json in output dir matches input config.

5. **test_result_json_schema**: Load result.json, run `validate_result`, assert no errors.

6. **test_npz_key_consistency**: Load token_metrics.npz, verify keys use dotted format (target.layer_N.metric_name), not slash format.

**Implementation notes:**
- Use `tmp_path` pytest fixture for config file and override results_dir
- Use `monkeypatch` to set results_dir to tmp_path if needed
- The full pipeline test may need to be marked with `@pytest.mark.slow` if it takes > 10s
- Use tiny config values so training is nearly instant (1 epoch)
- Mock or skip GPU-dependent parts if no CUDA available
</description>
</task>

## Verification

After all tasks complete, verify:

1. **SC1:** `python run_experiment.py --config config.json` runs full pipeline
   - Run with a test config, verify it completes without error
   - Verify output directory contains result.json, NPZ, figures/, report.html

2. **SC2:** `set_seed(config.seed)` called at pipeline start
   - Inspect run_experiment.py source, confirm set_seed is called before model creation
   - Run twice with same seed, verify same experiment_id

3. **SC3:** Predictive horizon in result.json
   - Load result.json, verify `metrics.predictive_horizon` exists and is non-empty dict
   - Verify it has `config`, `by_r_value` sub-keys

4. **SC4:** NPZ key format consistency
   - Load token_metrics.npz, verify keys use dotted format
   - Verify both legacy (3-part) and per-head (4-part) keys exist for single-head runs

5. **All tests pass:** `python -m pytest tests/test_e2e_pipeline.py -v`

---

*Phase: 17-e2e-pipeline-wiring*
*Plan: 17-01*
