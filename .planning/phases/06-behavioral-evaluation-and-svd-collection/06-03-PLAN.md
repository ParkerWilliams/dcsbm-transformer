---
phase: 06-behavioral-evaluation-and-svd-collection
plan: 03
type: execute
wave: 2
depends_on:
  - "06-01"
  - "06-02"
files_modified:
  - src/evaluation/pipeline.py
  - tests/test_evaluation_pipeline.py
autonomous: true
requirements:
  - EVAL-05
  - SVD-01
  - SVD-04
  - SVD-06

must_haves:
  truths:
    - "A single fused forward pass produces both behavioral labels and SVD metrics without separate inference runs"
    - "SVD is computed on three targets per layer at every step: QK^T (routing), WvWo (OV circuit, static), AVWo (net residual update)"
    - "SVD metrics are stored as token-level time series in token_metrics.npz keyed by target.layer.metric"
    - "SVD metrics are NaN for positions < w (context warmup) and real values for positions >= w"
    - "Tail extension resolves late jumper encounters: if jumper at position > 4w-r, extend to encounter+r+1"
    - "result.json contains aggregate summaries: mean/std per metric, guard activation counts, failure_index list"
  artifacts:
    - path: "src/evaluation/pipeline.py"
      provides: "Fused evaluation pipeline: autoregressive generation with SVD extraction and behavioral labeling"
      exports: ["fused_evaluate", "EvaluationResult"]
    - path: "tests/test_evaluation_pipeline.py"
      provides: "Integration tests for fused pipeline, NPZ output, warmup skip"
      min_lines: 100
  key_links:
    - from: "src/evaluation/pipeline.py"
      to: "src/evaluation/svd_metrics.py"
      via: "compute_all_metrics and guard_matrix_for_svd"
      pattern: "from src\\.evaluation\\.svd_metrics import"
    - from: "src/evaluation/pipeline.py"
      to: "src/evaluation/behavioral.py"
      via: "classify_steps for behavioral labels"
      pattern: "from src\\.evaluation\\.behavioral import"
    - from: "src/evaluation/pipeline.py"
      to: "src/model/transformer.py"
      via: "ExtractionMode.SVD_TARGETS for fused extraction"
      pattern: "ExtractionMode\\.SVD_TARGETS"
    - from: "src/evaluation/pipeline.py"
      to: "src/model/transformer.py"
      via: "model.get_wvwo() for static OV circuit SVD"
      pattern: "get_wvwo"
---

<objective>
Implement the fused evaluation pipeline that generates sequences autoregressively while simultaneously collecting SVD metrics across three targets and behavioral labels in a single inference pass.

Purpose: This is the core data-collection orchestrator for Phase 6. It combines the SVD metric functions (06-01) and behavioral classifier (06-02) into a fused generation loop that extracts model internals at every step using ExtractionMode.SVD_TARGETS. The output is a complete token_metrics.npz and result.json summary that Phase 7 consumes for predictive horizon analysis.

Output: `src/evaluation/pipeline.py` with fused_evaluate function and EvaluationResult dataclass, `tests/test_evaluation_pipeline.py` with integration tests.
</objective>

<execution_context>
@/root/.claude/get-shit-done/workflows/execute-plan.md
@/root/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-behavioral-evaluation-and-svd-collection/06-RESEARCH.md
@.planning/phases/06-behavioral-evaluation-and-svd-collection/06-01-SUMMARY.md
@.planning/phases/06-behavioral-evaluation-and-svd-collection/06-02-SUMMARY.md
@src/model/types.py
@src/model/transformer.py
@src/training/evaluate.py
@src/evaluation/svd_metrics.py
@src/evaluation/behavioral.py
@src/results/schema.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement fused evaluation pipeline with SVD collection</name>
  <files>
    src/evaluation/pipeline.py
    src/evaluation/__init__.py
  </files>
  <action>
Create `src/evaluation/pipeline.py` implementing the fused evaluation pipeline.

**EvaluationResult dataclass:**
```python
@dataclass
class EvaluationResult:
    generated: np.ndarray           # [n_sequences, max_steps] generated token IDs
    edge_valid: np.ndarray          # [n_sequences, max_steps-1] bool
    rule_outcome: np.ndarray        # [n_sequences, max_steps-1] int (RuleOutcome)
    failure_index: np.ndarray       # [n_sequences] int (-1 = no failure)
    svd_metrics: dict[str, np.ndarray]  # {target.layer.metric: [n_sequences, max_steps-1]}
    guard_activations: dict[str, int]   # {target.layer: count of guard activations}
    sequence_lengths: np.ndarray    # [n_sequences] actual length per sequence
```

**fused_evaluate function:**
```python
def fused_evaluate(
    model: nn.Module,
    eval_walks: np.ndarray,
    graph_data: GraphData,
    jumpers: list[JumperInfo],
    config: ExperimentConfig,
    device: torch.device,
    batch_size: int = 32,
) -> EvaluationResult:
```

Implementation details:

1. **Setup:**
   - Build jumper_map: `{j.vertex_id: j for j in jumpers}`
   - Compute default_length = 4 * config.training.w (default sequence length per CONTEXT.md)
   - Compute max_r = max(j.r for j in jumpers) (for tail extension upper bound)
   - Compute max_possible_length = default_length + max_r + 1 (maximum possible with tail extension)
   - w = config.training.w (context window, also warmup threshold per SVD-06)
   - n_layers = config.model.n_layers
   - Compute WvWo SVD once: `wvwo = model.get_wvwo()` -> per-layer SVD and metrics via compute_all_metrics (static, broadcast to all steps)
   - Also compute Grassmannian distance = NaN for WvWo (static matrix, no step-to-step change)

2. **Pre-allocate output arrays:**
   - All SVD metric arrays: NaN-filled float32 [n_sequences, max_possible_length - 1]
   - Keyed as `{target}.layer_{i}.{metric_name}` (e.g., `qkt.layer_0.stable_rank`)
   - Behavioral arrays: edge_valid, rule_outcome, failure_index
   - sequence_lengths array

3. **Batched autoregressive generation loop:**
   - Process eval walks in batches of batch_size
   - For each batch:
     a. Seed with first token of each eval walk: `start_tokens = eval_walks[batch_start:batch_end, :1]`
     b. Initialize generated tensor: `[B, 1]` on device
     c. Track per-sequence: active jumper constraints, whether tail extension is active, current target length
     d. **Per-step loop** (up to max_possible_length - 1 steps):
        - Context window: `context = generated[:, -w:]` (last w tokens)
        - Forward pass with extraction: `output = model(context, mode=ExtractionMode.SVD_TARGETS)`
        - Next token: `output.logits[:, -1, :].argmax(dim=-1, keepdim=True)`
        - Append to generated
        - **Tail extension check**: For each sequence, if the newly generated token is a jumper vertex at position > default_length - max_r, extend that sequence's target length to encounter_step + jumper.r + 1
        - **SVD collection (only for step >= w per SVD-06):**
          - QK^T: extract `output.qkt[:, layer, :, :]` for each layer, guard_matrix_for_svd, torch.linalg.svd(full_matrices=False), compute_all_metrics
          - AVWo: compute `A @ V @ W_o.weight.T` per layer (attention_weights @ values @ W_o.T), guard and SVD
          - Grassmannian distance: compare top-k singular vectors with previous step (retain U_prev per layer per target)
          - Store scalar metrics in pre-allocated arrays
          - **Compute-and-discard**: do NOT retain large SVD intermediates across steps
        - **Stop condition**: All sequences in batch have reached their target length
     e. After generation loop: call `classify_steps(generated, graph_data, jumper_map)` for behavioral labels
     f. Copy results into pre-allocated output arrays

4. **Memory management (Claude's discretion):**
   - Process in batches (default 32, can reduce to 8 on OOM)
   - Compute-and-discard: SVD intermediates freed per step
   - Retain only U_prev (top-k columns) for Grassmannian across steps
   - Move completed sequences to CPU periodically

5. **NaN padding convention:**
   - Positions < w: NaN (warmup per SVD-06)
   - Positions >= sequence_length: NaN (tail extension variance)
   - Only positions in [w, sequence_length-1) have real SVD metrics

Update `src/evaluation/__init__.py` to export `fused_evaluate`, `EvaluationResult`.

Commit: `feat(06-03): implement fused evaluation pipeline with SVD collection`
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -c "from src.evaluation.pipeline import fused_evaluate, EvaluationResult; print('imports OK')"</automated>
    <manual>Module imports without error, EvaluationResult and fused_evaluate are accessible</manual>
    <sampling_rate>run after this task commits, before next task begins</sampling_rate>
  </verify>
  <done>src/evaluation/pipeline.py exists with fused_evaluate function implementing the complete fused generation + SVD collection + behavioral labeling loop</done>
</task>

<task type="auto">
  <name>Task 2: Integration tests for fused pipeline, NPZ output, and warmup skip</name>
  <files>
    tests/test_evaluation_pipeline.py
    src/evaluation/pipeline.py
  </files>
  <action>
Create `tests/test_evaluation_pipeline.py` with integration tests using a small model and synthetic graph.

**Test fixtures (shared):**
- Small TransformerLM: vocab_size=10, d_model=16, n_layers=2, max_seq_len=8 (small w=8 for fast tests)
- Small graph: 10 vertices, 2 blocks, known adjacency
- 2 jumpers with r=3 and r=4
- Short eval walks: 5 walks of length 16

**TestFusedPass:**
- `test_fused_returns_correct_shapes`: Run fused_evaluate, verify generated shape [n_seq, L], edge_valid shape [n_seq, L-1], rule_outcome shape [n_seq, L-1], failure_index shape [n_seq]
- `test_fused_svd_metric_keys`: Verify svd_metrics dict has keys for all 3 targets x 2 layers x 8 metrics (e.g., qkt.layer_0.stable_rank, wvwo.layer_1.spectral_entropy, avwo.layer_0.grassmannian_distance)
- `test_fused_svd_metric_shapes`: Each SVD metric array has shape [n_seq, max_steps-1]
- `test_single_forward_pass`: Verify the model's forward is called with ExtractionMode.SVD_TARGETS (mock or check output.qkt is not None). This confirms EVAL-05 (fused, not separate inference).

**TestWarmupSkip:**
- `test_warmup_positions_are_nan`: For positions < w, all SVD metric values should be NaN
- `test_post_warmup_positions_have_values`: For positions >= w (within sequence length), SVD metrics should be finite (not NaN)

**TestNPZOutput:**
- `test_save_and_load_token_metrics`: Create an EvaluationResult, save its svd_metrics + behavioral arrays to NPZ via np.savez_compressed, reload, verify all keys present and values match
- `test_npz_key_convention`: Keys follow `target.layer_N.metric_name` pattern
- `test_behavioral_arrays_in_npz`: edge_valid, rule_outcome, failure_index stored correctly

**TestSVDTargets:**
- `test_qkt_svd_produces_finite_metrics`: QK^T metrics for positions >= w are all finite
- `test_wvwo_svd_is_static`: WvWo metrics are identical across all steps (static weight matrix)
- `test_avwo_svd_differs_from_qkt`: AVWo metrics are not identical to QK^T metrics (different targets)

**TestTailExtension:**
- `test_tail_extension_on_late_jumper`: Create a scenario where a jumper is encountered at position > default_length - max_r. Verify the generated sequence is longer than default_length for that walk.
- `test_no_extension_without_late_jumper`: Sequences without late jumper encounters have length = default_length

**TestGuardActivations:**
- `test_guard_activations_counted`: Verify guard_activations dict exists and counts are >= 0

Also add a `save_evaluation_results` helper function to `src/evaluation/pipeline.py` that:
- Writes token_metrics.npz with all SVD metric arrays + behavioral arrays + failure_index
- Returns a metrics summary dict for result.json: mean/std per metric (across valid positions), guard activation counts, failure_index list

Run all tests. Commit: `feat(06-03): add integration tests and NPZ output for fused pipeline`
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -m pytest tests/test_evaluation_pipeline.py tests/test_svd_metrics.py tests/test_behavioral.py -x -v</automated>
    <manual>All Phase 6 tests pass. Check that WvWo metrics are indeed static across steps.</manual>
    <sampling_rate>run after this task commits, before next task begins</sampling_rate>
  </verify>
  <done>All integration tests pass. Fused pipeline produces correct shapes, warmup skip works, NPZ output is well-formed, SVD targets are distinct, tail extension works, guard activations are counted. Full test suite has zero regressions.</done>
</task>

</tasks>

<verification>
1. `pytest tests/test_evaluation_pipeline.py -v` -- all integration tests pass
2. `pytest tests/test_svd_metrics.py tests/test_behavioral.py tests/test_evaluation_pipeline.py -v` -- all Phase 6 tests pass
3. `pytest tests/ -x` -- full suite passes with zero regressions
4. Fused evaluation uses ExtractionMode.SVD_TARGETS (EVAL-05)
5. SVD computed on 3 targets x all layers at every step (SVD-01)
6. NPZ keys follow target.layer.metric convention (SVD-04)
7. Positions < w have NaN SVD metrics (SVD-06)
8. Guard activations tracked and counted
</verification>

<success_criteria>
- Fused forward pass produces behavioral labels AND SVD metrics in single inference (EVAL-05)
- SVD computed on QK^T, WvWo, AVWo per layer per step (SVD-01)
- token_metrics.npz with target.layer.metric keys, 2D arrays [n_sequences, n_steps] (SVD-04)
- NaN for positions < w, real values for positions >= w (SVD-06)
- Tail extension for late jumper encounters
- result.json summary with mean/std, guard counts, failure_index list
- All tests pass with zero regressions
</success_criteria>

<output>
After completion, create `.planning/phases/06-behavioral-evaluation-and-svd-collection/06-03-SUMMARY.md`
</output>
