---
phase: 11-pre-registration-framework
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - docs/pre-registration.md
  - docs/deviation-log.md
  - src/evaluation/split.py
  - src/evaluation/pipeline.py
  - src/results/schema.py
  - tests/test_split.py
autonomous: true
requirements:
  - PREG-01
  - PREG-02
  - PREG-03

must_haves:
  truths:
    - "Pre-registration document exists at docs/pre-registration.md specifying Grassmannian distance of QK^T as primary hypothesis, alpha=0.05, Holm-Bonferroni correction, and three-outcome decision criterion (Confirm/Inconclusive/Reject)"
    - "assign_split() deterministically assigns each walk to exploratory or confirmatory set using fixed seed, stratified by violation status"
    - "save_evaluation_results() accepts optional split_labels and stores split assignment metadata in the summary dict and split labels in NPZ"
    - "result.json schema validation accepts split-related fields without requiring them (backward compatible)"
    - "Deviation log exists at docs/deviation-log.md and is referenced from the pre-registration document"
  artifacts:
    - path: "docs/pre-registration.md"
      provides: "Pre-registration document with hypothesis, metric, statistical plan, decision criterion"
      contains: "Grassmannian distance"
    - path: "docs/deviation-log.md"
      provides: "Timestamped deviation log template"
      contains: "deviation"
    - path: "src/evaluation/split.py"
      provides: "assign_split() function for deterministic exploratory/confirmatory walk assignment"
      exports: ["assign_split", "SPLIT_SEED"]
    - path: "tests/test_split.py"
      provides: "Tests for deterministic split assignment and stratification"
  key_links:
    - from: "docs/pre-registration.md"
      to: "docs/deviation-log.md"
      via: "markdown reference link"
      pattern: "deviation-log\\.md"
    - from: "src/evaluation/split.py"
      to: "numpy"
      via: "np.random.default_rng for deterministic assignment"
      pattern: "np\\.random\\.default_rng"
    - from: "src/evaluation/pipeline.py"
      to: "src/evaluation/split.py"
      via: "optional import for split integration"
      pattern: "from src\\.evaluation\\.split import"
---

<objective>
Create the pre-registration framework: a pre-registration document locked in git, a held-out walk splitting function with result.json integration, and a deviation log.

Purpose: Establish the methodological foundation for all v1.1 confirmatory analysis. The pre-registration must be committed before any confirmatory analysis runs (Phases 12-16). The held-out split enables separating exploratory from confirmatory walks at evaluation time.

Output: docs/pre-registration.md, docs/deviation-log.md, src/evaluation/split.py with tests, and modifications to pipeline.py and schema.py for split tag support.
</objective>

<execution_context>
@/root/.claude/get-shit-done/workflows/execute-plan.md
@/root/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-pre-registration-framework/11-CONTEXT.md
@.planning/phases/11-pre-registration-framework/11-RESEARCH.md
@src/evaluation/pipeline.py
@src/evaluation/behavioral.py
@src/results/schema.py
@src/analysis/auroc_horizon.py
@src/analysis/statistical_controls.py

<interfaces>
<!-- Key types and contracts the executor needs. Extracted from codebase. -->

From src/evaluation/pipeline.py:
```python
@dataclass
class EvaluationResult:
    generated: np.ndarray           # [n_sequences, max_steps]
    edge_valid: np.ndarray          # [n_sequences, max_steps-1]
    rule_outcome: np.ndarray        # [n_sequences, max_steps-1]
    failure_index: np.ndarray       # [n_sequences] -- -1 if no violation
    svd_metrics: dict[str, np.ndarray]
    guard_activations: dict[str, int]
    sequence_lengths: np.ndarray    # [n_sequences]

def save_evaluation_results(
    result: EvaluationResult,
    output_dir: str | Path,
) -> dict[str, Any]:
    # Writes token_metrics.npz, returns summary dict for result.json
```

From src/results/schema.py:
```python
REQUIRED_TOP_FIELDS = {
    "schema_version", "experiment_id", "timestamp",
    "description", "tags", "config", "metrics",
}
REQUIRED_METRICS_FIELDS = {"scalars"}

def validate_result(result: dict[str, Any]) -> list[str]:
    # Returns list of error strings (empty = valid)

def write_result(
    config: ExperimentConfig,
    metrics: dict[str, Any],
    sequences: list[dict[str, Any]] | None = None,
    metadata: dict[str, Any] | None = None,
    token_metrics: dict[str, dict[str, Any]] | None = None,
    results_dir: str = "results",
) -> str:
```

From src/analysis/auroc_horizon.py:
```python
PRIMARY_METRICS: frozenset[str] = frozenset({
    "qkt.grassmannian_distance",
    "qkt.spectral_gap_1_2",
    "qkt.spectral_entropy",
    "avwo.stable_rank",
    "avwo.grassmannian_distance",
})
```

From src/analysis/statistical_controls.py:
```python
def holm_bonferroni(p_values, alpha=0.05) -> tuple[np.ndarray, np.ndarray]
def cohens_d(group1, group2) -> float
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create pre-registration document and deviation log</name>
  <files>
    docs/pre-registration.md
    docs/deviation-log.md
  </files>
  <action>
Create `docs/` directory if it does not exist.

**Create `docs/pre-registration.md`** with this structure and content:

```markdown
# Pre-Registration: QK^T Subspace Departure as Transformer Rule Violation Predictor

**Version:** 1.0
**Date:** 2026-02-26
**Status:** Locked (committed to git before confirmatory analysis)
**Deviation log:** [deviation-log.md](deviation-log.md)

## 1. Prior Evidence

v1.0 exploratory analysis observed that SVD metrics computed on the QK^T attention matrix
exhibit systematic changes in the steps preceding block jumper rule violations in a
DCSBM-trained transformer. Specifically, Grassmannian distance of the top-2 left singular
subspace of QK^T showed elevated values at lookback distances j=1 through j=r before
violation events, with AUROC exceeding chance levels in exploratory runs. These findings
motivate the confirmatory hypothesis below.

All v1.0 results are exploratory. This pre-registration locks the confirmatory analysis
plan before any v1.1 confirmatory runs.

## 2. Primary Hypothesis

**H1:** The Grassmannian distance of the QK^T attention matrix's top-2 left singular
subspace increases significantly in the steps preceding a block jumper rule violation,
compared to steps preceding successful rule compliance.

**Directionality:** One-sided (violation events show *higher* Grassmannian distance than
control events at matched lookback distances).

## 3. Primary Metric

**Metric:** Grassmannian distance of QK^T (key: `qkt.layer_{N}.grassmannian_distance`)

**Definition:** The Grassmannian distance between consecutive top-k left singular subspaces
of the QK^T matrix, computed as:

  d_G(U_t, U_{t-1}) = ||U_t U_t^T - U_{t-1} U_{t-1}^T||_F / sqrt(2k)

where U_t is the matrix of the top-k=2 left singular vectors at step t.

**All lookback distances j from 1 to r** are tested. Holm-Bonferroni correction is applied
across all j values within the primary metric.

## 4. Statistical Analysis Plan

### 4.1 Event Extraction
- Events: block jumper encounters extracted from generated sequences
- Contamination filter: encounters whose countdown window overlaps a preceding
  violation's window are excluded
- Stratification: events grouped by r value; each r-group analyzed independently

### 4.2 Test Statistic
- AUROC (Area Under the Receiver Operating Characteristic curve) computed via the
  rank-based method (equivalent to Mann-Whitney U / (n1 * n0))
- Computed at each lookback distance j = 1, 2, ..., r

### 4.3 Multiple Comparison Correction
- **Method:** Holm-Bonferroni step-down procedure
- **Family:** All lookback distances j = 1..r for the primary metric
- **Alpha level:** 0.05 (family-wise error rate)
- **Implementation:** `holm_bonferroni()` in `src/analysis/statistical_controls.py`

### 4.4 Shuffle Controls
- 10,000 label permutations per metric per r-value
- Flag threshold: shuffled p95 AUROC > 0.6 indicates positional artifact
- Any flagged metric's results are annotated but not excluded

### 4.5 Confidence Intervals
- BCa bootstrap confidence intervals (10,000 resamples) on AUROC at the
  max-signal lookback distance
- Reported for strata with 10+ events per class

### 4.6 Effect Size
- Cohen's d (pooled standard deviation) at each lookback distance
- Computed for violation vs. control metric value distributions

## 5. Held-Out Protocol

### 5.1 Split Design
- **Ratio:** 50% exploratory / 50% confirmatory
- **Unit:** Individual walk (sequence)
- **Timing:** Split applied at evaluation time, after behavioral labels are computed
- **Stratification:** Equal proportions of violation and non-violation walks in each set
- **Seed:** Fixed seed (2026) with `np.random.default_rng`
- **Assignment:** Deterministic by walk index — same walks always map to same split

### 5.2 Usage Convention
- **Exploratory set:** Used for hypothesis generation, visualization, debugging,
  parameter tuning. No restrictions on analysis.
- **Confirmatory set:** Used ONLY for the pre-registered statistical tests defined
  in this document. Results from the confirmatory set determine the final
  Confirm/Inconclusive/Reject outcome.
- **Separation:** Soft (both sets accessible in the same result.json/NPZ).
  Discipline enforced by convention and documentation, not code barriers.

### 5.3 Tagging
- Each walk tagged with `split: "exploratory"` or `split: "confirmatory"` in result.json
- Split assignment metadata (seed, counts) stored in metrics.scalars.split_assignment

## 6. Decision Criterion

### Three-Outcome Framework

**Confirm** (Gate 1 AND Gate 2 pass):
  QK^T subspace departure is a statistically significant, practically meaningful,
  and superior-to-output-level-metrics predictor of rule violations.

  - **Gate 1:** Holm-Bonferroni corrected p < 0.05 at any lookback j
  - **Gate 2:** Cohen's d >= 0.5 (medium effect) at the j that passes Gate 1,
    AND Grassmannian distance AUROC exceeds the best probability-level baseline
    metric's AUROC (entropy of softmax output, oracle KL divergence)

**Inconclusive** (Gate 1 passes, Gate 2 fails):
  Signal is real but either too small (d < 0.5) or does not beat probability-level
  baselines. Informative about attention geometry but scientifically redundant with
  output-level metrics.

**Reject** (Gate 1 fails):
  No statistically significant Grassmannian distance change precedes violations
  on the confirmatory set.

### Notes
- The confirmatory script outputs one of these three words alongside the numbers
- Baseline comparators for Gate 2 are probability-level metrics only (entropy of
  softmax output, oracle KL divergence) — NOT other SVD metrics
- This pre-registration defines criteria only — does not commit to post-outcome
  next steps

## 7. Secondary Metrics (Exploratory)

The following SVD metrics are computed and reported but are NOT used for the
Confirm/Inconclusive/Reject decision. They are exploratory:

| Metric | Target | Key Pattern |
|--------|--------|-------------|
| Stable rank | QK^T, AVWo | `{target}.layer_{N}.stable_rank` |
| Spectral entropy | QK^T, AVWo | `{target}.layer_{N}.spectral_entropy` |
| Spectral gap (1-2) | QK^T, AVWo | `{target}.layer_{N}.spectral_gap_1_2` |
| Spectral gap (2-3) | QK^T, AVWo | `{target}.layer_{N}.spectral_gap_2_3` |
| Spectral gap (4-5) | QK^T, AVWo | `{target}.layer_{N}.spectral_gap_4_5` |
| Condition number | QK^T, AVWo | `{target}.layer_{N}.condition_number` |
| Rank-1 residual norm | QK^T, AVWo | `{target}.layer_{N}.rank1_residual_norm` |
| Read-write alignment | WvWo | `wvwo.layer_{N}.read_write_alignment` |
| Grassmannian distance | AVWo | `avwo.layer_{N}.grassmannian_distance` |

These metrics have their own AUROC curves, horizons, and statistical summaries
computed and stored in result.json, but their p-values are not part of the
Holm-Bonferroni family and do not affect the decision criterion.

## 8. Deviation Policy

Any change to sections 2-6 of this document after the initial git commit
constitutes a deviation and MUST be recorded in the [deviation log](deviation-log.md)
with:
- Date
- Section affected
- Original specification
- New specification
- Rationale for the change
- Impact on interpretation of results

Changes to section 7 (secondary metrics) do not require deviation logging
since they do not affect the confirmatory decision.

---

*Pre-registration committed: 2026-02-26*
*Project: DCSBM Transformer SVD Hallucination Prediction*
```

**Create `docs/deviation-log.md`** with this content:

```markdown
# Deviation Log

**Referenced from:** [pre-registration.md](pre-registration.md)
**Project:** DCSBM Transformer SVD Hallucination Prediction

## Purpose

This log records any changes to the pre-registered analysis plan (sections 2-6 of
pre-registration.md) made after the initial commit. Each entry includes a timestamp,
the section affected, what changed, why, and how it affects interpretation.

## Log

_No deviations recorded._

<!-- Template for new entries:

### YYYY-MM-DD: [Brief description of change]

**Section affected:** [Which pre-registration section number and name]
**Original plan:** [What was originally specified]
**Change:** [What changed]
**Rationale:** [Why the change was necessary — scientific or technical justification]
**Impact on interpretation:** [How this affects the strength of confirmatory claims]
**Committed:** [git hash of the commit containing the change]

-->

---

*Deviation log created: 2026-02-26*
```

Verify both files exist and the cross-reference link is correct.
  </action>
  <verify>
    <automated>test -f docs/pre-registration.md && test -f docs/deviation-log.md && grep -q "deviation-log.md" docs/pre-registration.md && grep -q "pre-registration.md" docs/deviation-log.md && echo "PASS" || echo "FAIL"</automated>
  </verify>
  <done>
    - docs/pre-registration.md exists with all required sections: Prior Evidence, Primary Hypothesis, Primary Metric, Statistical Analysis Plan, Held-Out Protocol, Decision Criterion, Secondary Metrics, Deviation Policy
    - docs/deviation-log.md exists with template for recording deviations
    - Cross-references between the two documents are correct
    - Pre-registration specifies: Grassmannian distance of QK^T as primary hypothesis, alpha=0.05, Holm-Bonferroni correction, three-outcome decision criterion (Confirm/Inconclusive/Reject) with Gate 1 and Gate 2
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement held-out split function with tests (TDD)</name>
  <files>
    src/evaluation/split.py
    tests/test_split.py
  </files>
  <action>
**RED phase: Write tests first in tests/test_split.py**

Create test file with these test cases:

1. `test_split_deterministic` -- Call assign_split() twice with same failure_index and same seed. Arrays must be identical (np.array_equal).

2. `test_split_different_seeds` -- Call assign_split() with two different seeds. Arrays must differ.

3. `test_split_all_walks_assigned` -- Every walk gets either 'exploratory' or 'confirmatory'. No empty strings or other values.

4. `test_split_roughly_equal` -- For 100 walks, each split gets between 45 and 55 walks (50/50 within rounding).

5. `test_split_stratified_violations` -- Create failure_index with 20 violations and 80 non-violations. Verify that the number of violations in each split differs by at most 1 (stratification works).

6. `test_split_stratified_nonviolations` -- Same as above but verify non-violations are balanced across splits (differ by at most 1).

7. `test_split_all_violations` -- Edge case: all walks have violations. Split should still produce 50/50.

8. `test_split_no_violations` -- Edge case: no walks have violations. Split should still produce 50/50.

9. `test_split_single_walk` -- Edge case: only 1 walk. Should assign to one split without error.

10. `test_split_empty` -- Edge case: empty failure_index array. Should return empty array.

11. `test_split_odd_count` -- Odd number of violations (e.g., 3). One split gets 2, the other gets 1. Total is correct.

**GREEN phase: Implement src/evaluation/split.py**

```python
"""Held-out evaluation split for exploratory/confirmatory walk assignment.

Implements deterministic, stratified 50/50 split of evaluation walks into
exploratory and confirmatory sets. Split is applied at evaluation time
(after behavioral labels are computed) and tagged in result.json.

Pre-registered in docs/pre-registration.md, Section 5.
"""

import numpy as np

# Fixed seed for deterministic split assignment.
# Documented in pre-registration.md Section 5.1.
SPLIT_SEED: int = 2026

# Split label constants
EXPLORATORY: str = "exploratory"
CONFIRMATORY: str = "confirmatory"


def assign_split(
    failure_index: np.ndarray,
    split_seed: int = SPLIT_SEED,
) -> np.ndarray:
    """Assign each walk to exploratory or confirmatory split.

    Stratified by event type (violation vs non-violation) to ensure
    equal proportions in each set. Deterministic via fixed seed +
    numpy default_rng.

    Args:
        failure_index: First violation step per walk, shape [n_walks].
            -1 indicates no violation.
        split_seed: Fixed seed for reproducibility.

    Returns:
        Array of strings, shape [n_walks], each 'exploratory' or 'confirmatory'.
    """
    n_walks = len(failure_index)
    if n_walks == 0:
        return np.array([], dtype="U13")

    splits = np.empty(n_walks, dtype="U13")
    rng = np.random.default_rng(split_seed)

    # Separate by violation status
    violation_mask = failure_index >= 0
    viol_indices = np.where(violation_mask)[0]
    nonviol_indices = np.where(~violation_mask)[0]

    # Shuffle and split each pool 50/50
    for indices in [viol_indices, nonviol_indices]:
        if len(indices) == 0:
            continue
        shuffled = rng.permutation(indices)
        half = len(shuffled) // 2
        splits[shuffled[:half]] = EXPLORATORY
        splits[shuffled[half:]] = CONFIRMATORY

    return splits
```

Run tests: `pytest tests/test_split.py -x -v`
  </action>
  <verify>
    <automated>pytest tests/test_split.py -x -v</automated>
  </verify>
  <done>
    - All 11 test cases pass
    - assign_split() produces deterministic results for the same seed
    - Stratification ensures balanced violation/non-violation counts across splits
    - Edge cases handled: empty, single walk, all violations, no violations, odd counts
    - SPLIT_SEED constant exported and documented
  </done>
</task>

<task type="auto">
  <name>Task 3: Integrate split into evaluation pipeline and result schema</name>
  <files>
    src/evaluation/pipeline.py
    src/results/schema.py
  </files>
  <action>
**Modify `src/evaluation/pipeline.py`:**

1. Add import at top of file:
```python
from src.evaluation.split import SPLIT_SEED
```

2. Modify the `save_evaluation_results` function signature to accept an optional `split_labels` parameter:
```python
def save_evaluation_results(
    result: EvaluationResult,
    output_dir: str | Path,
    split_labels: np.ndarray | None = None,
) -> dict[str, Any]:
```

3. Inside save_evaluation_results, after `npz_data["generated"] = result.generated` and before `np.savez_compressed(...)`, add:
```python
    # Held-out split labels (Phase 11: Pre-Registration Framework)
    if split_labels is not None:
        # Store as integer array: 0=exploratory, 1=confirmatory
        split_int = np.where(split_labels == "confirmatory", 1, 0).astype(np.int8)
        npz_data["split"] = split_int
```

4. Inside save_evaluation_results, after the summary dict is built (after `summary["scalars"]["n_violations"] = ...`), add:
```python
    # Split assignment metadata
    if split_labels is not None:
        n_exploratory = int((split_labels == "exploratory").sum())
        n_confirmatory = int((split_labels == "confirmatory").sum())
        violation_mask = result.failure_index >= 0
        summary["scalars"]["split_assignment"] = {
            "split_seed": SPLIT_SEED,
            "n_exploratory": n_exploratory,
            "n_confirmatory": n_confirmatory,
            "n_exploratory_violations": int(
                (violation_mask & (split_labels == "exploratory")).sum()
            ),
            "n_confirmatory_violations": int(
                (violation_mask & (split_labels == "confirmatory")).sum()
            ),
        }
```

**Modify `src/results/schema.py`:**

The schema validation in `validate_result()` already accepts arbitrary keys in `metrics.scalars` (it only checks that `scalars` exists as a key). No structural change needed for split_assignment.

However, add a validation check to ensure that if `split_assignment` is present in `metrics.scalars`, it has the expected fields. Add this after the `metrics.scalars` check block (after line ~82):

```python
    # Optional split_assignment validation (backward compatible)
    if "metrics" in result and isinstance(result["metrics"], dict):
        scalars = result["metrics"].get("scalars", {})
        if isinstance(scalars, dict) and "split_assignment" in scalars:
            sa = scalars["split_assignment"]
            if not isinstance(sa, dict):
                errors.append("metrics.scalars.split_assignment must be a dict")
            else:
                for field in ["split_seed", "n_exploratory", "n_confirmatory"]:
                    if field not in sa:
                        errors.append(
                            f"metrics.scalars.split_assignment missing field: {field}"
                        )
```

**Verify backward compatibility:** Run existing tests to ensure old code paths (without split_labels) still work.

```bash
pytest tests/test_evaluation_pipeline.py tests/test_results.py -x -v
```
  </action>
  <verify>
    <automated>pytest tests/test_evaluation_pipeline.py tests/test_results.py tests/test_split.py -x -v</automated>
  </verify>
  <done>
    - save_evaluation_results() accepts optional split_labels parameter
    - When split_labels provided: NPZ contains integer split array, summary dict contains split_assignment metadata
    - When split_labels is None (default): behavior unchanged, backward compatible
    - validate_result() accepts split_assignment in metrics.scalars with proper field validation
    - All existing tests pass (no regressions)
    - Split data flows correctly: assign_split() -> save_evaluation_results() -> result.json
  </done>
</task>

</tasks>

<verification>
```bash
# All split tests pass
pytest tests/test_split.py -x -v

# No regressions in evaluation pipeline or results tests
pytest tests/test_evaluation_pipeline.py tests/test_results.py -x -v

# Full test suite passes
pytest tests/ -x --timeout=120

# Pre-registration document exists and contains key terms
grep -q "Grassmannian distance" docs/pre-registration.md
grep -q "alpha.*0.05" docs/pre-registration.md
grep -q "Holm-Bonferroni" docs/pre-registration.md
grep -q "Confirm" docs/pre-registration.md
grep -q "Inconclusive" docs/pre-registration.md
grep -q "Reject" docs/pre-registration.md

# Deviation log exists and is referenced
grep -q "deviation-log.md" docs/pre-registration.md
test -f docs/deviation-log.md

# Split module is importable
python -c "from src.evaluation.split import assign_split, SPLIT_SEED; print('split OK')"

# Pipeline accepts split_labels parameter
python -c "import inspect; from src.evaluation.pipeline import save_evaluation_results; sig = inspect.signature(save_evaluation_results); assert 'split_labels' in sig.parameters; print('pipeline OK')"
```
</verification>

<success_criteria>
- Pre-registration document committed to git with complete hypothesis, metric, alpha level, correction method, held-out protocol, and three-outcome decision criterion
- Deviation log committed with template for recording changes
- assign_split() deterministically splits walks 50/50 stratified by violation status
- save_evaluation_results() stores split labels in NPZ and split metadata in summary dict
- Result schema validates split_assignment fields when present
- All existing tests pass (backward compatible)
- No confirmatory analysis code has been run (pre-registration precedes analysis)
</success_criteria>

<output>
After completion, create `.planning/phases/11-pre-registration-framework/11-01-SUMMARY.md`
</output>
