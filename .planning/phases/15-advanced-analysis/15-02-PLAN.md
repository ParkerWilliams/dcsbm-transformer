---
phase: 15-advanced-analysis
plan: 02
type: execute
wave: 2
depends_on: [15-01]
files_modified:
  - src/visualization/spectrum.py
  - src/visualization/render.py
  - src/reporting/single.py
  - src/reporting/templates/single_report.html
  - src/results/schema.py
  - tests/test_spectrum.py
autonomous: true
requirements: [SPEC-03]
---

must_haves:
  truths:
    - "curvature and torsion time series are fed into the AUROC pipeline as additional secondary predictive metrics with lookback-based analysis"
    - "curvature/torsion AUROC results are stored in result.json in a separate spectrum_analysis block, clearly labeled as exploratory"
    - "spectrum trajectory visualization shows curvature time series aligned with behavioral events"
    - "HTML report includes a collapsible Spectrum Analysis section with curvature/torsion AUROC table and figures"
    - "schema validates spectrum_analysis block when present, passes when absent (backward compatible)"
  artifacts:
    - src/visualization/spectrum.py
    - tests/test_spectrum.py
  key_links:
    - "spectrum.py visualization imports style.py palette and save_figure"
    - "render.py calls spectrum plotting functions when spectrum_trajectories.npz exists"
    - "single.py passes spectrum analysis data to HTML template"
    - "AUROC computation on curvature/torsion uses existing auroc_from_groups and compute_auroc_curve"

<objective>
Integrate curvature/torsion as secondary AUROC predictive metrics with visualization, report rendering, and schema validation.

Purpose: SPEC-03 requires curvature and torsion time series to feed into the AUROC pipeline as additional (secondary) predictive metrics. The results must be clearly separated from primary metrics and reported as exploratory.
Output: AUROC analysis on curvature/torsion, spectrum visualization, report integration, schema validation, and extended tests.
</objective>

<execution_context>
@/root/.claude/get-shit-done/workflows/execute-plan.md
@/root/.claude/get-shit-done/references/tdd.md
@/root/.claude/get-shit-done/references/checkpoints.md
@/root/.claude/get-shit-done/references/model-profile-resolution.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-advanced-analysis/15-RESEARCH.md
@.planning/phases/15-advanced-analysis/15-CONTEXT.md
@.planning/phases/15-advanced-analysis/15-01-SUMMARY.md

@src/analysis/spectrum.py
@src/analysis/auroc_horizon.py
@src/analysis/event_extraction.py
@src/evaluation/pipeline.py
@src/visualization/style.py
@src/visualization/render.py
@src/visualization/auroc.py
@src/reporting/single.py
@src/reporting/templates/single_report.html
@src/results/schema.py

<interfaces>
<!-- Key types and contracts the executor needs. -->

From src/analysis/spectrum.py (created in 15-01):
```python
def compute_spectrum_analysis(spectra: np.ndarray, ...) -> dict
    # Returns {"curvature": [n_steps], "torsion": [n_steps], ...}

def compute_spectrum_analysis_batch(spectra_batch: np.ndarray, ...) -> dict
    # Returns {"curvature": [n_seqs, n_steps], "torsion": [n_seqs, n_steps], ...}
```

From src/analysis/auroc_horizon.py:
```python
PRIMARY_METRICS: frozenset[str]  # 5 pre-registered metrics -- DO NOT MODIFY
def auroc_from_groups(violations, controls) -> float
def compute_auroc_curve(violation_events, control_events, metric_array, r_value, min_per_class=2) -> np.ndarray
def run_auroc_analysis(eval_result_data, jumper_map, metric_keys, ...) -> dict
```

From src/analysis/event_extraction.py:
```python
def extract_events(generated, rule_outcome, failure_index, jumper_map) -> list[AnalysisEvent]
def filter_contaminated_events(events) -> tuple[list[AnalysisEvent], dict]
def stratify_by_r(events) -> dict[int, list[AnalysisEvent]]
```

From src/visualization/style.py:
```python
PALETTE = sns.color_palette("colorblind", n_colors=8)
def apply_style() -> None
def save_figure(fig, output_dir, name) -> tuple[Path, Path]
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create spectrum AUROC analysis function and visualization module</name>
  <files>
    src/analysis/spectrum.py
    src/visualization/spectrum.py
    tests/test_spectrum.py
  </files>
  <action>
**Extend `src/analysis/spectrum.py` with AUROC integration:**

1. **`run_spectrum_auroc_analysis(spectrum_path, eval_result_data, jumper_map, layers=None, min_events_per_class=5)`:**
   - Load spectrum_trajectories.npz from spectrum_path
   - For each layer, compute curvature and torsion via compute_spectrum_analysis_batch
   - Build metric arrays compatible with AUROC pipeline:
     - Key pattern: "qkt.layer_N.curvature" -> [n_sequences, n_steps-1]
     - Key pattern: "qkt.layer_N.torsion" -> [n_sequences, n_steps-1]
   - Extract events using extract_events, filter_contaminated_events, stratify_by_r
   - For each r_value, for each curvature/torsion metric, compute AUROC curve via compute_auroc_curve
   - Return nested dict:
     ```python
     {
         "status": "exploratory",  # clearly label as secondary
         "config": {"layers": [0,1,2,3], "smoothing": {"window": 7, "polyorder": 3}},
         "by_r_value": {
             r_val: {
                 "n_violations": N,
                 "n_controls": N,
                 "by_metric": {
                     "qkt.layer_N.curvature": {
                         "auroc_by_lookback": [float, ...],  # length r
                         "peak_auroc": float,
                         "peak_lookback": int,
                     },
                     "qkt.layer_N.torsion": { ... }
                 }
             }
         }
     }
     ```

**Create `src/visualization/spectrum.py`:**

1. **`plot_curvature_timeseries(curvature, failure_indices, ax=None)`:**
   - Plot curvature time series for a single sequence (or mean across sequences)
   - Mark failure events with vertical lines (using VIOLATION_COLOR)
   - X-axis: step, Y-axis: curvature
   - Apply style from style.py

2. **`plot_spectrum_auroc(spectrum_auroc_results, r_value, ax=None)`:**
   - AUROC vs lookback distance for curvature and torsion metrics
   - Similar to existing AUROC plots but clearly labeled "Exploratory: Spectrum Geometry"
   - One line per curvature/torsion per layer
   - Horizontal line at 0.5 (chance level)
   - Returns plt.Figure

3. **`plot_spectrum_trajectory_sample(spectra, sequence_idx=0, layer_idx=0, top_k=4)`:**
   - Plot the top-k singular value trajectories over time for a single sequence
   - Each singular value as a different colored line
   - Mark any ordering crossings
   - X-axis: step, Y-axis: singular value magnitude
   - Returns plt.Figure

**Extend `tests/test_spectrum.py`:**

1. **test_run_spectrum_auroc_analysis_structure:** Build synthetic spectrum_trajectories.npz, synthetic eval_result_data with events. Run run_spectrum_auroc_analysis. Verify output structure has by_r_value with by_metric containing auroc_by_lookback.

2. **test_spectrum_auroc_exploratory_label:** Verify output dict contains "status": "exploratory".

3. **test_curvature_auroc_perfect_signal:** Create synthetic spectrum data where curvature spikes exactly at violation events. AUROC at lookback j=1 should be high (>0.8).

4. **test_plot_curvature_timeseries:** Call plot_curvature_timeseries with synthetic data. Verify returns a matplotlib Figure.

5. **test_plot_spectrum_auroc:** Call plot_spectrum_auroc with synthetic results dict. Verify returns a matplotlib Figure.

6. **test_plot_spectrum_trajectory_sample:** Call with synthetic spectra. Verify returns Figure.
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -m pytest tests/test_spectrum.py -x -v</automated>
  </verify>
  <done>
    - run_spectrum_auroc_analysis produces correctly structured output with exploratory label
    - AUROC computation on curvature/torsion uses existing auroc_from_groups pipeline
    - Visualization functions produce valid matplotlib figures
    - All spectrum tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate spectrum analysis into render pipeline, HTML report, and schema</name>
  <files>
    src/visualization/render.py
    src/reporting/single.py
    src/reporting/templates/single_report.html
    src/results/schema.py
  </files>
  <action>
**Modify `src/visualization/render.py`:**

Add spectrum analysis section after the existing SVD benchmark section, following the try/except pattern:

```python
# -- Spectrum Analysis (SPEC-03) --
spectrum_analysis = result.get("metrics", {}).get("spectrum_analysis", {})
spectrum_npz_path = Path(result_dir) / "spectrum_trajectories.npz"
if spectrum_analysis or spectrum_npz_path.exists():
    try:
        from src.visualization.spectrum import (
            plot_spectrum_auroc,
            plot_spectrum_trajectory_sample,
        )

        # AUROC plots per r-value
        by_r = spectrum_analysis.get("by_r_value", {})
        for r_val_str, r_data in by_r.items():
            by_metric = r_data.get("by_metric", {})
            if by_metric:
                r_value = int(r_val_str)
                fig = plot_spectrum_auroc(by_metric, r_value=r_value)
                paths = save_figure(fig, figures_dir, f"spectrum_auroc_r{r_value}")
                generated_files.extend(paths)
                log.info("Generated: spectrum_auroc_r%d", r_value)

        # Sample trajectory plot (first sequence, first layer)
        if spectrum_npz_path.exists():
            import numpy as np
            spectrum_data = np.load(str(spectrum_npz_path))
            for key in spectrum_data.files:
                if key.endswith(".spectrum"):
                    spectra = spectrum_data[key]
                    fig = plot_spectrum_trajectory_sample(spectra, sequence_idx=0)
                    layer_tag = key.replace(".spectrum", "").replace(".", "_")
                    paths = save_figure(fig, figures_dir, f"spectrum_traj_{layer_tag}")
                    generated_files.extend(paths)
                    break  # Only plot first layer for brevity
    except Exception as e:
        log.warning("Failed to generate spectrum analysis plots: %s", e)
```

**Modify `src/reporting/single.py`:**

1. In `_collect_figures`, add:
   ```python
   "spectrum_figures": [],
   ```
   In the for loop:
   ```python
   elif name.startswith("spectrum_"):
       result["spectrum_figures"].append({"title": title, "data_uri": data_uri})
   ```

2. In `generate_single_report`, extract spectrum data and pass to template:
   ```python
   spectrum_analysis = metrics.get("spectrum_analysis")
   ```
   Add to template.render():
   ```python
   spectrum_analysis=spectrum_analysis,
   spectrum_figures=figures.get("spectrum_figures", []),
   ```

**Modify `src/reporting/templates/single_report.html`:**

Add after the SVD Computational Cost section, using the same collapsible pattern:

```html
<!-- Spectrum Analysis (SPEC-03) -->
{% if spectrum_analysis or spectrum_figures %}
<details class="enrichment-section">
  <summary><strong>Spectrum Trajectory Analysis (Exploratory)</strong></summary>
  <div class="section-content">
    <p><em>Status: Exploratory analysis. Curvature and torsion metrics are secondary
    and not part of the pre-registered primary hypothesis.</em></p>

    {% if spectrum_analysis %}
    <h3>Curvature/Torsion AUROC Summary</h3>
    <table>
      <thead>
        <tr>
          <th>r-value</th>
          <th>Metric</th>
          <th>Peak AUROC</th>
          <th>Peak Lookback</th>
          <th>AUROC by lookback</th>
        </tr>
      </thead>
      <tbody>
        {% for r_str, r_data in spectrum_analysis.by_r_value|dictsort %}
        {% for metric_name, m_data in r_data.by_metric|dictsort %}
        <tr>
          <td>{{ r_str }}</td>
          <td>{{ metric_name }}</td>
          <td>{{ "%.4f"|format(m_data.peak_auroc) if m_data.peak_auroc is number else 'N/A' }}</td>
          <td>{{ m_data.peak_lookback if m_data.peak_lookback is number else 'N/A' }}</td>
          <td>{{ m_data.auroc_by_lookback | map('round', 4) | join(', ') if m_data.auroc_by_lookback else 'N/A' }}</td>
        </tr>
        {% endfor %}
        {% endfor %}
      </tbody>
    </table>
    {% endif %}

    {% if spectrum_figures %}
    <h3>Spectrum Plots</h3>
    {% for fig in spectrum_figures %}
    <div class="figure-container">
      <div class="figure-title">{{ fig.title }}</div>
      <img src="{{ fig.data_uri }}" alt="{{ fig.title }}">
    </div>
    {% endfor %}
    {% endif %}
  </div>
</details>
{% endif %}
```

**Modify `src/results/schema.py`:**

Add optional spectrum_analysis validation block (backward-compatible):
```python
# Optional spectrum_analysis validation (Phase 15, backward compatible)
if "metrics" in result and isinstance(result["metrics"], dict):
    spectrum_analysis = result["metrics"].get("spectrum_analysis")
    if spectrum_analysis is not None:
        if not isinstance(spectrum_analysis, dict):
            errors.append("metrics.spectrum_analysis must be a dict")
        else:
            if "status" not in spectrum_analysis:
                errors.append("metrics.spectrum_analysis missing required field: status")
            if "by_r_value" not in spectrum_analysis:
                errors.append("metrics.spectrum_analysis missing required block: by_r_value")
```
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -m pytest tests/test_spectrum.py tests/test_reporting.py -x -v</automated>
  </verify>
  <done>
    - Curvature/torsion AUROC results appear in spectrum_analysis block with exploratory label
    - Render pipeline generates spectrum_auroc and spectrum_traj figures
    - HTML report shows collapsible "Spectrum Trajectory Analysis (Exploratory)" section
    - Schema validates spectrum_analysis block when present, passes when absent
    - All existing tests pass (no regressions)
  </done>
</task>

</tasks>

<verification>
- `pytest tests/test_spectrum.py -x -v` passes all spectrum tests (analysis + AUROC + visualization)
- `pytest tests/test_reporting.py -x -v` passes (existing + spectrum integration)
- `pytest tests/ -x` full suite passes (no regressions)
- Spectrum AUROC clearly labeled as "exploratory" in output dict and HTML report
- PRIMARY_METRICS in auroc_horizon.py is NOT modified
</verification>

<success_criteria>
- SPEC-03: Curvature and torsion time series are fed into the AUROC pipeline as additional (secondary) predictive metrics, with results clearly separated from primary metrics
- Visualization and report integration following Phase 13 patterns
- Schema backward-compatible
- All tests pass including existing suite
</success_criteria>

<output>
After completion, create `.planning/phases/15-advanced-analysis/15-02-SUMMARY.md`
</output>
