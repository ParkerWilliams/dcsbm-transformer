---
phase: 07-predictive-horizon-and-statistical-analysis
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/analysis/__init__.py
  - src/analysis/event_extraction.py
  - src/analysis/auroc_horizon.py
  - tests/test_event_extraction.py
  - tests/test_auroc_horizon.py
  - src/evaluation/pipeline.py
autonomous: true
requirements:
  - PRED-01
  - PRED-02
  - PRED-03
  - PRED-04
  - PRED-05

must_haves:
  truths:
    - "Jumper encounters are correctly extracted from generated sequences with walk index, encounter step, resolution step, r value, and outcome"
    - "Contamination filtering excludes encounters whose countdown window overlaps a preceding violation window, with exclusion count recorded"
    - "AUROC is computed at each lookback distance j (1 to r) for each SVD metric, stratified by r value"
    - "Predictive horizon (furthest j where AUROC > 0.75) is calculated per metric per r-value"
    - "Shuffle controls flag metrics where permuted-label AUROC exceeds 0.6"
    - "Per-metric AUROC curves are stored in a structured results dict matching result.json schema"
  artifacts:
    - path: "src/analysis/__init__.py"
      provides: "Public API for analysis module"
    - path: "src/analysis/event_extraction.py"
      provides: "AnalysisEvent dataclass, extract_events, filter_contaminated, stratify_by_r"
      contains: "class AnalysisEvent"
    - path: "src/analysis/auroc_horizon.py"
      provides: "AUROC computation, predictive horizon, shuffle controls"
      contains: "def compute_auroc_curve"
    - path: "tests/test_event_extraction.py"
      provides: "Tests for event extraction and contamination filtering"
    - path: "tests/test_auroc_horizon.py"
      provides: "Tests for AUROC computation, horizon, shuffle controls"
  key_links:
    - from: "src/analysis/event_extraction.py"
      to: "src/evaluation/behavioral.py"
      via: "RuleOutcome enum import"
      pattern: "from src\\.evaluation\\.behavioral import RuleOutcome"
    - from: "src/analysis/auroc_horizon.py"
      to: "src/analysis/event_extraction.py"
      via: "AnalysisEvent import"
      pattern: "from src\\.analysis\\.event_extraction import AnalysisEvent"
    - from: "src/analysis/auroc_horizon.py"
      to: "scipy.stats"
      via: "rankdata for AUROC, permutation_test for shuffle"
      pattern: "from scipy\\.stats import rankdata"
---

<objective>
Implement event extraction from evaluation output and AUROC-based predictive horizon analysis with shuffle controls.

Purpose: This is the core analysis pipeline that answers the project's central research question -- how far in advance can SVD metrics predict transformer rule violations. It extracts jumper encounter events from generated sequences, applies contamination filtering, computes AUROC at each lookback distance stratified by r value, determines predictive horizons, and validates with shuffle controls.

Output: Two modules (event_extraction.py, auroc_horizon.py) with comprehensive TDD test coverage, plus a one-line NPZ extension to save generated sequences.
</objective>

<execution_context>
@/root/.claude/get-shit-done/workflows/execute-plan.md
@/root/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-predictive-horizon-and-statistical-analysis/07-CONTEXT.md
@.planning/phases/07-predictive-horizon-and-statistical-analysis/07-RESEARCH.md
@.planning/phases/06-behavioral-evaluation-and-svd-collection/06-03-SUMMARY.md
@src/evaluation/pipeline.py
@src/evaluation/behavioral.py
@src/graph/jumpers.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Event extraction with contamination filtering (TDD)</name>
  <files>
    src/analysis/__init__.py
    src/analysis/event_extraction.py
    tests/test_event_extraction.py
    src/evaluation/pipeline.py
  </files>
  <action>
**RED phase: Write tests first in tests/test_event_extraction.py**

Create test file with these test cases using synthetic data (no model needed):

1. `test_extract_events_finds_jumper_encounters` -- Build a synthetic generated array with known jumper vertices at known positions. Provide a jumper_map and rule_outcome array. Verify that extract_events returns AnalysisEvent records with correct walk_idx, encounter_step, resolution_step, r_value, and outcome (FOLLOWED or VIOLATED).

2. `test_extract_events_first_violation_only` -- Walk with two violations: verify is_first_violation=True only for the first one.

3. `test_contamination_filter_excludes_overlapping` -- Two encounters in the same walk where encounter B's countdown window overlaps violation A's window (B.encounter_step < A.resolution_step). Verify B is excluded and exclusion_count=1.

4. `test_contamination_filter_keeps_non_overlapping` -- Two encounters where B.encounter_step >= A.resolution_step. Verify both kept.

5. `test_contamination_filter_successful_prior_does_not_contaminate` -- Encounter A is FOLLOWED (success), encounter B starts within A's window. Verify B is NOT excluded (only violations contaminate per CONTEXT.md).

6. `test_stratify_by_r` -- Events with r=32 and r=45. Verify stratify_by_r produces correct grouping.

7. `test_contamination_audit_threshold` -- >30% exclusion rate produces flagged=True.

8. `test_no_events_returns_empty` -- No jumper vertices in generated sequences. Returns empty list.

9. `test_resolution_step_alignment` -- Verify resolution_step = encounter_step + r exactly (off-by-one guard).

**GREEN phase: Implement src/analysis/event_extraction.py**

Create `src/analysis/__init__.py` (empty or minimal exports).

Create `src/analysis/event_extraction.py` with:

```python
@dataclass(frozen=True, slots=True)
class AnalysisEvent:
    walk_idx: int
    encounter_step: int      # Step where jumper vertex appeared
    resolution_step: int     # encounter_step + r
    r_value: int
    outcome: int             # RuleOutcome.FOLLOWED or VIOLATED
    is_first_violation: bool
```

Functions:
- `extract_events(generated: np.ndarray, rule_outcome: np.ndarray, failure_index: np.ndarray, jumper_map: dict[int, JumperInfo]) -> list[AnalysisEvent]` -- For each sequence, scan generated tokens for jumper vertices. For each found jumper at step t, resolution_step = t + jumper.r. Cross-reference with rule_outcome[walk, resolution_step - 1] (note: rule_outcome[t] reflects the transition at step t, and the deadline check in behavioral.py uses t+1 == deadline, meaning rule_outcome is indexed at deadline-1 = encounter_step + r - 1). Carefully match indexing with behavioral.py's convention. Set is_first_violation = True only when outcome == VIOLATED and resolution_step - 1 matches failure_index[walk]. Skip encounters where resolution_step exceeds sequence length.

- `filter_contaminated_events(events: list[AnalysisEvent]) -> tuple[list[AnalysisEvent], dict]` -- Group by walk_idx, sort by encounter_step. Track last_violation_end (resolution step of most recent violation). Exclude encounters where encounter_step < last_violation_end. IMPORTANT: Only violation events set last_violation_end, not FOLLOWED events (per CONTEXT.md: "Successful prior encounters do not contaminate subsequent ones"). Returns (filtered_events, audit_dict) where audit_dict contains total_encounters, excluded_encounters, exclusion_rate, flagged (>30%), and per_r breakdown.

- `stratify_by_r(events: list[AnalysisEvent]) -> dict[int, list[AnalysisEvent]]` -- Group events by r_value. Returns dict mapping r -> events.

**NPZ extension:** Add one line to `save_evaluation_results` in `src/evaluation/pipeline.py`:
```python
npz_data["generated"] = result.generated
```
Place this after `npz_data["sequence_lengths"] = ...` and before `np.savez_compressed(...)`. This enables standalone re-analysis from saved NPZ files.

Run all tests: `pytest tests/test_event_extraction.py -x -v`
  </action>
  <verify>
    <automated>pytest tests/test_event_extraction.py -x -v</automated>
    <manual>Verify event extraction correctly handles off-by-one alignment between encounter_step, resolution_step, and rule_outcome indexing</manual>
    <sampling_rate>run after this task commits, before next task begins</sampling_rate>
  </verify>
  <done>
    - All 9 test cases pass
    - AnalysisEvent correctly aligns encounter_step + r = resolution_step
    - Contamination filter only excludes encounters after violations (not after successes)
    - Audit dict flags when exclusion_rate > 0.3
    - Generated array saved to NPZ for standalone re-analysis
  </done>
</task>

<task type="auto">
  <name>Task 2: AUROC computation, predictive horizon, and shuffle controls (TDD)</name>
  <files>
    src/analysis/auroc_horizon.py
    tests/test_auroc_horizon.py
  </files>
  <action>
**RED phase: Write tests first in tests/test_auroc_horizon.py**

Create test file with these test cases using synthetic data (fabricate metric arrays and events):

1. `test_auroc_from_groups_perfect_separation` -- violations=[5,6,7], controls=[1,2,3]. AUROC must equal 1.0.

2. `test_auroc_from_groups_no_separation` -- violations=[1,2,3], controls=[1,2,3]. AUROC must equal 0.5.

3. `test_auroc_from_groups_reversed` -- violations=[1,2,3], controls=[5,6,7]. AUROC must equal 0.0.

4. `test_auroc_from_groups_empty_returns_nan` -- Either group empty. Returns NaN.

5. `test_compute_auroc_curve_shape` -- For r=8, verify returned array has shape (8,) with AUROC at each lookback j=1..8.

6. `test_compute_auroc_curve_known_signal` -- Create a synthetic metric array where violation events have consistently higher values at lookback j=1..3 and similar values at j=4..8. Verify AUROC > 0.8 for j<=3 and ~0.5 for j>3.

7. `test_compute_auroc_curve_nan_handling` -- Some metric values are NaN (warmup positions). Verify those lookback distances get NaN AUROC (not crash).

8. `test_compute_auroc_curve_min_events` -- Fewer than 2 events per class at some lookback j. Verify NaN returned (not crash).

9. `test_predictive_horizon_basic` -- AUROC curve [0.5, 0.6, 0.8, 0.9, 0.85, 0.7, 0.5, 0.5] with threshold 0.75. Horizon = 5 (furthest j where AUROC > 0.75, j indexed 1-based from the curve).

10. `test_predictive_horizon_none_exceeds` -- All AUROC < 0.75. Horizon = 0.

11. `test_shuffle_control_no_signal` -- Random metric values for both groups. Verify shuffle_flag=False (shuffled AUROC should be ~0.5, well below 0.6).

12. `test_shuffle_control_positional_artifact` -- Metric values correlated with position, not class label. Verify shuffle_flag=True (shuffled AUROC can exceed 0.6 because signal is positional).

13. `test_run_auroc_analysis_full_pipeline` -- Integration test: create synthetic EvaluationResult-like arrays, fake jumper_map, run full pipeline (extract_events -> stratify_by_r -> compute_auroc_curve for each metric). Verify output dict structure matches result.json schema from RESEARCH.md.

**GREEN phase: Implement src/analysis/auroc_horizon.py**

Functions:

- `auroc_from_groups(violations: np.ndarray, controls: np.ndarray) -> float` -- AUROC via rank method using scipy.stats.rankdata. Return NaN if either group is empty. Handles ties via midrank.

- `compute_auroc_curve(violation_events: list[AnalysisEvent], control_events: list[AnalysisEvent], metric_array: np.ndarray, r_value: int) -> np.ndarray` -- For each lookback j from 1 to r_value, extract metric values at resolution_step - j for both groups, filter NaN values, compute AUROC. Return array of shape (r_value,). Use minimum 2 events per class to compute; otherwise NaN.

- `compute_predictive_horizon(auroc_curve: np.ndarray, threshold: float = 0.75) -> int` -- Scan from largest j to smallest, return the largest j where auroc_curve[j-1] > threshold. Return 0 if none exceed.

- `run_shuffle_control(violation_events: list[AnalysisEvent], control_events: list[AnalysisEvent], metric_array: np.ndarray, r_value: int, n_permutations: int = 10_000, flag_threshold: float = 0.6, rng: int | np.random.Generator = 42) -> dict` -- Shuffle violation/control labels n_permutations times, recompute max AUROC each time. Return dict with shuffle_auroc_mean, shuffle_auroc_p95, shuffle_flag (p95 > flag_threshold), p_value (fraction of shuffles with max AUROC >= observed max AUROC).

- `run_auroc_analysis(eval_result_data: dict, jumper_map: dict[int, JumperInfo], metric_keys: list[str], horizon_threshold: float = 0.75, shuffle_flag_threshold: float = 0.6, n_shuffle: int = 10_000, min_events_per_class: int = 5) -> dict` -- Top-level function that orchestrates the full pipeline. Takes a dict-like input with keys: 'generated', 'rule_outcome', 'failure_index', 'sequence_lengths', and metric arrays. Returns a nested dict matching the result.json `predictive_horizon` schema from RESEARCH.md: config block, contamination_audit, by_r_value with AUROC curves and horizons per metric per layer. Separate primary vs exploratory metric labeling using the 5 pre-registered metric identifiers: qkt.grassmannian_distance, qkt.spectral_gap_1_2, qkt.spectral_entropy, avwo.stable_rank, avwo.grassmannian_distance. Edge case handling per RESEARCH.md: 0-1 events = skip, 2-4 = point estimate only with "low_n" flag, 5-9 = AUROC + effect size (no bootstrap, "moderate_n"), 10+ = full analysis.

All AUROC computations use `auroc_from_groups` internally. Store n_valid_by_lookback for each metric.

PRIMARY_METRICS constant listing the 5 pre-registered metric name patterns (without layer prefix).

Run all tests: `pytest tests/test_auroc_horizon.py -x -v`
  </action>
  <verify>
    <automated>pytest tests/test_auroc_horizon.py tests/test_event_extraction.py -x -v</automated>
    <manual>Verify AUROC values match expected for known synthetic distributions</manual>
    <sampling_rate>run after this task commits, before next task begins</sampling_rate>
  </verify>
  <done>
    - All 13 test cases pass
    - AUROC computed via rank method matches known answers for perfect/no/reversed separation
    - Predictive horizon correctly identifies furthest j exceeding threshold
    - Shuffle controls correctly flag positional artifacts
    - run_auroc_analysis produces structured output matching result.json schema
    - Primary vs exploratory metrics correctly distinguished
    - Edge cases (too few events, NaN values) handled gracefully
  </done>
</task>

</tasks>

<verification>
```bash
# All event extraction and AUROC tests pass
pytest tests/test_event_extraction.py tests/test_auroc_horizon.py -x -v

# No regressions in existing tests
pytest tests/ -x --timeout=120

# Verify module is importable
python -c "from src.analysis.event_extraction import AnalysisEvent, extract_events, filter_contaminated_events, stratify_by_r; print('event_extraction OK')"
python -c "from src.analysis.auroc_horizon import auroc_from_groups, compute_auroc_curve, compute_predictive_horizon, run_shuffle_control; print('auroc_horizon OK')"
```
</verification>

<success_criteria>
- Event extraction correctly identifies jumper encounters from generated sequences and cross-references with rule_outcome for outcome classification
- Contamination filtering excludes encounters after violations but not after successes, with audit metrics
- AUROC at each lookback distance is computed per metric per r-value, stratified correctly
- Predictive horizon identifies furthest j where AUROC exceeds configurable threshold
- Shuffle controls validate that signal is not a positional artifact
- Output structure matches result.json predictive_horizon schema for downstream phases
- No regressions in existing test suite
</success_criteria>

<output>
After completion, create `.planning/phases/07-predictive-horizon-and-statistical-analysis/07-01-SUMMARY.md`
</output>
