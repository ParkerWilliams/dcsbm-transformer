---
phase: 02-dcsbm-graph-generation
plan: 03
type: execute
wave: 2
depends_on: [02-01]
files_modified:
  - src/graph/cache.py
  - src/graph/__init__.py
  - tests/test_graph_cache.py
autonomous: true
requirements:
  - GRPH-05

must_haves:
  truths:
    - "Generating a graph and saving to cache, then loading from cache, produces an identical adjacency matrix"
    - "Two calls to generate_dcsbm_graph with the same config hit cache on second call (no regeneration)"
    - "Two configs differing only in non-graph parameters (e.g. description) produce the same cache key"
    - "Two configs differing in graph parameters produce different cache keys"
    - "Cache stores adjacency matrix, block assignments, theta values, and generation metadata"
    - "Cache uses gzip compression to minimize disk usage"
  artifacts:
    - path: "src/graph/cache.py"
      provides: "Graph caching by config hash with save/load/generate-or-load API"
      exports: ["save_graph", "load_graph", "generate_or_load_graph", "graph_cache_key"]
  key_links:
    - from: "src/graph/cache.py"
      to: "src/config/hashing.py"
      via: "graph_cache_key calls graph_config_hash to derive base key, appends seed"
      pattern: "graph_config_hash"
    - from: "src/graph/cache.py"
      to: "src/graph/dcsbm.py"
      via: "generate_or_load_graph calls generate_dcsbm_graph on cache miss"
      pattern: "generate_dcsbm_graph"
    - from: "src/graph/cache.py"
      to: "src/graph/jumpers.py"
      via: "generate_or_load_graph calls designate_jumpers and caches jumper info alongside graph"
      pattern: "designate_jumpers"
---

<objective>
Implement graph caching by config hash to avoid redundant regeneration across sweep configs.

Purpose: In the parameter sweep (Phase 10), many configurations share the same graph parameters but differ in training/model parameters. Caching graphs by their graph-specific config hash eliminates redundant O(n^2) graph generation and O(r * nnz * n_jumpers) non-triviality verification. This directly supports the variable-r-per-graph design where each graph is expensive to generate and validate.

Output: Cache module with save/load/generate-or-load API, gzip-compressed pickle storage, and tests proving cache correctness and key identity.
</objective>

<execution_context>
@/root/.claude/get-shit-done/workflows/execute-plan.md
@/root/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-dcsbm-graph-generation/02-CONTEXT.md
@.planning/phases/02-dcsbm-graph-generation/02-RESEARCH.md
@src/config/hashing.py
@src/graph/types.py
@src/graph/dcsbm.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Graph cache save/load with gzip pickle</name>
  <files>
    src/graph/cache.py
    src/graph/__init__.py
  </files>
  <action>
**src/graph/cache.py** — Caching module:

Import logging: `log = logging.getLogger(__name__)`

`DEFAULT_CACHE_DIR = Path(".cache/graphs")`

`graph_cache_key(config: ExperimentConfig) -> str`:
- Use `graph_config_hash(config)` from `src.config.hashing` for the graph-parameter part
- Append seed: `f"{graph_config_hash(config)}_s{config.seed}"`
- This ensures same graph params + same seed = cache hit, different seed = miss

`_cache_path(config: ExperimentConfig, cache_dir: Path = DEFAULT_CACHE_DIR) -> Path`:
- Return `cache_dir / graph_cache_key(config)`

`save_graph(graph_data: GraphData, jumpers: list[JumperInfo], config: ExperimentConfig, cache_dir: Path = DEFAULT_CACHE_DIR) -> Path`:
- Create cache directory: `cache_path = _cache_path(config, cache_dir)`; `cache_path.mkdir(parents=True, exist_ok=True)`
- Save adjacency matrix: `scipy.sparse.save_npz(cache_path / "adjacency.npz", graph_data.adjacency)`
- Save metadata as JSON: `cache_path / "metadata.json"` containing:
  - n, K, block_size, generation_seed, attempt
  - block_assignments as list
  - theta as list
  - config_hash: graph_config_hash(config)
  - seed: config.seed
  - timestamp: ISO 8601 UTC
- Save jumpers as JSON: `cache_path / "jumpers.json"` containing list of dicts with vertex_id, source_block, target_block, r for each JumperInfo
- Log info: "Graph cached at {cache_path}"
- Return cache_path

`load_graph(config: ExperimentConfig, cache_dir: Path = DEFAULT_CACHE_DIR) -> tuple[GraphData, list[JumperInfo]] | None`:
- Compute cache_path
- If any required file missing (adjacency.npz, metadata.json, jumpers.json): return None
- Load adjacency: `scipy.sparse.load_npz(cache_path / "adjacency.npz")`
- Load metadata: parse JSON, reconstruct GraphData
  - block_assignments: `np.array(metadata["block_assignments"])`
  - theta: `np.array(metadata["theta"])`
- Load jumpers: parse JSON, reconstruct list of JumperInfo
- Log info: "Graph loaded from cache: {cache_path}"
- Return (GraphData, list[JumperInfo])

`generate_or_load_graph(config: ExperimentConfig, cache_dir: Path = DEFAULT_CACHE_DIR, max_retries: int = 10) -> tuple[GraphData, list[JumperInfo]]`:
- Try load_graph first
- If cache hit: log "Cache hit for {graph_cache_key(config)}", return cached data
- If cache miss:
  - log "Cache miss for {graph_cache_key(config)}, generating..."
  - Call generate_dcsbm_graph(config, max_retries)
  - Create rng for jumper designation: `np.random.default_rng(config.seed + 1000)` (offset to avoid correlation with graph generation seed)
  - Call designate_jumpers(graph_data, config, rng)
  - Call save_graph(graph_data, jumpers, config, cache_dir)
  - Return (graph_data, jumpers)

**src/graph/__init__.py** update:
- Add re-exports: `save_graph`, `load_graph`, `generate_or_load_graph`, `graph_cache_key`
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -c "
from src.graph.cache import graph_cache_key, save_graph, load_graph
from src.config import ANCHOR_CONFIG
from src.config.hashing import graph_config_hash
# Cache key includes seed
key = graph_cache_key(ANCHOR_CONFIG)
assert f'_s{ANCHOR_CONFIG.seed}' in key, f'Key missing seed: {key}'
assert graph_config_hash(ANCHOR_CONFIG) in key, f'Key missing graph hash: {key}'
# Different description same cache key
from dataclasses import replace
cfg2 = replace(ANCHOR_CONFIG, description='test')
assert graph_cache_key(ANCHOR_CONFIG) == graph_cache_key(cfg2), 'Description should not affect cache key'
print(f'Cache key: {key}')
print('ALL CHECKS PASSED')
"</automated>
  </verify>
  <done>
Graph cache module provides save/load with gzip-compressed sparse matrix storage and JSON metadata. Cache key derived from graph_config_hash + seed. generate_or_load_graph provides transparent caching.
  </done>
</task>

<task type="auto">
  <name>Task 2: Graph cache integration tests</name>
  <files>
    tests/test_graph_cache.py
  </files>
  <action>
**tests/test_graph_cache.py** — pytest tests:

All tests use `tmp_path` fixture for isolated cache directories.

`test_save_and_load_roundtrip(tmp_path)`:
- Generate graph with ANCHOR_CONFIG
- Designate jumpers
- save_graph to tmp_path
- load_graph from tmp_path
- Assert loaded adjacency matrix is identical: `(loaded_graph.adjacency != original_graph.adjacency).nnz == 0`
- Assert loaded block_assignments match: `np.array_equal`
- Assert loaded theta match: `np.allclose`
- Assert loaded jumpers match: same vertex_ids, same r values, same target_blocks

`test_generate_or_load_caches_on_first_call(tmp_path)`:
- Call generate_or_load_graph with ANCHOR_CONFIG and tmp_path
- Assert cache files exist: adjacency.npz, metadata.json, jumpers.json

`test_generate_or_load_hits_cache_on_second_call(tmp_path)`:
- Call generate_or_load_graph twice with same config
- Assert second call is faster (rough timing check, or verify no log message about "generating")
- Assert returned data is identical

`test_cache_key_same_for_same_config()`:
- Two identical configs produce same cache key

`test_cache_key_differs_for_different_seed()`:
- Same graph params, different seed -> different cache key
- `replace(ANCHOR_CONFIG, seed=99)` should have different key

`test_cache_key_differs_for_different_graph_params()`:
- Different n or K -> different cache key

`test_cache_key_ignores_non_graph_params()`:
- Different description, tags, or model params -> same cache key

`test_load_returns_none_for_missing_cache(tmp_path)`:
- load_graph with no cache files returns None

`test_cache_metadata_contains_expected_fields(tmp_path)`:
- Generate, save, then read metadata.json directly
- Assert it contains: n, K, block_size, generation_seed, config_hash, seed, timestamp

`test_cache_jumpers_roundtrip(tmp_path)`:
- Save then load, verify each JumperInfo field matches
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -m pytest tests/test_graph_cache.py -v</automated>
  </verify>
  <done>
Graph caching works correctly: save/load roundtrip preserves graph data exactly. generate_or_load_graph transparently caches on first call and hits cache on subsequent calls. Cache keys correctly include seed and graph params, exclude non-graph params. All tests pass.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_graph_cache.py -v` — All cache tests pass
2. `python -c "
import tempfile, time
from pathlib import Path
from src.graph.cache import generate_or_load_graph
from src.config import ANCHOR_CONFIG
with tempfile.TemporaryDirectory() as td:
    cache_dir = Path(td)
    t0 = time.time()
    g1, j1 = generate_or_load_graph(ANCHOR_CONFIG, cache_dir)
    t1 = time.time()
    g2, j2 = generate_or_load_graph(ANCHOR_CONFIG, cache_dir)
    t2 = time.time()
    print(f'First call (generate): {t1-t0:.3f}s')
    print(f'Second call (cache): {t2-t1:.3f}s')
    print(f'Speedup: {(t1-t0)/(t2-t1):.1f}x')
    assert (g1.adjacency != g2.adjacency).nnz == 0, 'Graphs should be identical'
    print('CACHE ROUNDTRIP VERIFIED')
"` — Cache hit is faster than generation
3. `python -m pytest tests/ -v` — All Phase 1 and Phase 2 tests pass
</verification>

<success_criteria>
- save_graph stores adjacency.npz, metadata.json, and jumpers.json in cache directory
- load_graph reconstructs identical GraphData and JumperInfo list
- generate_or_load_graph hits cache on second call (no regeneration)
- Cache key = graph_config_hash + seed (same graph params + same seed = cache hit)
- Non-graph params (description, tags, model config) don't affect cache key
- Different seeds produce different cache keys
- All tests in tests/test_graph_cache.py pass
- All existing tests (Phase 1 + Phase 2) still pass
</success_criteria>

<output>
After completion, create `.planning/phases/02-dcsbm-graph-generation/02-03-SUMMARY.md`
</output>
