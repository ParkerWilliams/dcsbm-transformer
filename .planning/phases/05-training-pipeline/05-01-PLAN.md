---
phase: 05-training-pipeline
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/training/__init__.py
  - src/training/data.py
  - src/training/trainer.py
  - tests/test_training.py
autonomous: true
requirements: [TRNG-01, TRNG-04]

must_haves:
  truths:
    - "TransformerLM trains on walk data with cross-entropy next-token prediction loss"
    - "AdamW optimizer with cosine LR schedule (10% linear warmup) drives parameter updates"
    - "Training loss decreases over multiple epochs on walk data"
    - "Per-step training losses are accumulated in a list for curve logging"
    - "tqdm progress bar shows per-epoch progress with one-line epoch summary"
  artifacts:
    - path: "src/training/__init__.py"
      provides: "Package init with public API exports"
    - path: "src/training/data.py"
      provides: "WalkDataset and DataLoader creation from WalkResult"
      exports: ["WalkDataset", "create_dataloader"]
    - path: "src/training/trainer.py"
      provides: "Trainer class with train_epoch, cosine_with_warmup, training loop"
      exports: ["Trainer", "TrainResult"]
    - path: "tests/test_training.py"
      provides: "TDD tests for training loop, optimizer, scheduler, data loading"
      min_lines: 80
  key_links:
    - from: "src/training/data.py"
      to: "src/walk/types.py"
      via: "WalkResult.walks -> torch.Tensor chunking"
      pattern: "WalkResult"
    - from: "src/training/trainer.py"
      to: "src/model/transformer.py"
      via: "TransformerLM forward pass in training mode"
      pattern: "ExtractionMode.NONE"
    - from: "src/training/trainer.py"
      to: "torch.optim.AdamW"
      via: "Optimizer creation with weight decay"
      pattern: "AdamW"
---

<objective>
Implement the core training loop that trains TransformerLM on walk corpus data using cross-entropy next-token prediction with AdamW optimizer and cosine LR schedule with 10% linear warmup.

Purpose: Enable the transformer to learn edge structure from DCSBM walk data, forming the foundation for sufficiency gate evaluation (Plan 02).
Output: `src/training/` package with data loading, trainer, and comprehensive TDD tests.
</objective>

<execution_context>
@/root/.claude/get-shit-done/workflows/execute-plan.md
@/root/.claude/get-shit-done/references/tdd.md
@/root/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/phases/05-training-pipeline/05-RESEARCH.md
@.planning/phases/05-training-pipeline/05-CONTEXT.md
@src/model/transformer.py
@src/model/types.py
@src/config/experiment.py
@src/walk/types.py
@src/walk/corpus.py
@src/reproducibility/seed.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: RED - Write failing tests for training data loading and training loop</name>
  <files>tests/test_training.py, src/training/__init__.py</files>
  <action>
Create `src/training/__init__.py` as an empty package init.

Create `tests/test_training.py` with comprehensive failing tests. Use a small model (vocab_size=20, d_model=32, n_layers=2, max_seq_len=16) and tiny synthetic walk data (50 walks of length 32) for fast test execution.

Tests to write:

**WalkDataset tests:**
- `test_walk_dataset_chunks_walks_into_sequences`: Given walks of shape (N, L) and context window w, dataset produces input/target pairs where input is w tokens and target is input shifted by 1. Each walk of length L should produce floor(L/w) - 1 non-overlapping chunks of length w+1 (first w tokens = input, last w tokens = target). Verify shapes and content.
- `test_walk_dataset_len`: Dataset length equals total number of chunks across all walks.
- `test_create_dataloader_batches`: create_dataloader returns a DataLoader that yields batches of the correct shape [batch_size, w+1].

**Cosine schedule with warmup tests:**
- `test_cosine_with_warmup_starts_at_zero`: At step 0, LR multiplier is 0 (or near 0).
- `test_cosine_with_warmup_reaches_peak`: At warmup_steps, LR multiplier is 1.0.
- `test_cosine_with_warmup_decays_to_min`: At total_steps, LR multiplier approaches min_lr_ratio.
- `test_cosine_with_warmup_monotonic_after_warmup`: After warmup, LR is monotonically non-increasing.

**Trainer tests:**
- `test_trainer_single_epoch_returns_losses`: Trainer.train_epoch returns a list of per-step float losses.
- `test_trainer_loss_decreases_over_epochs`: After 5+ epochs on small data, average loss in last epoch is lower than first epoch.
- `test_trainer_uses_adamw`: Verify optimizer is AdamW instance with correct LR and weight decay.
- `test_trainer_gradient_clipping`: Verify gradients are clipped (inject large gradients and check norm after step).

All tests should import from src.training and FAIL because the module doesn't exist yet.

Commit: `test(05-01): add failing tests for training data loading and training loop`
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -m pytest tests/test_training.py -x 2>&1 | tail -5</automated>
    <manual>All tests should FAIL with ImportError or similar (RED phase)</manual>
  </verify>
  <done>tests/test_training.py exists with 10+ test functions, all failing due to missing implementation</done>
</task>

<task type="auto">
  <name>Task 2: GREEN - Implement training data loading, cosine schedule, and Trainer</name>
  <files>src/training/__init__.py, src/training/data.py, src/training/trainer.py</files>
  <action>
**src/training/data.py:**
- `WalkDataset(torch.utils.data.Dataset)`: Takes a numpy walk array (N, L) and context window w. Chunks each walk into non-overlapping subsequences of length w+1. The first w tokens of each chunk are input, the last w tokens are target (shifted by 1). Store as a single torch.LongTensor of shape (total_chunks, w+1).
- `create_dataloader(dataset, batch_size, shuffle, seed)`: Returns a DataLoader with `worker_init_fn=seed_worker` from `src.reproducibility.seed`, using a torch.Generator seeded deterministically. Pin memory if CUDA available.

**src/training/trainer.py:**
- `cosine_with_warmup(step, warmup_steps, total_steps, min_lr_ratio=0.1) -> float`: LambdaLR function. Linear warmup from 0 to 1 over warmup_steps, then cosine decay from 1 to min_lr_ratio over remaining steps.
- `@dataclass TrainResult`: Holds `epoch_losses: list[list[float]]` (per-epoch list of per-step losses), `final_lr: float`.
- `class Trainer`:
  - `__init__(self, model, config, device)`: Creates AdamW optimizer with lr=config.training.learning_rate, weight_decay=0.01 (Claude's discretion per CONTEXT.md). Creates LambdaLR scheduler using cosine_with_warmup. Stores gradient clip max_norm=1.0 (Claude's discretion).
  - `train_epoch(self, dataloader) -> list[float]`: One epoch of training. For each batch: zero_grad, forward pass with ExtractionMode.NONE, compute cross-entropy loss on logits vs shifted targets (reshape to [N*w, vocab_size] and [N*w]), backward, clip_grad_norm_, optimizer.step(), scheduler.step(). Returns list of per-step loss floats. Uses tqdm for progress bar with format: `Epoch {N}: {bar} {n}/{total} [loss={loss:.4f}, lr={lr:.2e}]`.
  - Properties: `optimizer`, `scheduler`, `current_lr`.

**src/training/__init__.py:**
- Export: WalkDataset, create_dataloader, Trainer, TrainResult, cosine_with_warmup.

Implementation notes per CONTEXT.md:
- Batch size 64 (from config.training.batch_size)
- Peak LR 3e-4 (from config.training.learning_rate)
- Total steps = max_epochs * steps_per_epoch (for cosine schedule)
- The Trainer doesn't handle epochs/gate — that's Plan 02. It provides train_epoch and the building blocks.
- Weight decay 0.01 (Claude's discretion)
- Gradient clipping max_norm=1.0 (Claude's discretion)
- Cosine min LR ratio 0.1 (Claude's discretion — 10% of peak)

Commit: `feat(05-01): implement training data loading and training loop`
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -m pytest tests/test_training.py -x -v</automated>
    <manual>All tests pass (GREEN phase)</manual>
  </verify>
  <done>All tests pass. WalkDataset correctly chunks walks, cosine_with_warmup produces correct LR schedule, Trainer trains for one epoch and returns per-step losses, loss decreases over multiple epochs.</done>
</task>

</tasks>

<verification>
- `python -m pytest tests/test_training.py -v` — all tests pass
- `python -m pytest tests/ -x` — full test suite still passes (no regressions)
- Training loss demonstrably decreases over multiple epochs on small synthetic data
- LR schedule starts near 0, reaches peak at 10% of steps, decays to min_lr_ratio
</verification>

<success_criteria>
- src/training/ package exists with data.py, trainer.py, __init__.py
- WalkDataset chunks walks into input/target pairs correctly
- Trainer uses AdamW with cosine+warmup LR schedule
- Per-step losses are returned for downstream curve logging
- All tests pass with pytest
- No regressions in existing test suite
</success_criteria>

<output>
After completion, create `.planning/phases/05-training-pipeline/05-01-SUMMARY.md`
</output>
