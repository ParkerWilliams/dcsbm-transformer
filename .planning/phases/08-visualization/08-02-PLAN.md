---
phase: 08-visualization
plan: 02
type: tdd
wave: 2
depends_on:
  - "08-01"
files_modified:
  - src/visualization/distributions.py
  - src/visualization/heatmap.py
  - src/visualization/render.py
  - tests/test_visualization.py
autonomous: true
requirements:
  - PLOT-05
  - PLOT-06

must_haves:
  truths:
    - "Pre/post failure distribution comparison plots show metric value distributions for positions before vs after the failure event"
    - "Predictive horizon heatmap renders across the (r, w) parameter grid with at least the anchor config data point visible"
    - "render_all() loads result.json + token_metrics.npz and orchestrates generation of all figure types to results/{experiment_id}/figures/"
    - "All plots produced by render_all use the consistent project style and are saved as both PNG and SVG"
  artifacts:
    - path: "src/visualization/distributions.py"
      provides: "plot_pre_post_distributions() for PLOT-05"
      contains: "def plot_pre_post_distributions"
    - path: "src/visualization/heatmap.py"
      provides: "plot_horizon_heatmap() for PLOT-06"
      contains: "def plot_horizon_heatmap"
    - path: "src/visualization/render.py"
      provides: "render_all() orchestrator, load_result_data()"
      contains: "def render_all"
    - path: "tests/test_visualization.py"
      provides: "Tests for distributions, heatmap, and render_all"
  key_links:
    - from: "src/visualization/render.py"
      to: "src/results/schema.py"
      via: "load_result for reading result.json"
      pattern: "from src\\.results\\.schema import load_result"
    - from: "src/visualization/render.py"
      to: "src/visualization/style.py"
      via: "apply_style() called at start of render_all"
      pattern: "from src\\.visualization\\.style import apply_style"
    - from: "src/visualization/render.py"
      to: "src/visualization/event_aligned.py"
      via: "plot_event_aligned import"
      pattern: "from src\\.visualization\\.event_aligned import"
    - from: "src/visualization/render.py"
      to: "src/visualization/training.py"
      via: "plot_training_curves import"
      pattern: "from src\\.visualization\\.training import"
    - from: "src/visualization/render.py"
      to: "src/visualization/auroc.py"
      via: "plot_auroc_curves import"
      pattern: "from src\\.visualization\\.auroc import"
    - from: "src/visualization/render.py"
      to: "src/visualization/confusion.py"
      via: "plot_confusion_matrix import"
      pattern: "from src\\.visualization\\.confusion import"
---

<objective>
Implement distribution comparison plots, predictive horizon heatmap, and the render_all orchestrator that ties all visualization functions together.

Purpose: This plan completes the visualization module by adding the remaining plot types (pre/post failure distributions, horizon heatmap) and the orchestrator function that loads experiment results and generates all figures. The render_all function is the single entry point that downstream workflows (Phase 9 reporting) and the user call to generate all figures for an experiment.

Output: Three modules (distributions.py, heatmap.py, render.py) with test coverage, completing the full visualization package.
</objective>

<execution_context>
@/root/.claude/get-shit-done/workflows/execute-plan.md
@/root/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-visualization/08-CONTEXT.md
@.planning/phases/08-visualization/08-RESEARCH.md
@.planning/phases/08-visualization/08-01-SUMMARY.md
@src/visualization/style.py
@src/visualization/event_aligned.py
@src/visualization/training.py
@src/visualization/auroc.py
@src/visualization/confusion.py
@src/results/schema.py
@src/analysis/event_extraction.py
@src/analysis/auroc_horizon.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Distribution plots and predictive horizon heatmap (TDD)</name>
  <files>
    src/visualization/distributions.py
    src/visualization/heatmap.py
    tests/test_visualization.py
  </files>
  <action>
**RED phase: Add tests to tests/test_visualization.py**

Add these test cases:

15. `test_distribution_plot_returns_figure` -- Create synthetic metric_values array (20 sequences x 100 steps), a failure_index array (some with violations, some with -1 for no violation). Call `plot_pre_post_distributions(metric_values, failure_index, window=5, metric_name="qkt.stable_rank")`. Verify returns a matplotlib Figure with at least 1 axes.

16. `test_distribution_plot_shows_two_groups` -- Verify the returned axes contains visual elements for both pre-failure and post-failure groups. Check that there are at least 2 collection artists or 2 violin bodies (depending on implementation).

17. `test_distribution_plot_handles_no_violations` -- Call with failure_index all -1 (no violations). Verify it returns a Figure (possibly with a "no data" annotation or just the post-failure distribution) without raising.

18. `test_horizon_heatmap_single_point` -- Create a horizon_data dict with a single (r=57, w=64) entry: `{(57, 64): 3}`. Call `plot_horizon_heatmap(horizon_data)`. Verify returns Figure with one axes. The single cell should be visible.

19. `test_horizon_heatmap_full_grid` -- Create a horizon_data dict with a 3x3 grid (r in [32, 45, 57], w in [32, 64, 128]) with integer horizon values. Verify heatmap renders correctly with row/column labels.

20. `test_horizon_heatmap_sparse_grid` -- Create a horizon_data dict with only 2 out of 9 possible cells filled (rest NaN). Verify the heatmap handles sparse data gracefully (NaN cells should be masked or shown differently).

**GREEN phase: Implement**

**src/visualization/distributions.py (PLOT-05):**
```python
def plot_pre_post_distributions(
    metric_values: np.ndarray,   # [n_sequences, max_steps]
    failure_index: np.ndarray,   # [n_sequences], -1 if no violation
    window: int = 5,
    metric_name: str = "SVD metric",
    plot_type: str = "violin",   # "violin", "box", or "histogram"
) -> plt.Figure:
```
- For each sequence with a violation (failure_index >= 0):
  - Pre-failure values: metric at positions [failure_index - window, failure_index)
  - Post-failure values: metric at positions (failure_index, failure_index + window]
- Filter NaN values from both groups
- Create violin plot (default), box plot, or histogram comparing the two distributions
- Use VIOLATION_COLOR for post-failure, CONTROL_COLOR for pre-failure
- X-axis labels: "Pre-failure", "Post-failure"
- Y-axis: metric_name
- Title: f"Distribution comparison: {metric_name}"
- If no violations exist, create a figure with a text annotation "No violations detected"

**src/visualization/heatmap.py (PLOT-06):**
```python
def plot_horizon_heatmap(
    horizon_data: dict[tuple[int, int], float],  # (r, w) -> horizon value
    metric_name: str = "Best primary metric",
    threshold: float = 0.75,
) -> plt.Figure:
```
- Extract unique r_values and w_values from keys, sort them
- Build a 2D array (rows=r_values, cols=w_values) filled with NaN
- Fill in available horizon values from horizon_data
- Use sns.heatmap with annot=True, fmt=".1f", mask for NaN cells
- Colormap: "YlOrRd" (yellow for low, red for high horizon)
- X-axis: "Context window w", Y-axis: "Jump length r"
- Title: f"Predictive horizon: {metric_name} (AUROC > {threshold})"
- Handle single data point gracefully (1x1 heatmap)

Run tests: `.venv/bin/pytest tests/test_visualization.py -x -v -k "distribution or heatmap"`
  </action>
  <verify>
    <automated>.venv/bin/pytest tests/test_visualization.py -x -v -k "distribution or heatmap"</automated>
    <manual>Verify distribution plots show pre/post comparison and heatmap renders sparse grid</manual>
    <sampling_rate>run after this task commits, before next task begins</sampling_rate>
  </verify>
  <done>
    - Distribution plots compare pre-failure vs post-failure metric values with violin/box visualization
    - Distribution plots handle edge case of no violations gracefully
    - Heatmap renders (r, w) parameter grid with NaN masking for missing entries
    - Heatmap handles single data point (anchor config) gracefully
    - All 6 new tests pass plus all prior tests
  </done>
</task>

<task type="auto">
  <name>Task 2: Render orchestrator -- load data and generate all figures (TDD)</name>
  <files>
    src/visualization/render.py
    tests/test_visualization.py
  </files>
  <action>
**RED phase: Add tests to tests/test_visualization.py**

Add these test cases:

21. `test_render_all_creates_figures_directory` -- Create a minimal synthetic result directory with a result.json and token_metrics.npz. Call `render_all(result_dir)`. Verify a `figures/` subdirectory is created inside result_dir.

22. `test_render_all_produces_png_and_svg_pairs` -- After render_all, verify that for every .png file in figures/, there is a matching .svg file. Verify at least 3 figure pairs were created (training_curves, confusion_matrix, and at least one event-aligned or AUROC).

23. `test_render_all_handles_missing_curves` -- Create result.json without metrics.curves (e.g., gate_passed=false, minimal data). Verify render_all does not crash -- it skips training curves gracefully.

24. `test_render_all_handles_no_violations` -- Create token_metrics.npz where failure_index is all -1 (model never violated). Verify render_all still produces whatever plots it can (training curves, confusion matrix) without crashing.

25. `test_load_result_data_returns_expected_keys` -- Call the internal `load_result_data(result_dir)` function. Verify it returns a dict with keys including "result" (the JSON), "metric_arrays" (from NPZ), and "curves" (from result.json).

For these tests, create fixtures that build minimal but valid result.json files (using the schema from src/results/schema.py) and token_metrics.npz files with small synthetic arrays.

**GREEN phase: Implement src/visualization/render.py**

```python
"""Orchestrator: render all figures for a single experiment.

Reads result.json + token_metrics.npz, calls all plot functions,
saves to results/{experiment_id}/figures/ as PNG + SVG.
"""
import json
import logging
from pathlib import Path

import numpy as np

from src.visualization.style import apply_style, save_figure
from src.visualization.event_aligned import plot_event_aligned
from src.visualization.training import plot_training_curves
from src.visualization.auroc import plot_auroc_curves
from src.visualization.confusion import plot_confusion_matrix
from src.visualization.distributions import plot_pre_post_distributions
from src.visualization.heatmap import plot_horizon_heatmap
from src.analysis.event_extraction import AnalysisEvent, extract_events, filter_contaminated_events
from src.evaluation.behavioral import RuleOutcome

log = logging.getLogger(__name__)


def load_result_data(result_dir: str | Path) -> dict:
    """Load result.json and token_metrics.npz from an experiment directory.

    Args:
        result_dir: Path to results/{experiment_id}/ directory.

    Returns:
        Dict with keys: result (JSON dict), metric_arrays (NPZ dict),
        curves (from result.metrics.curves if present).
    """
    ...


def render_all(result_dir: str | Path) -> list[Path]:
    """Generate all figures for a single experiment.

    Loads data, applies style, generates each plot type, saves
    as PNG + SVG to {result_dir}/figures/.

    Args:
        result_dir: Path to results/{experiment_id}/ directory.

    Returns:
        List of paths to generated figure files.
    """
    ...
```

Implementation details for render_all:
1. Call `apply_style()` at the start
2. Call `load_result_data(result_dir)` to get JSON + NPZ data
3. Set `figures_dir = Path(result_dir) / "figures"`
4. Render each plot type, wrapping each in try/except to ensure one failure doesn't block others:
   - **Training curves (PLOT-02)**: If `curves` data exists in result.json metrics, call `plot_training_curves(curves)` and save as "training_curves"
   - **Confusion matrix (PLOT-04)**: If edge_valid and rule_outcome in NPZ, call `plot_confusion_matrix(edge_valid, rule_outcome)` and save as "confusion_matrix"
   - **Event-aligned plots (PLOT-01)**: For each primary SVD metric key in NPZ, extract events from generated + rule_outcome + failure_index arrays (using event_extraction module), call `plot_event_aligned(metric_values, events, ...)` and save as f"event_aligned_{metric_key}" (sanitize dots to underscores in filename)
   - **AUROC curves (PLOT-03)**: If predictive_horizon results exist in result.json metrics, call `plot_auroc_curves(auroc_results, r_value)` and save as "auroc_curves"
   - **Distribution plots (PLOT-05)**: For each primary metric, call `plot_pre_post_distributions(metric_values, failure_index, ...)` and save as f"distribution_{metric_key}"
   - **Heatmap (PLOT-06)**: Only if multiple (r, w) configs are available. For single experiment, skip with log message. Include a `render_heatmap_from_sweep(sweep_dir)` function placeholder that loads multiple result.json files.
5. Log summary: "Generated {N} figures to {figures_dir}"
6. Return list of all generated file paths

The heatmap is primarily useful with sweep data (Phase 10). For single experiments, include a `render_horizon_heatmap(sweep_results_dir)` function that:
- Scans sweep_results_dir for all result.json files
- Extracts (r, w) from config and horizon from metrics
- Calls plot_horizon_heatmap with the aggregated data
- This makes the heatmap testable with synthetic multi-result data

Run all tests: `.venv/bin/pytest tests/test_visualization.py -x -v`
  </action>
  <verify>
    <automated>.venv/bin/pytest tests/test_visualization.py -x -v</automated>
    <manual>Verify render_all produces PNG+SVG pairs for all available plot types</manual>
    <sampling_rate>run after this task commits, before next task begins</sampling_rate>
  </verify>
  <done>
    - render_all loads result.json + token_metrics.npz and generates all applicable figures
    - Each figure is saved as PNG (300 dpi) + SVG pair in figures/ subdirectory
    - Missing data (no curves, no violations) is handled gracefully without crashing
    - render_horizon_heatmap can aggregate sweep results for the (r, w) heatmap
    - All 25 tests pass with no regressions in existing suite
  </done>
</task>

</tasks>

<verification>
```bash
# All visualization tests pass
.venv/bin/pytest tests/test_visualization.py -x -v

# No regressions in existing tests
.venv/bin/pytest tests/ -x --timeout=120

# Verify all modules importable
.venv/bin/python -c "from src.visualization.distributions import plot_pre_post_distributions; print('distributions OK')"
.venv/bin/python -c "from src.visualization.heatmap import plot_horizon_heatmap; print('heatmap OK')"
.venv/bin/python -c "from src.visualization.render import render_all, load_result_data; print('render OK')"
```
</verification>

<success_criteria>
- Pre/post failure distribution plots compare metric values before vs after violation events (PLOT-05)
- Predictive horizon heatmap renders (r, w) grid with at least anchor config data point (PLOT-06)
- render_all orchestrator generates all figures from a single experiment's result data
- Every figure saved as PNG (300 dpi) + SVG pair (PLOT-08)
- Graceful handling of missing data, no violations, and single-experiment scenarios
- All tests pass with no regressions
</success_criteria>

<output>
After completion, create `.planning/phases/08-visualization/08-02-SUMMARY.md`
</output>
