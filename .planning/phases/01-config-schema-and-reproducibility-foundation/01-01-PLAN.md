---
phase: 01-config-schema-and-reproducibility-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - Makefile
  - src/__init__.py
  - src/config/__init__.py
  - src/config/experiment.py
  - src/config/defaults.py
  - src/config/hashing.py
  - src/config/serialization.py
  - src/results/__init__.py
  - src/results/schema.py
  - src/results/experiment_id.py
  - tests/__init__.py
  - tests/test_config.py
  - tests/test_results.py
autonomous: true
requirements:
  - MGMT-01
  - MGMT-05

must_haves:
  truths:
    - "ExperimentConfig with anchor defaults instantiates without error and is immutable (frozen)"
    - "ExperimentConfig serializes to JSON and deserializes back with identical config hash"
    - "Cross-parameter validation rejects invalid configs (walk_length < 2*w, corpus_size < 100*n, n_heads != 1)"
    - "Config hash excludes seed for graph caching but includes seed for full identity"
    - "A result dict with all required fields passes validate_result() with zero errors"
    - "A result dict missing required fields fails validate_result() with specific error messages"
    - "generate_experiment_id() produces scannable slug format matching n500_w64_r57_d128_L4_s42_{timestamp}"
  artifacts:
    - path: "src/config/experiment.py"
      provides: "GraphConfig, ModelConfig, TrainingConfig, SweepConfig, ExperimentConfig dataclasses"
      contains: "frozen=True, slots=True"
    - path: "src/config/defaults.py"
      provides: "ANCHOR_CONFIG constant"
      contains: "ANCHOR_CONFIG"
    - path: "src/config/hashing.py"
      provides: "config_hash, graph_config_hash, full_config_hash functions"
      exports: ["config_hash", "graph_config_hash", "full_config_hash"]
    - path: "src/config/serialization.py"
      provides: "config_to_json, config_from_json round-trip functions"
      exports: ["config_to_json", "config_from_json"]
    - path: "src/results/schema.py"
      provides: "validate_result, write_result functions"
      exports: ["validate_result", "write_result"]
    - path: "src/results/experiment_id.py"
      provides: "generate_experiment_id function"
      exports: ["generate_experiment_id"]
  key_links:
    - from: "src/config/serialization.py"
      to: "src/config/hashing.py"
      via: "round-trip hash identity: config_hash(original) == config_hash(config_from_json(config_to_json(original)))"
      pattern: "config_hash.*config_from_json.*config_to_json"
    - from: "src/config/hashing.py"
      to: "src/config/experiment.py"
      via: "graph_config_hash operates on config.graph only (excludes seed)"
      pattern: "graph_config_hash.*config\\.graph"
    - from: "src/results/schema.py"
      to: "src/results/experiment_id.py"
      via: "write_result calls generate_experiment_id to create output directory"
      pattern: "generate_experiment_id"
    - from: "src/results/schema.py"
      to: "validate_result"
      via: "write_result validates before writing to disk"
      pattern: "validate_result.*errors"
---

<objective>
Create the experiment configuration system (frozen, hashable, serializable dataclasses) and result.json schema with validation.

Purpose: Every downstream phase depends on ExperimentConfig for parameterization and result.json for output storage. This plan establishes both foundational data structures so they are frozen before any experiment code is written.

Output: Config module (`src/config/`) with dataclasses, hashing, serialization, and anchor defaults. Results module (`src/results/`) with schema validation, result writing, and experiment ID generation. Project scaffolding (pyproject.toml, Makefile, package structure).
</objective>

<execution_context>
@/root/.claude/get-shit-done/workflows/execute-plan.md
@/root/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-config-schema-and-reproducibility-foundation/01-CONTEXT.md
@.planning/phases/01-config-schema-and-reproducibility-foundation/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Project scaffolding and ExperimentConfig dataclass system</name>
  <files>
    pyproject.toml
    Makefile
    src/__init__.py
    src/config/__init__.py
    src/config/experiment.py
    src/config/defaults.py
    src/config/hashing.py
    src/config/serialization.py
  </files>
  <action>
Create the project scaffolding and full config module:

**pyproject.toml:**
- Project name: `dcsbm-transformer`
- Python >= 3.12
- Dependencies: `torch >= 2.0`, `numpy >= 2.0`, `dacite >= 1.8`
- Dev dependencies: `pytest >= 8.0`
- Configure pytest to discover tests in `tests/`

**Makefile:**
- `make test`: run `python -m pytest tests/ -v`
- `make run`: placeholder `python run_experiment.py` (script not yet created)
- `make sweep`: placeholder
- `make pdf`: placeholder
- All Python commands should use venv python (but don't create the venv in the Makefile, just document it)

**src/config/experiment.py** — All config dataclasses, all `@dataclass(frozen=True, slots=True)`:

`GraphConfig`: n (int, default 500), K (int, default 4), p_in (float, default 0.25), p_out (float, default 0.03), n_jumpers_per_block (int, default 2). These anchor defaults come from CONTEXT.md (n=500) and RESEARCH.md open questions (K=4, p_in=0.25, p_out=0.03, n_jumpers_per_block=2 as anchor defaults).

`ModelConfig`: d_model (int, default 128), n_layers (int, default 4), n_heads (int, default 1), dropout (float, default 0.0).

`TrainingConfig`: w (int, default 64), walk_length (int, default 256), corpus_size (int, default 200_000), r (int, default 57), learning_rate (float, default 3e-4), batch_size (int, default 64), max_steps (int, default 50_000), eval_interval (int, default 1000), checkpoint_interval (int, default 5000).

`SweepConfig`: Define as a frozen dataclass holding tuples of allowed values for sweep parameters. Fields: n_values (tuple[int, ...]), w_values (tuple[int, ...]), r_scale_values (tuple[float, ...]), d_model_values (tuple[int, ...]), n_layers_values (tuple[int, ...]), K_values (tuple[int, ...]), p_in_values (tuple[float, ...]), p_out_values (tuple[float, ...]), seeds (tuple[int, ...], default (42, 123, 7)). Per RESEARCH.md: define the structure but defer sweep execution logic to Phase 10.

`ExperimentConfig`: graph (GraphConfig), model (ModelConfig), training (TrainingConfig), sweep (SweepConfig | None, default None), seed (int, default 42), description (str, default ""), tags (tuple[str, ...], default ()).

In `ExperimentConfig.__post_init__`, add cross-parameter validation using `object.__setattr__` pattern (since frozen):
- `training.walk_length >= 2 * training.w` or raise ValueError
- `training.corpus_size >= 100 * graph.n` or raise ValueError
- `model.n_heads == 1` or raise ValueError
- `training.r <= training.walk_length` or raise ValueError

**src/config/defaults.py:**
- `ANCHOR_CONFIG = ExperimentConfig()` — instantiate with all defaults, which are the anchor values
- This serves as the single source of truth for anchor parameters

**src/config/hashing.py:**
- `config_hash(config, exclude_fields=None) -> str`: Convert to dict via `asdict()`, remove excluded field paths, `json.dumps(sort_keys=True, ensure_ascii=True, separators=(',', ':'), indent=None)`, SHA-256, return first 16 hex chars.
- `_remove_nested(d, field_path)`: Helper to remove a dotted-path key from nested dict.
- `graph_config_hash(config: ExperimentConfig) -> str`: Hash only `config.graph` (for graph cache sharing across seeds).
- `full_config_hash(config: ExperimentConfig) -> str`: Hash entire config including seed.

**src/config/serialization.py:**
- `config_to_json(config: ExperimentConfig) -> str`: `json.dumps(asdict(config), indent=2, sort_keys=True)`
- `config_from_json(json_str: str) -> ExperimentConfig`: `json.loads` + `dacite.from_dict` with `Config(cast=[tuple], check_types=True, strict=True)`. Note: strict=True per RESEARCH.md pitfall #6 to reject unknown keys.
- `config_to_dict(config: ExperimentConfig) -> dict`: `asdict(config)` wrapper
- `config_from_dict(d: dict) -> ExperimentConfig`: `dacite.from_dict` wrapper

**src/config/__init__.py:**
- Re-export all public names: `ExperimentConfig`, `GraphConfig`, `ModelConfig`, `TrainingConfig`, `SweepConfig`, `ANCHOR_CONFIG`, `config_hash`, `graph_config_hash`, `full_config_hash`, `config_to_json`, `config_from_json`

**src/__init__.py:** Empty file for package.
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -c "
from src.config import ExperimentConfig, ANCHOR_CONFIG, config_to_json, config_from_json, config_hash, graph_config_hash, full_config_hash
# Anchor config instantiates
assert ANCHOR_CONFIG.graph.n == 500
assert ANCHOR_CONFIG.training.w == 64
assert ANCHOR_CONFIG.model.n_heads == 1
# Round-trip
json_str = config_to_json(ANCHOR_CONFIG)
restored = config_from_json(json_str)
assert config_hash(ANCHOR_CONFIG) == config_hash(restored), 'Round-trip hash mismatch'
# Graph hash excludes seed
from dataclasses import replace
cfg2 = replace(ANCHOR_CONFIG, seed=99)
assert graph_config_hash(ANCHOR_CONFIG) == graph_config_hash(cfg2), 'Graph hash should ignore seed'
assert full_config_hash(ANCHOR_CONFIG) != full_config_hash(cfg2), 'Full hash should differ with seed'
# Validation rejects bad configs
try:
    ExperimentConfig(training=__import__('src.config.experiment', fromlist=['TrainingConfig']).TrainingConfig(walk_length=10, w=64))
    assert False, 'Should have raised ValueError'
except ValueError:
    pass
print('ALL CHECKS PASSED')
"</automated>
    <manual>Verify pyproject.toml has correct dependencies and Makefile has required targets</manual>
  </verify>
  <done>
All 5 config dataclasses exist with frozen=True and slots=True. ANCHOR_CONFIG instantiates with locked defaults. Cross-parameter validation rejects invalid configs. Config hashing works with field exclusion. JSON round-trip preserves hash identity.
  </done>
</task>

<task type="auto">
  <name>Task 2: Result schema validation, result writer, and experiment ID generation</name>
  <files>
    src/results/__init__.py
    src/results/schema.py
    src/results/experiment_id.py
    tests/__init__.py
    tests/test_config.py
    tests/test_results.py
  </files>
  <action>
Create the results module and tests for both config and results:

**src/results/experiment_id.py:**
- `generate_experiment_id(config: ExperimentConfig) -> str`: Format `n{n}_w{w}_r{r}_d{d_model}_L{n_layers}_s{seed}_{YYYYMMDD}_{HHMMSS}` using UTC timestamp. Example: `n500_w64_r57_d128_L4_s42_20260224_143012`.

**src/results/schema.py:**
- `REQUIRED_TOP_FIELDS = {"schema_version", "experiment_id", "timestamp", "description", "tags", "config", "metrics"}`
- `REQUIRED_METRICS_FIELDS = {"scalars"}` — at minimum, metrics must have scalars
- `validate_result(result: dict) -> list[str]`: Returns list of error strings (empty = valid). Checks:
  - All REQUIRED_TOP_FIELDS present
  - `metrics.scalars` present
  - For each sequence in `sequences`: if `token_logprobs` or `token_entropy` arrays exist, their length must match `tokens` length
  - `schema_version` is a string
  - `tags` is a list
  - `config` is a dict
  - `timestamp` is a string in ISO 8601 format (basic check: contains "T" or validate with datetime.fromisoformat)
- `write_result(config, metrics, sequences=None, metadata=None, token_metrics=None, results_dir="results") -> str`:
  - Calls `generate_experiment_id(config)` to get experiment_id
  - Creates `results/{experiment_id}/` directory
  - Builds result dict with schema_version="1.0", experiment_id, timestamp (UTC ISO 8601), description from config, tags from config, config as dict, metrics, sequences, and metadata block containing `code_hash` (placeholder "unknown" for now — git_hash comes in Plan 02) and `config_hash` and `graph_config_hash`
  - Calls `validate_result()` — raises ValueError if errors
  - Writes `result.json` with `json.dump(indent=2)`
  - If `token_metrics` provided, writes `token_metrics.npz` with flat keys `{seq_id}/{metric_name}`
  - Returns experiment_id
- `load_result(result_path: str | Path) -> dict`: Load and validate a result.json file

**src/results/__init__.py:**
- Re-export: `validate_result`, `write_result`, `load_result`, `generate_experiment_id`

**tests/__init__.py:** Empty file.

**tests/test_config.py** — pytest tests:
- `test_anchor_config_defaults()`: ANCHOR_CONFIG has correct values (n=500, w=64, t=200000, d_model=128, n_layers=4, n_heads=1, r=57, walk_length=256)
- `test_config_frozen()`: Assigning to ANCHOR_CONFIG.seed raises FrozenInstanceError
- `test_config_round_trip_hash()`: config_to_json -> config_from_json preserves config_hash
- `test_graph_hash_ignores_seed()`: Two configs differing only in seed have same graph_config_hash
- `test_full_hash_includes_seed()`: Two configs differing only in seed have different full_config_hash
- `test_validation_walk_length()`: walk_length < 2*w raises ValueError
- `test_validation_corpus_size()`: corpus_size < 100*n raises ValueError
- `test_validation_n_heads()`: n_heads != 1 raises ValueError
- `test_config_hash_deterministic()`: Same config produces same hash on repeated calls
- `test_serialization_strict_rejects_extra_keys()`: JSON with unknown keys raises error via dacite strict mode

**tests/test_results.py** — pytest tests:
- `test_validate_result_valid()`: Complete result dict passes with no errors
- `test_validate_result_missing_fields()`: Result missing required fields returns appropriate errors
- `test_validate_result_array_length_mismatch()`: Sequence with mismatched token_logprobs length returns error
- `test_generate_experiment_id_format()`: ID matches expected pattern `n500_w64_r57_d128_L4_s42_\d{8}_\d{6}`
- `test_write_result_creates_files(tmp_path)`: write_result creates result.json in correct directory structure
- `test_write_result_validates_before_write(tmp_path)`: Invalid result raises ValueError and does not create file
- `test_write_result_with_token_metrics(tmp_path)`: write_result with token_metrics creates both result.json and token_metrics.npz
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -m pytest tests/test_config.py tests/test_results.py -v</automated>
  </verify>
  <done>
validate_result correctly identifies both valid and invalid result dicts. write_result creates proper directory structure with result.json and optional token_metrics.npz. generate_experiment_id produces scannable slug format. All config and result tests pass.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/ -v` — All tests pass
2. `python -c "from src.config import ANCHOR_CONFIG; print(ANCHOR_CONFIG)"` — Config prints with anchor defaults
3. `python -c "from src.config import config_to_json, config_from_json, config_hash, ANCHOR_CONFIG; assert config_hash(ANCHOR_CONFIG) == config_hash(config_from_json(config_to_json(ANCHOR_CONFIG)))"` — Round-trip identity
4. `make test` — Makefile target runs tests successfully
</verification>

<success_criteria>
- ExperimentConfig with all 5 sub-dataclasses instantiates with anchor defaults
- Cross-parameter validation catches invalid configs
- JSON round-trip preserves config hash identity
- Graph hash is seed-independent, full hash is seed-dependent
- validate_result accepts valid dicts and rejects invalid ones
- write_result creates proper directory with result.json
- Experiment IDs are scannable (param slug format)
- All tests in tests/test_config.py and tests/test_results.py pass
- pyproject.toml has correct deps, Makefile has required targets
</success_criteria>

<output>
After completion, create `.planning/phases/01-config-schema-and-reproducibility-foundation/01-01-SUMMARY.md`
</output>
