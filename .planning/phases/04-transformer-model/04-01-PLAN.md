---
phase: 04-transformer-model
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/model/__init__.py
  - src/model/types.py
  - src/model/attention.py
  - src/model/block.py
  - src/model/transformer.py
  - tests/test_model.py
autonomous: true
requirements:
  - MODL-01
  - MODL-02
  - MODL-03

must_haves:
  truths:
    - "TransformerLM accepts configurable d_model (64, 128, 256), n_layers (2, 4, 6), and enforces exactly 1 attention head"
    - "With ExtractionMode.SVD_TARGETS, forward pass returns ForwardOutput containing logits plus detached QK^T (zero-filled causal mask), attention weights A, and value matrix V stacked across all layers"
    - "Wv and Wo are accessible as named model parameters per layer for WvWo SVD target computation"
    - "get_wvwo() returns stacked [n_layers, d_model, d_model] detached weight product"
    - "Vocabulary size equals config.graph.n (tokens are vertex IDs, no special tokens)"
    - "ExtractionMode.NONE returns logits only with no extraction overhead"
    - "Residual stream exposed when mode is RESIDUAL or FULL, providing per-layer states at every position"
  artifacts:
    - path: "src/model/types.py"
      provides: "ExtractionMode StrEnum (4 values), ForwardOutput dataclass, AttentionInternals dataclass"
      contains: "class ExtractionMode"
    - path: "src/model/attention.py"
      provides: "CausalSelfAttention with manual QK^T computation and optional extraction"
      contains: "class CausalSelfAttention"
    - path: "src/model/block.py"
      provides: "Pre-norm TransformerBlock with attention + MLP + residual connections"
      contains: "class TransformerBlock"
    - path: "src/model/transformer.py"
      provides: "TransformerLM full model with embeddings, blocks, output head, extraction, and factory"
      contains: "class TransformerLM"
      exports: ["TransformerLM", "create_model"]
    - path: "src/model/__init__.py"
      provides: "Public API for model package"
      exports: ["TransformerLM", "ExtractionMode", "ForwardOutput", "create_model"]
    - path: "tests/test_model.py"
      provides: "Comprehensive model tests covering all three requirements"
      min_lines: 100
  key_links:
    - from: "src/model/transformer.py"
      to: "src/config/experiment.py"
      via: "create_model factory reads ExperimentConfig"
      pattern: "config\\.graph\\.n|config\\.model\\.d_model|config\\.model\\.n_layers|config\\.training\\.w"
    - from: "src/model/attention.py"
      to: "src/model/types.py"
      via: "returns AttentionInternals when extract=True"
      pattern: "AttentionInternals"
    - from: "src/model/transformer.py"
      to: "src/model/types.py"
      via: "returns ForwardOutput with stacked layer tensors"
      pattern: "ForwardOutput"
    - from: "src/model/transformer.py"
      to: "src/model/block.py"
      via: "nn.ModuleList of TransformerBlock instances"
      pattern: "self\\.blocks"
    - from: "tests/test_model.py"
      to: "src/model/__init__.py"
      via: "imports public API"
      pattern: "from src\\.model import"
---

<objective>
Implement a NanoGPT-scale single-head causal transformer with transparent QK^T extraction for SVD stability analysis.

Purpose: The transformer is the instrument for this research -- it processes vertex-ID token sequences and exposes internal attention components (QK^T, A, V, Wv, Wo) so that Phase 6 can compute three SVD targets (QK^T routing, WvWo OV circuit, AVWo net residual update). The model must be fully transparent (no Flash Attention, no opaque wrappers) and support 4 extraction modes controlled by a string enum.

Output: Complete `src/model/` package with CausalSelfAttention, TransformerBlock, TransformerLM, types, factory function, and comprehensive tests.
</objective>

<execution_context>
@/root/.claude/get-shit-done/workflows/execute-plan.md
@/root/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-transformer-model/04-CONTEXT.md
@.planning/phases/04-transformer-model/04-RESEARCH.md
@src/config/experiment.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Types, Attention, and Block modules</name>
  <files>
    src/model/__init__.py
    src/model/types.py
    src/model/attention.py
    src/model/block.py
  </files>
  <action>
Create the `src/model/` package with three core modules:

**src/model/types.py** -- Data types for the model package:
- `ExtractionMode(StrEnum)` with 4 values: `NONE = "none"`, `SVD_TARGETS = "svd_targets"`, `RESIDUAL = "residual"`, `FULL = "full"`. Use `from enum import StrEnum` (Python 3.11+).
- `AttentionInternals` dataclass with fields: `qkt: torch.Tensor` (zero-filled causal masked, [B, T, T]), `attention_weights: torch.Tensor` ([B, T, T]), `values: torch.Tensor` ([B, T, D]). All tensors must already be detached when stored here.
- `ForwardOutput` dataclass with fields: `logits: torch.Tensor` ([B, T, vocab_size]), plus optional fields defaulting to None: `qkt: torch.Tensor | None` ([B, n_layers, T, T] zero-filled), `attention_weights: torch.Tensor | None` ([B, n_layers, T, T]), `values: torch.Tensor | None` ([B, n_layers, T, D]), `residual_stream: torch.Tensor | None` ([B, T, n_layers, D] -- full residual states per layer at every position), `residual_norms: torch.Tensor | None` ([B, T, n_layers] -- L2 norms per layer). Note: the model always returns the full residual stream when mode is RESIDUAL/FULL; Phase 6 handles the selective windowing and norm-only storage.

**src/model/attention.py** -- `CausalSelfAttention(nn.Module)`:
- Constructor args: `d_model: int`, `max_seq_len: int`, `dropout: float = 0.0`.
- Separate W_q, W_k, W_v, W_o as `nn.Linear(d_model, d_model, bias=False)`. No multi-head reshape -- single head means Q, K, V are each [B, T, D] directly.
- `register_buffer("causal_mask", torch.tril(torch.ones(max_seq_len, max_seq_len, dtype=torch.bool)))` -- created once, moves with model.
- `attn_dropout = nn.Dropout(dropout)` on attention weights, `resid_dropout = nn.Dropout(dropout)` on output projection.
- `forward(self, x: torch.Tensor, extract: bool = False) -> tuple[torch.Tensor, AttentionInternals | None]`:
  1. Compute q, k, v via W_q, W_k, W_v.
  2. Compute `qkt_raw = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(d_model))`.
  3. Slice causal_mask to current seq length T: `mask = self.causal_mask[:T, :T]`.
  4. For softmax path: `att_scores = qkt_raw.masked_fill(~mask, float('-inf'))`, then `att_weights = F.softmax(att_scores, dim=-1)`, then `att_weights_dropped = self.attn_dropout(att_weights)`.
  5. Output: `y = self.resid_dropout(self.W_o(att_weights_dropped @ v))`.
  6. If `extract=True`: create `qkt_target = qkt_raw.masked_fill(~mask, 0.0).detach()`, return `(y, AttentionInternals(qkt=qkt_target, attention_weights=att_weights.detach(), values=v.detach()))`.
  7. If `extract=False`: return `(y, None)`.

CRITICAL: The QK^T SVD target uses ZERO fill for masked positions (clean SVD input). The softmax path uses -inf fill. These are two different masks on the same raw QK^T. Do NOT confuse them.

CRITICAL: All tensors in AttentionInternals must be `.detach()` -- no gradient flow through extraction.

**src/model/block.py** -- `TransformerBlock(nn.Module)`:
- Constructor args: `d_model: int`, `max_seq_len: int`, `dropout: float = 0.0`.
- Pre-norm architecture: `self.ln_1 = nn.LayerNorm(d_model)` before attention, `self.ln_2 = nn.LayerNorm(d_model)` before MLP.
- `self.attention = CausalSelfAttention(d_model, max_seq_len, dropout)`.
- MLP: `self.mlp = nn.Sequential(nn.Linear(d_model, 4 * d_model), nn.GELU(), nn.Linear(4 * d_model, d_model), nn.Dropout(dropout))`. The 4x expansion is standard NanoGPT.
- `forward(self, x: torch.Tensor, extract: bool = False) -> tuple[torch.Tensor, AttentionInternals | None]`:
  1. `attn_out, internals = self.attention(self.ln_1(x), extract=extract)`
  2. `x = x + attn_out` (residual connection)
  3. `x = x + self.mlp(self.ln_2(x))` (residual connection)
  4. Return `(x, internals)`

**src/model/__init__.py** -- Stub initially exporting types:
```python
from src.model.types import ExtractionMode, ForwardOutput, AttentionInternals
```
(TransformerLM and create_model added in Task 2)
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -c "
from src.model.types import ExtractionMode, ForwardOutput, AttentionInternals
from src.model.attention import CausalSelfAttention
from src.model.block import TransformerBlock
import torch

# Test ExtractionMode enum values
assert ExtractionMode.NONE == 'none'
assert ExtractionMode.SVD_TARGETS == 'svd_targets'
assert ExtractionMode.RESIDUAL == 'residual'
assert ExtractionMode.FULL == 'full'

# Test attention module shapes
attn = CausalSelfAttention(d_model=64, max_seq_len=32)
x = torch.randn(2, 16, 64)
y, internals = attn(x, extract=True)
assert y.shape == (2, 16, 64), f'Output shape wrong: {y.shape}'
assert internals is not None
assert internals.qkt.shape == (2, 16, 16), f'QK^T shape wrong: {internals.qkt.shape}'
assert internals.attention_weights.shape == (2, 16, 16)
assert internals.values.shape == (2, 16, 64)
assert not internals.qkt.requires_grad, 'QK^T should be detached'

# Verify zero-fill masking on QK^T
assert internals.qkt[0, 0, 1] == 0.0, 'Future positions in QK^T should be zero'
assert internals.qkt[0, 1, 0] != 0.0, 'Past positions in QK^T should be non-zero (very likely)'

# Test block
block = TransformerBlock(d_model=64, max_seq_len=32)
y_block, block_internals = block(x, extract=True)
assert y_block.shape == (2, 16, 64)
assert block_internals is not None

# Test no extraction mode
y2, none_internals = attn(x, extract=False)
assert none_internals is None

print('All Task 1 checks passed')
"
    </automated>
  </verify>
  <done>ExtractionMode enum has 4 string values. CausalSelfAttention produces correct shapes with manual attention (no Flash Attention). QK^T zero-filled for SVD target, -inf for softmax. All extracted tensors detached. TransformerBlock wraps attention + MLP with pre-norm and residual connections.</done>
</task>

<task type="auto">
  <name>Task 2: TransformerLM, factory function, and comprehensive tests</name>
  <files>
    src/model/transformer.py
    src/model/__init__.py
    tests/test_model.py
  </files>
  <action>
**src/model/transformer.py** -- `TransformerLM(nn.Module)` and `create_model` factory:

`TransformerLM` constructor args: `vocab_size: int`, `d_model: int`, `n_layers: int`, `max_seq_len: int`, `dropout: float = 0.0`.
- `self.token_embedding = nn.Embedding(vocab_size, d_model)` -- learned token embeddings, vocab = vertex IDs.
- `self.position_embedding = nn.Embedding(max_seq_len, d_model)` -- learned positional embeddings, GPT-2 style.
- `self.embed_dropout = nn.Dropout(dropout)`.
- `self.blocks = nn.ModuleList([TransformerBlock(d_model, max_seq_len, dropout) for _ in range(n_layers)])`.
- `self.ln_f = nn.LayerNorm(d_model)` -- final layer norm (pre-norm convention: LN before output head).
- `self.lm_head = nn.Linear(d_model, vocab_size, bias=False)` -- output projection. SEPARATE from token_embedding (no weight tying, per CONTEXT.md).
- Store `self.d_model`, `self.n_layers`, `self.max_seq_len`, `self.vocab_size` as attributes for inspection.
- Weight initialization via `self.apply(self._init_weights)` then special residual scaling:
  - `_init_weights`: Normal(0, 0.02) for nn.Linear weights, zeros for biases, Normal(0, 0.02) for nn.Embedding weights.
  - After apply: scale W_o in each attention block and second linear in MLP by `1/sqrt(2 * n_layers)`. Access via `block.attention.W_o.weight` and `block.mlp[2].weight` (the second nn.Linear in the Sequential, index 2 after Linear->GELU->Linear->Dropout).

`forward(self, idx: torch.Tensor, mode: ExtractionMode = ExtractionMode.NONE) -> ForwardOutput`:
1. `B, T = idx.shape`. Assert `T <= self.max_seq_len`.
2. `tok_emb = self.token_embedding(idx)` -- [B, T, D].
3. `pos_ids = torch.arange(T, device=idx.device)`, `pos_emb = self.position_embedding(pos_ids)` -- [T, D].
4. `x = self.embed_dropout(tok_emb + pos_emb)`.
5. Determine `extract = mode != ExtractionMode.NONE`.
6. Collect per-layer internals: `all_qkt, all_attn, all_values = [], [], []`.
7. If mode in (RESIDUAL, FULL): also collect `residual_states = []` and append `x.detach()` before the first block (embedding output). Then after each block, append `x.detach()`.
8. Loop through blocks: `for block in self.blocks: x, internals = block(x, extract=extract)`. If extract and internals is not None, append qkt/attn/values to collectors.
9. Apply final LN: `x = self.ln_f(x)`.
10. Compute logits: `logits = self.lm_head(x)` -- [B, T, vocab_size].
11. Build ForwardOutput:
    - Always: `logits=logits`.
    - If extract: `qkt = torch.stack(all_qkt, dim=1)` ([B, n_layers, T, T]), similarly for attention_weights and values.
    - If mode in (RESIDUAL, FULL): `residual_stream = torch.stack(residual_states, dim=2)` to get [B, T, n_layers+1, D] (includes pre-block embedding state). Also compute `residual_norms = torch.linalg.norm(residual_stream, dim=-1)` -- [B, T, n_layers+1].
12. Return ForwardOutput.

`get_wvwo(self) -> torch.Tensor`:
- Convenience method returning WvWo = W_v.weight.T @ W_o.weight for each layer, stacked.
- Shape: [n_layers, d_model, d_model]. Detached.
- Implementation: `torch.stack([block.attention.W_v.weight.T @ block.attention.W_o.weight for block in self.blocks]).detach()`.
- Note on weight matrix convention: nn.Linear stores weights as [out_features, in_features]. So W_v.weight is [d_model, d_model]. The value projection takes input x and produces v = x @ W_v.weight.T. WvWo represents the OV circuit: the composition of value and output projections. The product W_v.weight.T @ W_o.weight gives [d_model, d_model] mapping input space through value to output space.

`create_model(config: ExperimentConfig) -> TransformerLM`:
- Factory function deriving all constructor args from config:
  - `vocab_size = config.graph.n` (MODL-03)
  - `d_model = config.model.d_model` (MODL-01)
  - `n_layers = config.model.n_layers` (MODL-01)
  - `max_seq_len = config.training.w` (context window)
  - `dropout = config.model.dropout`
- Import ExperimentConfig from `src.config.experiment`.

**src/model/__init__.py** -- Update to export full public API:
```python
from src.model.types import ExtractionMode, ForwardOutput, AttentionInternals
from src.model.transformer import TransformerLM, create_model
```

**tests/test_model.py** -- Comprehensive tests covering all three requirements:

MODL-01 tests:
- `test_anchor_config_creation`: create_model with ANCHOR_CONFIG, verify d_model=128, n_layers=4, vocab=500, max_seq_len=64.
- `test_configurable_d_model`: create models with d_model=64, 128, 256 -- each produces correct logit shape.
- `test_configurable_n_layers`: create models with n_layers=2, 4, 6 -- verify correct number of blocks.
- `test_single_head_enforced`: ExperimentConfig already rejects n_heads != 1. Verify model has no head dimension reshaping (Q, K, V are [B, T, D]).

MODL-02 tests:
- `test_extraction_none_returns_logits_only`: ExtractionMode.NONE returns ForwardOutput with logits, all other fields None.
- `test_extraction_svd_targets_shapes`: ExtractionMode.SVD_TARGETS returns qkt [B, n_layers, T, T], attention_weights [B, n_layers, T, T], values [B, n_layers, T, D]. No residual_stream.
- `test_qkt_zero_filled_causal_mask`: Verify upper triangle of qkt is all zeros (zero fill, not -inf).
- `test_qkt_lower_triangle_nonzero`: Verify lower triangle and diagonal of qkt contain actual values (not zeros).
- `test_attention_weights_sum_to_one`: Verify each row of attention_weights sums to ~1.0 (valid probability distribution).
- `test_attention_weights_causal`: Verify attention_weights have zeros above diagonal (causal mask enforced).
- `test_extracted_tensors_detached`: Verify qkt, attention_weights, values all have requires_grad=False.
- `test_get_wvwo_shape`: Verify get_wvwo() returns [n_layers, d_model, d_model] detached tensor.
- `test_get_wvwo_input_agnostic`: Call get_wvwo() twice, verify identical (no input dependence).
- `test_residual_mode_returns_stream`: ExtractionMode.RESIDUAL returns residual_stream [B, T, n_layers+1, D] and residual_norms [B, T, n_layers+1].
- `test_full_mode_returns_everything`: ExtractionMode.FULL returns all fields populated.

MODL-03 tests:
- `test_vocab_equals_graph_n`: create_model with different config.graph.n values (100, 500), verify embedding and lm_head sizes match n.
- `test_no_special_tokens`: Verify vocab_size attribute equals config.graph.n exactly (no +1 for BOS/PAD/EOS).
- `test_valid_token_range`: Forward pass with token IDs from 0 to n-1 succeeds; verify all logits have vocab_size=n.

Integration tests:
- `test_weight_initialization`: After construction, check embedding std is roughly 0.02 (within tolerance), W_o weights have smaller std (residual scaling applied).
- `test_no_weight_tying`: Verify token_embedding.weight is NOT the same tensor as lm_head.weight (`is not` check).
- `test_deterministic_output`: With same seed, two forward passes produce identical logits.
- `test_parameter_count`: With anchor config (d_model=128, n_layers=4, vocab=500), parameter count is reasonable (sanity check, not exact match).

Use `from src.config.defaults import ANCHOR_CONFIG` and `from src.config.experiment import ExperimentConfig, GraphConfig, ModelConfig, TrainingConfig` for config construction in tests. Use `from src.model import TransformerLM, ExtractionMode, ForwardOutput, create_model` for model imports.
  </action>
  <verify>
    <automated>cd /root/Repos/dcsbm-transformer && python -m pytest tests/test_model.py -x -v 2>&1 | tail -40</automated>
  </verify>
  <done>TransformerLM processes token sequences with correct shapes for all d_model/n_layers configs. All 4 ExtractionMode values produce the correct ForwardOutput fields. QK^T is zero-filled (not -inf) in masked positions. All extracted tensors are detached. get_wvwo() returns stacked weight product. Residual stream returned when requested. create_model factory correctly derives parameters from ExperimentConfig. Vocabulary equals graph.n with no special tokens. All tests pass.</done>
</task>

</tasks>

<verification>
Run all model tests:
```bash
python -m pytest tests/test_model.py -x -v
```

Run full test suite to ensure no regressions:
```bash
python -m pytest tests/ -x --timeout=60
```

Quick shape verification with anchor config:
```bash
python -c "
from src.config.defaults import ANCHOR_CONFIG
from src.model import create_model, ExtractionMode
import torch

model = create_model(ANCHOR_CONFIG)
model.eval()
x = torch.randint(0, ANCHOR_CONFIG.graph.n, (2, ANCHOR_CONFIG.training.w))
with torch.no_grad():
    out = model(x, mode=ExtractionMode.SVD_TARGETS)
print(f'Logits: {out.logits.shape}')
print(f'QKT: {out.qkt.shape}')
print(f'Attn: {out.attention_weights.shape}')
print(f'Values: {out.values.shape}')
print(f'WvWo: {model.get_wvwo().shape}')
print('All shapes correct')
"
```
</verification>

<success_criteria>
1. `python -m pytest tests/test_model.py -x -v` -- all tests pass
2. `python -m pytest tests/ -x --timeout=60` -- full suite passes (no regressions)
3. TransformerLM with anchor config (d_model=128, n_layers=4, vocab=500, w=64) produces logits [B, 64, 500], qkt [B, 4, 64, 64], attention_weights [B, 4, 64, 64], values [B, 4, 64, 128]
4. get_wvwo() returns [4, 128, 128] detached tensor
5. ExtractionMode.NONE returns only logits (all other fields None)
6. No weight tying between token_embedding and lm_head
</success_criteria>

<output>
After completion, create `.planning/phases/04-transformer-model/04-01-SUMMARY.md`
</output>
