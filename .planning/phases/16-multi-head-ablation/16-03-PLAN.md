---
phase: 16-multi-head-ablation
plan: 03
type: execute
wave: 3
depends_on: [16-01, 16-02]
files_modified:
  - src/analysis/signal_concentration.py
  - src/analysis/auroc_horizon.py
  - tests/test_signal_concentration.py
  - tests/test_auroc_horizon.py
autonomous: true
requirements: [MHAD-03, MHAD-04]
---

must_haves:
  truths:
    - "Per-head AUROC is computed by parsing head index from NPZ metric keys in format target.layer_N.head_H.metric_name"
    - "signal_concentration.py computes entropy and Gini coefficient of per-head AUROC distribution across heads"
    - "Entropy is computed as H = -sum(p_h * log2(p_h)) where p_h = AUROC_h / sum(AUROC_h), normalized to [0, 1] by dividing by log2(n_heads)"
    - "Gini coefficient is computed on per-head max-AUROC values using the standard formula"
    - "auroc_horizon.py _is_primary_metric correctly handles per-head key format (strips head_H from key for matching)"
    - "Signal concentration report dict includes per-head AUROC values, entropy, Gini, max-to-mean ratio, and the index of the head with highest AUROC"
    - "Single-head results produce trivial concentration (entropy=0, Gini=0) since there is only one head"
    - "The analysis correctly handles NaN AUROC values (heads with no signal)"
    - "All tests pass for both single-head and multi-head configurations"
  artifacts:
    - src/analysis/signal_concentration.py
    - src/analysis/auroc_horizon.py
    - tests/test_signal_concentration.py
    - tests/test_auroc_horizon.py
  key_links:
    - "signal_concentration.py imports and uses compute_auroc_curve from auroc_horizon.py"
    - "auroc_horizon.py _is_primary_metric strips head_H component: target.layer_N.head_H.metric -> target.metric"
    - "signal_concentration.compute_signal_concentration takes per-head AUROC dict and returns concentration metrics"
    - "signal_concentration.compute_per_head_auroc takes NPZ arrays and events, returns per-head AUROC dict"

<objective>
Implement per-head AUROC computation, signal concentration analysis (entropy/Gini), and ablation comparison infrastructure.

Purpose: MHAD-03 requires per-head AUROC and signal concentration analysis. MHAD-04 requires ablation comparison on matched configs reporting per-head vs aggregate signal strength.
Output: New signal_concentration.py module, updated auroc_horizon.py for per-head key parsing, and comprehensive tests.
</objective>

<execution_context>
@/root/.claude/get-shit-done/workflows/execute-plan.md
@/root/.claude/get-shit-done/references/tdd.md
@/root/.claude/get-shit-done/references/checkpoints.md
@/root/.claude/get-shit-done/references/model-profile-resolution.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16-multi-head-ablation/16-CONTEXT.md
@.planning/phases/16-multi-head-ablation/16-RESEARCH.md

@src/analysis/auroc_horizon.py
@src/analysis/event_extraction.py
@src/analysis/statistical_controls.py
@src/evaluation/pipeline.py
@tests/test_auroc_horizon.py

<interfaces>
<!-- Key types and contracts the executor needs. -->

From src/analysis/auroc_horizon.py:
```python
PRIMARY_METRICS: frozenset[str] = frozenset({
    "qkt.grassmannian_distance",
    "qkt.spectral_gap_1_2",
    "qkt.spectral_entropy",
    "qkt.stable_rank",
    "avwo.stable_rank",
})

def _is_primary_metric(metric_key: str) -> bool:
    """Check if a metric key (e.g. 'qkt.layer_0.grassmannian_distance') is primary."""
    parts = metric_key.split(".")
    if len(parts) == 3:
        # target.layer_N.metric_name -> target.metric_name
        target_metric = f"{parts[0]}.{parts[2]}"
        return target_metric in PRIMARY_METRICS
    return False

def compute_auroc_curve(
    violation_events: list[AnalysisEvent],
    control_events: list[AnalysisEvent],
    metric_array: np.ndarray,
    r_value: int,
) -> np.ndarray:
    """Compute AUROC at each lookback distance j from 1 to r."""
```

After Plan 16-02, NPZ keys are:
- Single-head: `qkt.layer_0.grassmannian_distance` AND `qkt.layer_0.head_0.grassmannian_distance`
- Multi-head: `qkt.layer_0.head_0.grassmannian_distance`, `qkt.layer_0.head_1.grassmannian_distance`

Key parsing for `_is_primary_metric` must handle BOTH formats:
- 3-part: `target.layer_N.metric_name` (legacy) -> target.metric_name
- 4-part: `target.layer_N.head_H.metric_name` (per-head) -> target.metric_name

From src/analysis/event_extraction.py:
```python
@dataclass
class AnalysisEvent:
    sequence_idx: int
    event_step: int
    event_type: str  # "violation" or "control"
    r_value: int
```

Signal concentration metrics (from PITFALLS.md Pitfall 13):
- Frame as descriptive statistics, not hypothesis tests
- Per-head AUROC with confidence intervals
- Entropy measures uniformity of signal distribution
- Gini coefficient measures inequality
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create signal concentration analysis module</name>
  <files>
    src/analysis/signal_concentration.py
    tests/test_signal_concentration.py
  </files>
  <action>
Create `src/analysis/signal_concentration.py`:

```python
"""Signal concentration analysis for multi-head ablation (Phase 16: MHAD-03).

Measures whether the predictive SVD signal concentrates in specific attention
heads or distributes uniformly across all heads. Uses entropy and Gini
coefficient of per-head AUROC distributions as concentration metrics.

These are descriptive statistics, not hypothesis tests (see PITFALLS.md #13).
"""

import numpy as np
from typing import Any


def compute_auroc_entropy(per_head_aurocs: np.ndarray) -> float:
    """Compute normalized entropy of AUROC distribution across heads.

    Args:
        per_head_aurocs: Array of shape [n_heads] with max AUROC per head.
            Values should be in [0, 1]. NaN values are treated as 0.5 (chance).

    Returns:
        Normalized entropy in [0, 1]. 0 = signal concentrated in one head,
        1 = signal uniformly distributed across all heads.
        Returns NaN if n_heads < 2.
    """
    aurocs = np.array(per_head_aurocs, dtype=np.float64)
    n_heads = len(aurocs)
    if n_heads < 2:
        return float("nan")

    # Replace NaN with 0.5 (chance level)
    aurocs = np.where(np.isnan(aurocs), 0.5, aurocs)

    # Clamp to avoid issues with exactly 0
    aurocs = np.maximum(aurocs, 1e-10)

    # Normalize to probability distribution
    total = aurocs.sum()
    if total < 1e-10:
        return 1.0  # All zero -> uniform (degenerate)
    p = aurocs / total

    # Shannon entropy
    H = -np.sum(p * np.log2(p))

    # Normalize by max entropy (log2(n_heads))
    H_max = np.log2(n_heads)
    return float(H / H_max) if H_max > 0 else float("nan")


def compute_gini_coefficient(per_head_aurocs: np.ndarray) -> float:
    """Compute Gini coefficient of AUROC distribution across heads.

    Args:
        per_head_aurocs: Array of shape [n_heads] with max AUROC per head.
            NaN values are treated as 0.5 (chance).

    Returns:
        Gini coefficient in [0, 1]. 0 = perfectly equal, 1 = perfectly unequal.
        Returns NaN if n_heads < 2.
    """
    aurocs = np.array(per_head_aurocs, dtype=np.float64)
    n_heads = len(aurocs)
    if n_heads < 2:
        return float("nan")

    aurocs = np.where(np.isnan(aurocs), 0.5, aurocs)

    # Sort ascending
    sorted_aurocs = np.sort(aurocs)
    n = len(sorted_aurocs)
    mean_auroc = sorted_aurocs.mean()

    if mean_auroc < 1e-10:
        return 0.0  # All zero -> equal

    # Standard Gini formula
    index = np.arange(1, n + 1)
    gini = (2 * np.sum(index * sorted_aurocs) - (n + 1) * np.sum(sorted_aurocs)) / (n * np.sum(sorted_aurocs))
    return float(gini)


def compute_signal_concentration(
    per_head_max_aurocs: dict[int, float],
    metric_name: str = "",
) -> dict[str, Any]:
    """Compute signal concentration metrics for a set of per-head AUROC values.

    Args:
        per_head_max_aurocs: Mapping of head_idx -> max AUROC value for that head.
        metric_name: Optional name of the SVD metric (for labeling).

    Returns:
        Dict with keys:
            - metric_name: str
            - n_heads: int
            - per_head_aurocs: dict[int, float]
            - entropy: float (normalized, 0=concentrated, 1=distributed)
            - gini: float (0=equal, 1=unequal)
            - max_to_mean_ratio: float
            - dominant_head: int (head index with highest AUROC)
            - dominant_auroc: float
            - interpretation: str (human-readable summary)
    """
    n_heads = len(per_head_max_aurocs)
    auroc_values = np.array([per_head_max_aurocs[h] for h in sorted(per_head_max_aurocs.keys())])

    entropy = compute_auroc_entropy(auroc_values)
    gini = compute_gini_coefficient(auroc_values)

    # Max-to-mean ratio
    valid = auroc_values[~np.isnan(auroc_values)]
    if len(valid) > 0 and valid.mean() > 1e-10:
        max_to_mean = float(valid.max() / valid.mean())
    else:
        max_to_mean = float("nan")

    # Dominant head
    if len(valid) > 0:
        sorted_heads = sorted(per_head_max_aurocs.keys())
        dominant_idx = sorted_heads[int(np.nanargmax(auroc_values))]
        dominant_auroc = float(np.nanmax(auroc_values))
    else:
        dominant_idx = 0
        dominant_auroc = float("nan")

    # Interpretation
    if n_heads < 2:
        interp = "Single-head: concentration analysis not applicable"
    elif entropy < 0.5:
        interp = f"Signal CONCENTRATED in head {dominant_idx} (entropy={entropy:.3f}, Gini={gini:.3f})"
    elif entropy > 0.9:
        interp = f"Signal DISTRIBUTED across all heads (entropy={entropy:.3f}, Gini={gini:.3f})"
    else:
        interp = f"Signal PARTIALLY concentrated (entropy={entropy:.3f}, Gini={gini:.3f})"

    return {
        "metric_name": metric_name,
        "n_heads": n_heads,
        "per_head_aurocs": dict(per_head_max_aurocs),
        "entropy": entropy,
        "gini": gini,
        "max_to_mean_ratio": max_to_mean,
        "dominant_head": dominant_idx,
        "dominant_auroc": dominant_auroc,
        "interpretation": interp,
    }


def compute_ablation_comparison(
    results_by_n_heads: dict[int, dict[str, float]],
) -> dict[str, Any]:
    """Compare signal strength across ablation configs (1h/2h/4h).

    Args:
        results_by_n_heads: Mapping of n_heads -> {metric_key: max_auroc}.
            Each entry represents one ablation config's best AUROC per metric.

    Returns:
        Dict with ablation comparison summary:
            - configs: list of n_heads values tested
            - per_config_best_auroc: {n_heads: {metric: max_auroc}}
            - aggregate_comparison: {metric: {n_heads: auroc}}
            - conclusion: str (human-readable summary)
    """
    configs = sorted(results_by_n_heads.keys())

    # Collect all metric names across configs
    all_metrics = set()
    for n_heads in configs:
        all_metrics.update(results_by_n_heads[n_heads].keys())

    # Per-metric comparison
    aggregate = {}
    for metric in sorted(all_metrics):
        aggregate[metric] = {}
        for n_heads in configs:
            aggregate[metric][n_heads] = results_by_n_heads[n_heads].get(metric, float("nan"))

    # Simple conclusion based on whether multi-head improves over single-head
    conclusions = []
    if 1 in configs:
        for metric in sorted(all_metrics):
            single_auroc = aggregate[metric].get(1, float("nan"))
            for n_heads in configs:
                if n_heads == 1:
                    continue
                multi_auroc = aggregate[metric].get(n_heads, float("nan"))
                if not np.isnan(single_auroc) and not np.isnan(multi_auroc):
                    if multi_auroc > single_auroc + 0.02:
                        conclusions.append(f"{metric}: {n_heads}h improves over 1h ({multi_auroc:.3f} vs {single_auroc:.3f})")
                    elif single_auroc > multi_auroc + 0.02:
                        conclusions.append(f"{metric}: 1h outperforms {n_heads}h ({single_auroc:.3f} vs {multi_auroc:.3f})")
                    else:
                        conclusions.append(f"{metric}: comparable across 1h and {n_heads}h")

    return {
        "configs": configs,
        "per_config_best_auroc": dict(results_by_n_heads),
        "aggregate_comparison": aggregate,
        "conclusion": "; ".join(conclusions) if conclusions else "Insufficient data for comparison",
    }
```

Create `tests/test_signal_concentration.py`:

```python
"""Tests for signal concentration analysis (Phase 16: MHAD-03, MHAD-04)."""

import numpy as np
import pytest

from src.analysis.signal_concentration import (
    compute_auroc_entropy,
    compute_gini_coefficient,
    compute_signal_concentration,
    compute_ablation_comparison,
)


class TestAurocEntropy:
    def test_uniform_distribution_max_entropy(self):
        """Equal AUROC across heads -> entropy = 1.0."""
        aurocs = np.array([0.7, 0.7, 0.7, 0.7])
        assert compute_auroc_entropy(aurocs) == pytest.approx(1.0, abs=1e-6)

    def test_concentrated_low_entropy(self):
        """One high, rest low -> entropy < 0.5."""
        aurocs = np.array([0.9, 0.5, 0.5, 0.5])
        entropy = compute_auroc_entropy(aurocs)
        assert entropy < 0.95  # Not fully uniform

    def test_single_head_returns_nan(self):
        """Single head -> entropy is NaN."""
        assert np.isnan(compute_auroc_entropy(np.array([0.8])))

    def test_two_heads_equal(self):
        """Two equal heads -> entropy = 1.0."""
        assert compute_auroc_entropy(np.array([0.7, 0.7])) == pytest.approx(1.0, abs=1e-6)

    def test_two_heads_unequal(self):
        """Two unequal heads -> entropy < 1.0."""
        entropy = compute_auroc_entropy(np.array([0.9, 0.5]))
        assert 0 < entropy < 1.0

    def test_nan_values_treated_as_chance(self):
        """NaN AUROC values replaced with 0.5."""
        aurocs = np.array([0.8, float("nan"), 0.8, 0.8])
        entropy = compute_auroc_entropy(aurocs)
        assert not np.isnan(entropy)


class TestGiniCoefficient:
    def test_equal_values_zero_gini(self):
        """Equal AUROC -> Gini = 0."""
        aurocs = np.array([0.7, 0.7, 0.7, 0.7])
        assert compute_gini_coefficient(aurocs) == pytest.approx(0.0, abs=1e-6)

    def test_unequal_positive_gini(self):
        """Unequal AUROC -> Gini > 0."""
        aurocs = np.array([0.9, 0.5, 0.5, 0.5])
        gini = compute_gini_coefficient(aurocs)
        assert gini > 0

    def test_single_head_returns_nan(self):
        assert np.isnan(compute_gini_coefficient(np.array([0.8])))

    def test_gini_bounded_0_1(self):
        """Gini should be in [0, 1]."""
        aurocs = np.array([0.99, 0.01, 0.01, 0.01])
        gini = compute_gini_coefficient(aurocs)
        assert 0 <= gini <= 1


class TestSignalConcentration:
    def test_basic_output_structure(self):
        result = compute_signal_concentration({0: 0.8, 1: 0.6}, metric_name="grassmannian")
        assert result["metric_name"] == "grassmannian"
        assert result["n_heads"] == 2
        assert result["dominant_head"] == 0
        assert result["dominant_auroc"] == pytest.approx(0.8)
        assert "entropy" in result
        assert "gini" in result
        assert "interpretation" in result

    def test_single_head_not_applicable(self):
        result = compute_signal_concentration({0: 0.8})
        assert "not applicable" in result["interpretation"].lower()

    def test_concentrated_signal(self):
        result = compute_signal_concentration({0: 0.95, 1: 0.52, 2: 0.51, 3: 0.50})
        assert result["entropy"] < 0.8
        assert "concentrated" in result["interpretation"].lower() or "partially" in result["interpretation"].lower()


class TestAblationComparison:
    def test_basic_comparison(self):
        results = {
            1: {"qkt.grassmannian_distance": 0.75},
            2: {"qkt.grassmannian_distance": 0.78},
            4: {"qkt.grassmannian_distance": 0.72},
        }
        comp = compute_ablation_comparison(results)
        assert comp["configs"] == [1, 2, 4]
        assert "aggregate_comparison" in comp
        assert "conclusion" in comp

    def test_empty_results(self):
        comp = compute_ablation_comparison({})
        assert comp["configs"] == []
```
  </action>
</task>

<task type="auto">
  <name>Task 2: Update auroc_horizon.py for per-head key parsing</name>
  <files>
    src/analysis/auroc_horizon.py
  </files>
  <action>
Update `_is_primary_metric` to handle both 3-part (legacy) and 4-part (per-head) key formats:

```python
def _is_primary_metric(metric_key: str) -> bool:
    """Check if a metric key is primary.

    Handles both legacy format (target.layer_N.metric_name) and
    per-head format (target.layer_N.head_H.metric_name).

    Args:
        metric_key: NPZ key like 'qkt.layer_0.grassmannian_distance'
            or 'qkt.layer_0.head_0.grassmannian_distance'.

    Returns:
        True if the underlying metric (ignoring layer and head) is in PRIMARY_METRICS.
    """
    parts = metric_key.split(".")
    if len(parts) == 3:
        # Legacy: target.layer_N.metric_name -> target.metric_name
        target_metric = f"{parts[0]}.{parts[2]}"
        return target_metric in PRIMARY_METRICS
    elif len(parts) == 4:
        # Per-head: target.layer_N.head_H.metric_name -> target.metric_name
        target_metric = f"{parts[0]}.{parts[3]}"
        return target_metric in PRIMARY_METRICS
    return False
```

Add a helper function to parse metric key components:

```python
def parse_metric_key(metric_key: str) -> dict[str, str | int | None]:
    """Parse a metric key into its components.

    Args:
        metric_key: NPZ key like 'qkt.layer_0.head_1.grassmannian_distance'
            or 'qkt.layer_0.grassmannian_distance'.

    Returns:
        Dict with keys: target, layer_idx, head_idx (None for legacy), metric_name.
    """
    parts = metric_key.split(".")
    if len(parts) == 4:
        return {
            "target": parts[0],
            "layer_idx": int(parts[1].split("_")[1]),
            "head_idx": int(parts[2].split("_")[1]),
            "metric_name": parts[3],
        }
    elif len(parts) == 3:
        return {
            "target": parts[0],
            "layer_idx": int(parts[1].split("_")[1]),
            "head_idx": None,
            "metric_name": parts[2],
        }
    else:
        return {"target": "", "layer_idx": -1, "head_idx": None, "metric_name": metric_key}
```

Ensure the main `compute_auroc_analysis` function works with per-head keys without modification -- it iterates over all keys in the NPZ, so per-head keys are automatically included. The `_is_primary_metric` fix ensures they are correctly flagged as primary/secondary.
  </action>
</task>

<task type="auto">
  <name>Task 3: Update auroc_horizon tests for per-head keys</name>
  <files>
    tests/test_auroc_horizon.py
  </files>
  <action>
Add tests for per-head key parsing:

```python
class TestIsPerHeadPrimaryMetric:
    def test_legacy_key_is_primary(self):
        assert _is_primary_metric("qkt.layer_0.grassmannian_distance")

    def test_per_head_key_is_primary(self):
        assert _is_primary_metric("qkt.layer_0.head_0.grassmannian_distance")

    def test_per_head_secondary_not_primary(self):
        assert not _is_primary_metric("qkt.layer_0.head_0.condition_number")

    def test_multi_head_key_different_heads(self):
        assert _is_primary_metric("qkt.layer_0.head_1.spectral_entropy")
        assert _is_primary_metric("avwo.layer_2.head_3.stable_rank")


class TestParseMetricKey:
    def test_legacy_format(self):
        parsed = parse_metric_key("qkt.layer_0.grassmannian_distance")
        assert parsed["target"] == "qkt"
        assert parsed["layer_idx"] == 0
        assert parsed["head_idx"] is None
        assert parsed["metric_name"] == "grassmannian_distance"

    def test_per_head_format(self):
        parsed = parse_metric_key("qkt.layer_2.head_1.spectral_entropy")
        assert parsed["target"] == "qkt"
        assert parsed["layer_idx"] == 2
        assert parsed["head_idx"] == 1
        assert parsed["metric_name"] == "spectral_entropy"
```

Run full test suite to verify nothing is broken:
```
pytest tests/ -x -v
```
  </action>
</task>

<task type="auto">
  <name>Task 4: Run all tests and verify requirements</name>
  <files></files>
  <action>
Run the complete test suite and verify each MHAD requirement:

```
pytest tests/ -x -v
```

Verification checklist:
- [ ] MHAD-01: n_heads=1,2,4 accepted; d_k constant; per-head QK^T extractable
- [ ] MHAD-02: Per-head NPZ keys; dual emission for n_heads=1; backward compat
- [ ] MHAD-03: Per-head AUROC; entropy; Gini; signal concentration report
- [ ] MHAD-04: Ablation comparison infrastructure (compute_ablation_comparison)

Success criteria from ROADMAP:
1. Transformer accepts n_heads=1,2,4 with d_k=128 constant -- VERIFIED by config tests
2. SVD metrics per-head with correct key format -- VERIFIED by pipeline tests
3. Per-head AUROC + signal concentration -- VERIFIED by signal_concentration tests
4. Ablation comparison infrastructure -- VERIFIED by compute_ablation_comparison tests

Note: MHAD-04 "runs ablation comparison" is about the infrastructure to compare results.
The actual ablation experiment (training 1h/2h/4h models on same data) requires running
the training pipeline, which is beyond the scope of automated tests. The infrastructure
to compare results across configs is what this plan delivers.
  </action>
</task>

</tasks>
